https://www.dezyre.com/article/pig-interview-questions-and-answers-for-2017/244
https://intellipaat.com/interview-question/pig-interview-questions/
https://acadgild.com/blog/top-12-pig-interview-questions-in-2017/
-------------------------------------------
What happens when we load a schema without schema:
USA  = LOAD 'UserInfo_USA.txt' USING PigStorage(',');
DUMP USA;   //shows data.
usa_eval = FOREACH USA GENERATE $0,$1,$2,$3;
DUMP usa_eval;
o/p: i.e. the field not presented is automatically set to NULL.
(John,Eickhoff,50,)(Brenda,Akin,30,)(Tina,Caffey,60,)(Kathy,Rhodes,70,)(Sue,Flachs,45,)(Debra,Reller,45,)(Keith,Rohrer,60,)(George,Lucas,60,)

India = LOAD 'UserInfo_India.txt' USING PigStorage(',');
DUMP India; //shows data.
india_eval = FOREACH India GENERATE $0,$1,$2,$3,$4,$5;
DUMP india_eval;
o/p: i.e. the field not presented is automatically set to NULL. i.e. $3,$4,$5
(Akash,Chavan,34,,,)(Prateek,Kumar,35,,,)(Shubhro,Banerjee,33,,,)(Sai,Tez,33,,,)(Falak,Narang,35,,,)(Sumit,Kumar,36,,,)(Abhishek,Wason,33,,,)(Ajay,Upadhyay,40,,,)(Yogesh,Mandal,43,,,)

NOTE: DEFAULT DELIMITER IS '\t'. If the delimiter was a tab does not exist in the input records, the whole line was treated as one field. 
Pig does not complain if a field is null- It just outputs nothing when there is a null. Hence you see only the commas when you dump.

--------------------------------------------
Writing Macro in Pig: http://www.hadooplessons.info/2014/12/writing-macro-in-pig.html

-------------------------------------------
DAG: Directed acyclic graph in PIG is thrown on top of mapreduce (but DAG is not always mutually exclusive with map-reduce).
Tez uses the concept of a DAG to enforce concurrency and serialization between MapReduce jobs. 
Say, for example, jobs A and B are independent of each other, but job C needs the results from A and B to complete, 
Tez will execute A and B in any order and forward the results to C, which is most easily accomplished by specifying job ordering with a DAG.
 
Why is output in Pig is not sorted by default?
Outputs in PIG can be obtained by two methods.
1. DUMP – on the Screen 
2. STORE – in the file-system

Whenever either of these commands is given a DAG is created and fastest method for operation is used to generate the result which is then either displayed on screen or 
stored in file as per the command. Till then no operation is done as PIG follows Lazy Evaluation.

Now PIG allocates a fixed amount of memory to store bags of data. 
And that memory space is even shared with all the other bags of data that are used by the application. 
Also if the outputs are stored that with increase the processing time and overhead related to reading the data from memory and loading. 
So to speed up the process by default the outputs in PIG are not stored.

To speed up processing and leverage the underlying Map Reduce framework to its full potential, PIG took this route. 
We can impose sorting on the output by using the 'ORDER BY' command

When you run a PIGLatin script, Job DAG:<> or NULL shows up in the LOGS.

--------------------------------------------
what does this warning mean "Encountered Warning IMPLICIT_CAST_TO_DOUBLE 16 time(s)."

You'll get the warnings when Pig has to implicitly cast from one type to another, for instance when you pass a field as an argument to a function 
that requires some type but the field is another type.
casts always cost, so you should try to eliminate data type casting as much as you can.
--------------------------------------------
What is Piggy Bank:
is a place for Pig users to share their functions

example:
REGISTER /public/share/pig/contrib/piggybank/java/piggybank.jar ;
TweetsInaug  = FILTER Tweets BY org.apache.pig.piggybank.evaluation.string.UPPER(text) MATCHES '.*(INAUG|OBAMA|BIDEN|CHENEY|BUSH).*' ;
STORE TweetsInaug INTO 'meta/inaug/tweets_inaug' ;

More details to use Piggy Bank: https://stackoverflow.com/questions/27414351/how-to-use-over-function-in-piggybank
-------------------------------------------
Pig  if-then-else construction:

Pig is data flow language not control flow. So having intense control flows is not preferred in PIG LATIN.

Pig's if-then-else is an arithmetic operator invoked with the shorthand "condition ? true_value : false_value" as part of an expression, such as:
X = FOREACH A GENERATE f2, (f2==1?1:COUNT(B));

case statement can be done as,
c2 = FOREACH c1 GENERATE (CASE group
                            WHEN 1 THEN 'NE'
                            WHEN 2 THEN 'SE'
                            WHEN 3 THEN 'AE'
                            ELSE 'VR' END), COUNT(country.zone);

--------------------------------------------
Explain about the BloomMapFile. (more info required)

It is an extended class of MapFile. Its functionality is similar to MapFile. 
It is used in the Hbase table format, Bloom Map File uses dynamic Bloom filters to provide rapid membership test for the keys.
--------------------------------------------
UNION() and SPLIT() functions

Union operator helps to merge the contents of two or more relations.
Syntax: grunt> Relation_name3 = UNION Relation_name1, Relation_name2
Example: grunt> all_country = UNION USA,India;
//data in USA will come first and then India

How about files which have different data formats i.e. one is delimited by ',' and one is delimited by '\t'
India = LOAD 'UserInfo_India.txt' USING PigStorage(',');
America = LOAD 'UserInfo_America.txt' USING PigStorage('\t');
all_cntry = UNION India,America;

SPLIT operator helps to divide the contents of two or more relations.
Syntax: grunt> SPLIT Relationa1_name INTO Relationa2_name IF (condition1), Relation2_name (condition2);
Example: 
SPLIT all_cntry into cntry1 if $2 <= 40, cntry2 if $3 > 40;

NOTE: Pig will do implicit casting. Also, the original relation which is SPLIT will be retained i.e. all_cntry will still exist.

--------------------------------------------
TOP() 
TOP () function returns the top N tuples from a bag of tuples or a relation. 
N is passed as a parameter to the function top () along with the column whose values are to be compared and the relation R.

TOP(topN,column,relation)

A =Load ..
B= Group A by age;
C = Foreach B {
top = TOP(2,0,A)
Generate top;
}

Better explaination: https://www.tutorialspoint.com/apache_pig/apache_pig_top.htm

--------------------------------------------
DESCRIBE, EXPLAIN, ILLUSTRATE operators in PIG: https://pig.apache.org/docs/r0.11.1/test.html
i.e.  the exception handling operators in Pig script

NOTE: if there is no schema defined, DESCRIBE, ILLUSTRATE errors out.
--------------------------------------------
12) Explain about the execution plans of a Pig Script 
	Or 
	Differentiate between the logical and physical plan of an Apache Pig script

	Logical and Physical plans are created during the execution of a pig script. Pig scripts are based on interpreter checking. 
	Logical plan is produced after semantic checking and basic parsing and no data processing takes place during the creation of a logical plan. 
	For each line in the Pig script, syntax check is performed for operators and a logical plan is created. 
	Whenever an error is encountered within the script, an exception is thrown and the program execution ends, else for each statement in the script has its own logical plan.

	A logical plan contains collection of operators in the script but does not contain the edges between the operators.

	After the logical plan is generated, the script execution moves to the physical plan where there is a description about the physical operators, Apache Pig will use,
	 to execute the Pig script. A physical plan is more or less like a series of MapReduce jobs but then the plan does not have any reference on 
	 how it will be executed in MapReduce. 
	 During the creation of physical plan, cogroup logical operator is converted into 3 physical operators namely –Local Rearrange, Global Rearrange and Package. 
	 Load and store functions usually get resolved in the physical plan.
	 
	Also,
		Both plans are created while to execute the pig script.

		Physical plan : It is a series of MapReduce jobs while creating the physical plan.
		It’s divided into three physical operators such as Local Rearrange, Global Rearrange, and package. 
		It illustrates the physical operators Pig will use to execute the script without referring to 
		how they will execute in MapReduce Loading and storing functions are resolved in physical plan.
		Example- A: Load(/emp:PigStorage(‘ ‘))

		Logical plan : The Logical plan is a plan which is created for each line in the Pig scripts. 
		It is produced after semantic checking and basic parsing. With every line, 
		the logical plan for that particular program becomes extended and larger because each and every statement 
		has its own logical plan.Loading and storing function are not resolved in logical plan.
		Example: X: (Name: LOLoad schema: emp_id#36:bytearray,emp_name#37:bytearray,city#38:bytearray,salary#39:bytearray)Required Fields:null
	 
----------------------------------------------
GROUP and COGROUP

group operator is normally used with one relation, while the cogroup operator is used in statements involving two or more relations.

example in https://www.tutorialspoint.com/apache_pig/apache_pig_cogroup_operator.htm

----------------------------------------------
What are the relation operations in Pig?
foreach, order by, filters, group, distinct, join, limit.foreach: It takes a set of expressions and applies them to all records in the data pipeline to the next operator
example, 
A =LOAD ‘input’ as (emp_name :charrarray, emp_id : long, emp_add : chararray, phone : chararray, preferences : map [] )
alias = FILTER alias BY expression;
It contains a predicate and it allows us to select which records will be retained in our data pipeline.

-------------------------------------------------
What is HCATALOG?
is a data and storage management layer which resides on top of HIVE metastore and provides a medium to PIG and MAP-REDUCE 
to access (read and write) data on HIVE system. 

https://stackoverflow.com/questions/22533814/what-is-use-of-hcatalog-in-hadoop
https://www.quora.com/What-is-HCatalog-in-Hadoop-and-how-is-it-used
https://www.tutorialspoint.com/hcatalog/hcatalog_introduction.htm         //for details

-------------------------------------------------
How to access Hive table inside a pig program?

Using pig, we can access Hive table and a HBase table as well.
For accessing Hive table, you have to use HCatalog while starting pig shell like pig -useHCatalog
Then with the help of HCataLoader(), you can load any Hive table into pig.

	A = LOAD ‘sample_07’ USING org.apache.hive.hcatalog.pig.HCataLoader();

Here sample_07 is table in hive. Below we are filtering the rows having salary>=4000.

	B = filter A by salary >= 4000;

Now using HCataStorer(), you can store any pig relation into a Hive table.

	STORE B into ‘HCatalog_table’ USING org.apache.hive.hcatalog.pig.HcatStorer();

All the schema of the records and the data types will be automatically preserved by the HCataLoader();

--------------------------------------------------------
How can we access HBase tables from pig?

To access the data in HBase, you have to use HbaseStorage just like pigStorage() to load the data from a file, we have got HBaseStorage() to load the data from HBase tables. 

Here you have to specify the column families and the column names that you wish to access from HBase. Optionally you can specify the schema also.

data = LOAD ‘hbase://employee’ USING org.apache.pig.backend.hadoop.hbase.HbaseStorage(‘personal:*, professional:*’,’-loadkey true’) as 
	   (id:CHARARRAY,personal:MAP[],professional:MAP[]);

DUMP data;

To access the data, you can use the following code:

result = FOREACH data GENERATE id, personal#’name’, professional#’exp’

Similarly, you can also store the pig relation into HBase. Suppose, if you want to store a file into HBase, you can use the following code:

T = LOAD ‘/home/acadgild/hbase/bulk_data.tsv’ AS (userid,name,exp);

STORE T into ‘bulk_pig’ USING org.apache.pig.backend.hadoop.hbase.HbaseStorage (‘cf1:name,cf1:exp’);

Here, whenever you are loading a file, the first field will go as the row key and the other column names will go as column families based on the schema you specify.

-------------------------------------------------------
