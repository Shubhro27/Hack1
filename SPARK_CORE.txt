Map-reduce disadvantages:
1) MR always follows fixed data pipeline. i.e. we cannot run reducer independently, every time we run reducer, the logi for mapper has to be preceded.
	i.e. the transformations made in the previous process cannot be used for a new process.
	
2) Not Flexible/limited parallel process. i.e. HDFS divides data into blocks ex, if a file is residing in 10 blocks,
	when a MR is initiated, there will be 10 mapper instances (one each block), but this cannot be changes (As it happens internally)
	(in spark ,data is divided into partitions and we can control the number of partitions. i.e. if we declare 20 partitions
	as there are 10 blocks, the total number of partitions will be 200 i.e. 10 * 20).

3) map reduce is bad for iterative algorithms. (in advanced analytics, 90% of algos are iterative).

Spark Advantages:
1) Transformations can be reused.
2) flexible control over parallel process (check 2nd point above).
3) Supports iterative algorithms.

SPARK is 100 times faster than MR when run in Memory (i.e. RAM)
SPARK is 10 times faster than MR when run in HDD.

SPARK can run on HADOOP, MESOS, AWS clusters.

Tez vs Spark: (both support DAG)
Spark is DAG + In Memory computing BUT Tez is DAG thus SPARK is faster as data cycles between processor and Memory/RAM
but In TEX, data cycle between processor <-> RAM <-> HDD


SAP Hana vs SPark (both support in memory computation)
Hana runs on server which is non-distributed but SPARK runs on distributed cluster.
so if we want to load 100 GB data in HANA Server (having 10 GB), it will not allow
but spark can load 100 GB by storing 10 GB data on each cluster machine (10 GB RAM) and process in parallel.

Pig vs Spark ()


NOTE:Hadoop Data is divided into blocks, Spark data is divided into partitions.
------
spark programming can be done in 2 ways:
1) spark shell: Spark has an interactive shell which can be accessed via 
				command "spark-shell" (for scala) and pyspark (for python).
				Spark can run on 4 language API's Scala, Python, R (can be accessed via shell) and Java (thru packaged jars).
				
				When we start a spark shell, usually it will show
				a) the spark version and the scala version running.
				b) master = [Local[*]] which implies spark is running in local mode and 
					"*" implies maximum number of processor cores to use.
					We can however control the number of cores we can use by following command,
					spark -shell --master local[1] //so OS will start shell only on 1 processor core.
					We can also set this in config file: /etc/spark/conf/spark-defaults.conf
				c) "Spark context available as sc" i.e. create object "sc" (invokes spark library).
				
		We can start spark shell in 2 modes
		a) local mode : by default i.e. spark-shell
		b) Yarn mode: to activate yarn mode, we will use the following command on OS
		   1.spark-shell --master yarn --deploy-mode client, in this mode, the driver will be running on 
			client machine / ur own machine (i.e. where the command was run).One problem is that if the machine
			shuts down, the entire application will go down.
		   2.spark-shell --master yarn --deploy-mode cluster, in this mode, the driver program will be running
				on one of the cluster machines (i.e. worker nodes). So even if user's PC goes down, it
				does not impact the application.
					
					
2) packaged JAR options via Maven or SBT.

-----
SPARK also has a UI: 
a) in cloudera VM, the transformations and actions, DAG can be viewed from the UI.
					open a browser and type ip address/port of cloudera machine.
					
	when starting spark-shell,the SPARK UI shows up in logs as,
		18/01/28 13:21:02 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
		18/01/28 13:21:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
		18/01/28 13:21:02 INFO SparkUI: Started SparkUI at http://172.31.20.58:4040
		
		error: Make sure the web address https://172.31.20.58:4040 is correct.
					
b) On cloudx lab,
	type in http://g.cloudxlab.com:4040 in your browser to access the Spark UI.
	Also, Spark History Server is located at http://b.cloudxlab.com:18080/

-----

SPARK components

SPARK GraphX, Spark Streaming (micro batch),MLlib (library for Machine learning),SPARK SQL (SQL context and Hive context)
^
|
SPARK Core (has DAG)
^
|
Hadoop, Mesos, AWS cluster

big data applications include,
a) online/realtime applications
	1) Transactional: i.e. recorded into RDBMS traditionally but now with so many transactions, these are handled with NoSQl DB.
	2) streaming: i.e. logging or collecting data (accomplished with flume, Kafka, Storm,  Trident).
b) Micro/mini/small Batching applications (Spark Streaming) i.e. realtime data fetching (like: in last 5 mins how many fraud attempts happned in different places)
	streaming is not good for live analytics (i.e. 1000 of transactions hitting at a time), but it is good for micro-analytics (i.e. near real time)
	i.e. why SS is used with KAFKA (which is best for live analytics)
c) Batch applications via Hadoop

RDD (rescielent distributed datasets) i.e. spark data objects.

RDD is subdivided into partitions which are distributed across different worker nodes' RAMs of spark/hadoop cluster.
Any transitions can be executed on all partitions is performed in parallel.

we can perform 2 types of operations over RDD
a Transformations
	1 operation over each element
	2 aggregated transformations
	3 filters
	example, map, flatmap, reduceByKey, groupByKey, filter, groupByKey.
	
b actions i.e. triggers a DAG and actually show result
	count, collect, take, saveAsTextFile etc
	
RDDs are not executed (loaded into RAM and computed) when they are declared, but when an action is performed. i.e. lazy Evaluation
3 ways to declare (i.e. NOT executed) RDDs
a) when u paralellize your local objects (i.e.resides in client machine)
	ex,
	val x = List(10,20,30,40,50)  //creates a local object i.e it is in client machine
	val y = sc.parallelize(x)     // distributes local object into SPARK CLUSTER i.e. it is a RDD. y has just one partition
	val z = sc.parallelize(x,2)   // Z is a RDD and has 2 partitions.
	
	
b) when u perform transformation or filter on RDD, result set will be a RDD
	val a = List(10,20,30,40,50)  //local object
	val b = a.filter(x => x > 30) // still a local object as we performed filter over a which is local object
	
	val m = List(10,20,30,40,50,60) //local object
	val r1 = sc.parallelize(m)      // creates a RDD
	val r2 = r1.filter( x => x > 30) // r2 is also a RDD
	val r3 = r1.map( x = x + 100 )   // r3 is also a RDD

c) when u read data from file using spark context (i.e. sc) i.e. reading data from scala will not create RDD, it has to be using spark context.
	val data = sc.textFile("/user/....../comments.txt")  //creates a RDD. Has 1 partition for each unique block
	val data = sc.textFile("/user/....../comments.txt",3) //each unique block is divided into 3 paritions distributed into RAM of different slaves.

What is spark context: client of Spark’s execution environment and it acts as the master of the Spark application.
					   The first step of any Spark driver application is to create a SparkContext. 
					   The SparkContext allows the Spark driver application to access the cluster through a resource manager. 
	
what is sc: variable created for the user for an interpreter aware SparkContext.
	
RDDs are executed when an action is triggered
val x = List(10,20,30,40,50,60)      //x  is local
val r1 = sc.parallelize(x)			//R1 is root RDD or Base RDD.Flow execution starts from this when action is triggered.
val r2 = r1.map(x => x + 100)
val r3 = r2.map(x => x * x)
val r4 = r3.filter ( x => x > 10000) // r1 thru r4 are RDDs, but these are still not stored in RAM as these are only declared
									 // r1 thru r4 are not yet distributed into SPARK cluster
									 //i.e data flow is created wiith sequence of RDDS
									 
r4.count							 //RDD will be executed with this action. i.e. data flow created with RDDs will be executed
									 //RDDs will be loaded to RAM and completed.
									 //action will be triggered from ROOT RDD.
									 

*** Partitions are created on the nodes where data is loaded, replicas are spared from partitioning.

STEPS followed, (this is all coordinated by SPARK CORE)
S-1: r1 being root RDD, no computation is required, once data is loaded into RAM, it waits for R2
S-2: once R2 is initiated in RAM, it reads data from R2 and loads into RAM and once computation is complete, R1 is removed and it waits for R3
S-3: r3 is initiated in RAM, it reads from R2 and once computation is complete, r2 is removed from RAM and r3 waits for r4
S-4: r4 is initiated in RAM, it reads from R3 and once computation is complete, r3 is removed from RAM
S-5: Action will be performed on R4 RDD. Once results are collected, R4 will be deleted from RAM.
S-6: RAM is empty. If the process has to be re-run, we will have to follow the above steps again.

------
RDD Reusability
consider an example where the data flow is 
a) R1 -> R2 -> R3 -> R4
b) R1 -> R2 -> R3 -> R5
c) R1 -> R2 -> R3 -> R6 -> R7

i.e. all R's are RDDs and R1 thru R3 are being used again and again for all flows
so we can keep R1 and R3 into persist mode i.e. they will be retained in the RAM even after 
next RDD of flow is ready (i.e. R4, R5, ..) and flow execution is complete.

NOTE: when RDD is 1st time loaded into RAM and computed, it will be persisted/cached. 
	  Explicit persists is done when flow is executed i.e action is performed.
	  After action, if we try to persist a RDD, it will not be done. When action is performed, DAG looks for caching requests.

a rdd can be cached or persisted via,
<RDD Name>.persist or <RDD Name>.cache  //Persist comes with other options.
recap, a RDD will be persisted only when an action is called (i.e. when data is loaded and computed)

more on cached vs persisted,
cached only operates in MEMORY-ONLY mode (this is the only option) i.e. the RDDs are kept in cluster memory as java objects
if some partittions are not cached due to insufficient cluster memory, such partitions are re-calculated realtime

Persisted by default operates on MEMORY-ONLY mode, but there are more modes available as,
a) MEMORY_AND_DISK: if RDD does not fit in memory, then store the partitions on the disk.
b) MEMORY_ONLY_SER: stores 1 byte array per partition, some partitions may not be cached and calculated realtime.
c) MEMORY_ONLY_DISK_SER: disk is used when memory is not sufficient.
d) DISC_ONLY: stores RDD only on the disk.
e) MEMORY_ONLY_2,MEMORY_AND_DISK_2: partitions are replicated on 2 worker nodes.

NOTE: once we exit the spark shell, the persisted RDD's are also deleted from memory.

caching paradigms:
ex-1,												ex-2,
val R1 = sc.textfile(----)							val R1 = sc.textfile(----)
val R2 = R1.----()									val R2 = R1.----()
val R3 = R2.----()									val R3 = R2.----()
R3.persist											val R4 = R3.---()
val R4 = R3.---()									val R5 = R3.---()
val R5 = R3.---()									R3.persist
R4.count											R4.count
R5.count											R5.count

whenever a action is triggered, only then R3 will be persisted. So there is no difference between ex-1 and ex-2.

Consider a scenario,
val R1 = sc.textfile(----)	
val R2 = R1.----()			
val R3 = R2.----()			
val R4 = R3.---()			
val R5 = R3.---()			
R4.count    //during flow execution, no RDD is persisted as no RDD has been declared as persist.					
R5.count					
R3.persist // R3 is just declared as persist but R3 is not persisted in cluster as action has already been triggered

if now,
R4.count  //now R3 will be persisted as action is now triggered.


How fault tolerence is applied during flow execution,
Case-1: R1 -> R2 -> R3 -> R4
		R4.count , if during computation of R3, one of the partitions on which R3 reside is down,
		all R3 partitions will be deallocated (i.e. removed) from current slaves and reallocated to other slaves
		and then again R3 is computed i.e. execution starts from R2.
		
		i.e. during flow execution, if any partition of current RDD is down, all partitions will be deallocated
		and reallocated in other working slaves and then the data will be read from previous RDD.
		
Case-2: R1 -> R2 -> R3 -> R4
		R4.count, if during R3 computation, R2 goes down, then computation will again starts from R1.
		
		i.e. In flow execution, during current RDD process if previous RDD is down, entire flow will be re-executed.
		
Case-3: R1 -> R2 -> R3 -> R3 is declated as persist -> R4 -> R5
		R5.count, if R4 is down, the processing re-starts from R3 (i.e. immediate available source)
		
		i.e. during current RDD process, if previous RDD is down, 
		flow execution will be re-started from immediate available source (i.e persisted RDD).
		if no RDD is persisted, flow restarts from beginning.
		
		
Consider a scenario,
R1 -> R2 -> R3 -> ........... -> R97 -> R98 -> R99 -> R100 

i.e. 100 RDDs and this flow needs to run on 1st of every month (i.e. montly job)
In such scenario, we will not want to persist anything as we do not want to reuse.

however, lets say, in 1st execution (R100.count)
1) during R97 execution, R96 goes down, so the flow will again start from R1
2) in the 2nd run, let's assume during R98 execution, R97 goes down, so the flow will again start from R1

to handle these situations, as a temporary solution, if we wo intermediate persist like,
R5.persist, R10.persist, ......, R95.persist

now, if during R97, if R96 goes down, flow restarts from R96 taking data from R95.
let's assume, no R100.count completed successfully, and we do not want the persisted RDD's to uncache for 2nd run next month

i.e to unpersist or uncache we use <RDD name>. unpersist() i.e. example, R95.unpersist()

Note: before unpersisting, SPARK core checks if any flow requiring R95 is running and if it does SPARK core waits for 
flow to complete before unpersisting (i.e. removing it from RAM).

-------------------------------------------------------------------------------
How to ignore information logs from displaying on the console,
sc.setLogLevel("ERROR") //will only display error messages ignoring other warnings and information.
						// we can add dierrent logging levels like INFO, DEBUG etc.
						
						
How to read file from local file system (i.e. linux file system) if we are logged in to spark-shell:
append the complete path by : "file:///"
val r1 = sc.textFile("file:///home/shubhro2705854012/sparkLocal/tryScalaTrim.txt")

------------------------------------------------------------------------------
SPARK PROGRAMMING
------------------------------------------------------------------------------
JOIN:
//consider a List of tuples
val R1 = List((11,10000),(11,20000),(12,30000),(12,40000),(13,5000))
val R2 = List((11,"Hyd"),(12,"Del"),(13,"Hyd"))

val rdd1 = sc.parallelize(R1)
val rdd2 = sc.parallelize(R2)
val join_RDD = rdd1.join(rdd2)  //internal join
join_RDD.collect.foreach(println)

o/p: if we have not explicitely specified the key, S Core assumes 1st element as Key
(13,(5000,Hyd))
(11,(10000,Hyd))
(11,(20000,Hyd))
(12,(30000,Del))
(12,(40000,Del))

//to do city based aggregation,
val citySalPair = join_RDD.map { x => 
				  val city = x._2._2
				  val sal = x._2._1
				  (city,sal)
				  }
				  
citySalPair.collect.foreach(println)

o/p:
(Hyd,5000)
(Hyd,10000)
(Hyd,20000)
(Del,30000)
(Del,40000)

val result = citySalPair.reduceByKey(_+_)

result.collect.foreach(println)
o/p:
(Hyd,35000)
(Del,70000)
----

//consider a scenario where one object is not in key value pair
val e1 = List((11,30000,10000),(11,40000,20000),(12,50000,30000),(13,60000,20000),(12,80000,30000))
val ee = sc.parallelize(e1)  //create RDD for local object

val R2 = List((11,"Hyd"),(12,"Del"),(13,"Hyd"))
val rdd2 = sc.parallelize(R2)

val join2 = ee.join(RDD2) //this will error : Join is not a member of RDD(Int,Int,Int)

so NOTE: joins can be applied only on KEY_VALUE PAIRS.

val e3 = ee.map { x => 
			val dno = x._1
			val sal = x._2
			val bonus = x._3
			(dno,(sal,bonus))
		}

e3.collect.foreach(println)

o/p:
(11,(30000,10000))
(11,(40000,20000))
(12,(50000,30000))
(13,(60000,20000))
(12,(80000,30000))

val join3 = e3.join(rdd2)

o/p:
(11,(30000,10000),Hyd))  -> NOTE, this is like (11,[(30000,10000),Hyd)]) where [] is a single tuple.
(11,(40000,20000),Hyd))
(12,(50000,30000),Del))
(13,(60000,20000),Del))
(12,(80000,30000),Del))

val pair = j3.map { x =>
			val sal = x._2._1._1
			val bonus = x._2._1._2
			val total = sal + bonus 
			val city = x._2._2
			(city,total)
			}
			
o/p will be a collection of tuple

val res2 = pair.reduceByKey(_+_)

-----------
reading files and creating RDD:

consider 2 files emp and dept

emp:
101,aaa,70000,m,12
102,bbb,90000,f,12
103,ccc,10000,m,11
104,ddd,20000,f,13
105,eee,90000,f,14
106,fff,40000,m,15

dept:
11,marketing,hyd
12,HR,del
13,finance,hyd
14,admin,del
15,accounts,hyd
.
.

we need the o/p as : 105,eee,40000,m,accounts,hyd i.e. join the data and write to HDFS
--
hadoop dfs -mkdir Sparks				 //create a directory in HDFS
hadoop dfs -copyFromLocal emp Sparks	 // copy file emp to HDFS
hadoop dfs -copyFromLocal dept Sparks	 //copy file dept to HDFS

--
val rdd_emp = sc.textFile("emp file")    //loads file to SPARK cluster as RDD
val rdd_dept = sc.textFile("dept file")	 //loads file to SPARK cluster as RDD

NOTE:
1) In order to read file from local directory, the following command is used
	sc.textFile("file:///<complete file path + name") i.e. file:/// will help read from local linux server.
	
2) In order to read file from HDFS directory, the following command can be used
	sc.textFile("<complete file path + name of the file in HDFS") //i.e. by default it tries to find the file in HDFS cluster.

--
rdd_emp.collect.foreach(println)  //data will be available as string
o/p:
101,aaa,70000,m,12
102,bbb,90000,f,12

rdd_dept.collect.foreach(println)
o/p:
11,marketing,hyd
12,HR,del
.
.
--
In order to join the RDDs should be in key-value pairs

val rdd_emp_v1 = rdd_emp.map{ x =>
				val w = x.split(",")
				val dno = w(4).toInt
				val id = w(0)
				val name = w(1)
				val sal = w(2).toInt
				val sex = w(3)
				val info = id + "," + name + "," + sal + "," + sex
				(dno,info)
				}
				
val rdd_dept_v1 = rdd_dept.map { x =>
				val w = x.split(",")
				val dno = w(0).toInt
				val info = w(1) + "," + w(2)
				(dno,info)
				}
				
val rdd_join_emp_dept = rdd_emp_v1.join(rdd_dept_v1)
(12,(101,aaa,70000,m,HR,del))      //altough it seems to be 1 string, it has 2 parts (101,aaa,70000,m),(HR,del)
(11,(103,ccc,10000,m,marketing,hyd))
.
.
.

val res1 = rdd_join_emp_dept.map{ x => 
					val einfo = x._2_.1
					val dinfo = x._2_.2
					val info = einfo + "," + dinfo
					info
					}
					
o/p:
101,aaa,70000,m,HR,del
103,ccc,10000,m,marketing,hyd
.
.

--
store it in HDFS

res1.saveAsTextFile("HDFS location")  //give path + file name (by which the data will be stored)

//if we want to aggregate i.e. total salary by city

val ednosal = rdd_emp.map { x =>
				val w = x.split(",")
				val dno = w(4)
				val sal = w(2).toInt
				(dno,sal)
				}
				
val dnocity = rdd_dept.map { x =>
				val w = x.split(",")
				val dno = w(0)
				val city = w(2)
				(dno,city)
				}
				
val edjoin = ednosal.join(dnocity)
(14,(90000,del))
(15,(90000,hyd))
.
.
.

val citysal = edjoin.map { x =>
				val city = x._2_._2
				val sal = x._2._1
				(city,sal)
				}
				
o/p:
(del,90000)
(hyd,90000)

val res4 = citysal.reduceByKey(_+_)

-----------
how to create and view number of partitions
example,
val list1 = List(10,20,30,40,50,60,70,80)
val rdd_list1 = sc.parallelize(list1)  
rdd_list1.partition.size  //o/p is 1 which is default number of partitions

val rdd2_list1 = sc.parallelize(list1,3)
rdd2_list1.partition.size //o/p is 3

val data = sc.textFile("filename",2)
data.partition.size   //o/p is 2

NOTE: the partition size value is NOT = the argument given in spark context.

f1 = B1 B2 B3 (i.e. file has 3 blocks and 9 replicas)
when we say val data = sc.textFile(f1) , it creates 3 partitions
i.e. BY DEFAULT, no of file blocks (original) = no of partitions created
 
val data2 = sc.textFile(f1,2) , it creates 6 partitions i.e. 3 blocks and 2 partition on each file block

Partitions are created in RAM, it may be on same cluster machine or different cluster machines depending on RAM available.

consider a scenario,
if we have a single core machine and we create 2 partitions, there is no advantage as P1 will be processed before P2
but if we have multi core processors, each core can work on each partition in parallel

Industry standard for slave machines is minimum 64 core i.e. 64 parallel tasks can be initiated.
-----------
word count demo via spark programming:

S1 - val data = sc.textFile("file path + filename")
S2 - val arr = data.map ( x => x.split(" ") )
S3 - val words = arr.flatMap(x => x)   //all the arrays in arr will be flattened

shortcut for S2 and S3 is
val words = data.flatMap( x => x.split(" "))   //same result as S3.

S4 - val pair = words.map( x => (x,1))   // Array((I,1), (spark,1),.........)

S5 - val result = pair.reduceByKey((a,b)=> a + b)  // word count output

shortcut for S5,
val result = pair.reduceByKey(_+_)  

S6 - To get word with highest word count, we can do
val max_cnt = pair.reduceByKey(Math.max(_,_))   //for min : Math.min(_,_)
OR
val max_cnt = pair.reduceByKey((x,y) => max(x,y))

NOTE: to use max or min, import scala.math.max

#word count in a singleline
val data = sc.textFile("file path + filename")
val wc = data.flatMap ( x => x.split(" ")).map(x => (x,1)).reduceByKey(_+_)

However, this is not recommended, as the intermediate RDDs (created in long notation) cannot be reused if required.

#another way of word count
val wc= data.map { x =>
			val arr = x.split(" ")
			val p = arr.map (w => (w,1))
			p
			}   //o/p will be Array(Array[string,Int])
			
val wc= data.map { x =>
			val arr = x.split(" ")
			val p = arr.map (w => (w,1))
			p
			}.flatMap(x => x)
			
val cnt = wc.reduceByKey(_+_)  //give the count.

#another way
val wc1 = data.flatMap {x =>
				val arr = x.split(" ")
				val p = arr.map (w => (w,1))
				p
			}.reduceByKey(_+_)



# let's consider a senario where the words are separated by multiple spaces 
spark			Spark 						Spark
hadoop	spark 	Hadoop HADOOP
Hadoop						HADOOP

develop a function to neutralize the spaces
def rmSpace (x: String) : String = {
	val line = x.trim()    //to remove unnecessary space before and after the string
	val w = line.split(" ")
	val words = w.filter ( x => x!= " ").map (x => x.toLowerCase)
	words.mkString(" ")   // as words is an Array
}

val lines = sc.textFile ("file path + file name")
val data = lines.map (x => rmSpace(x))
val arr = data.flatMap {x =>
				val arr = x.split(" ")
				val p = arr.map (w => (w,1))
				p
			}.reduceByKey(_+_)
			
-----------
How to find maximum value for a number of keys: use maxBy, but maxBy if used by itself on an array may fail, it should be used with "collect" action,
example (consider word count):
val r1 = sc.textFile("/user/shubhro2705854012/wordCount/wordCountFile.txt")  //reads from HDFS

val r2 = r1.map { x => 
val arr = x.split(" ")   //will split the lines based on SPACE
val mp = arr.map( w => (w,1))  //will form key,value pair as (word, 1)
mp
}.flatMap( y => y)    //will neutralize an iteration so o/p will be array((word1,1),(word2,1)....) instead of array(array(word1,1),array(word2,1)....) 

val r3 = r2.reduceByKey(_+_)   //summation

r3.collect.maxBy(_._2)   //will provide the (key, value) pair by maximum value for a key. _._2 is the 2nd part i.e. value in the key/Value pair.

NOTE: the same applies in case we want to find the minimum value.

-----------
grouping on multiple columns:
example,
emp file,
101,aaa,70000,m,12
102,bbb,90000,f,12
103,ccc,10000,m,11
104,ddd,20000,f,13
105,eee,90000,f,14
106,fff,40000,m,15

what to accomplish: select dno, sex, sum(sal) from emp group by sex;

val data = sc.textFile(" file ")
val pair = data.map { x => 
			val arr = x.split(",")
			dno = arr(4)
			sex = arr(3)
			sal = arr(2)
			val myKey = (dno,sex)
			(myKey,sal)
			}
			
((12,m),70000)
((12,f),90000)
.
.
.
i.e. now we have a key,value pair where key is a tuple.

val result = pair.reduceByKey((a,b) => a + b) OR val result = pair.reduceByKey(_+_)

----
collect action command collects the data of RDD from all the partitions into client

NOTE: when data is loaded into client, there can be scenarios, where collect operation fails
example: data for RDD is stored in 4 partitions and each partition holds 10 GB
when collect is triggered, it loads the data into client machine (which has only 20 GB memory), the operation will fail.

in such case when collect fails, we can check data as,
data.take(3)  //will load the 1st 3 rows.
data.skip(3)  //skips 1st 3 recs.
data.first    //print 1st line.

------
Sorting: can be done by sortBy clause.

sortBy (_._1) //will sort by key 
sortBy (_._2) //will sort by value.
	where
	1st _ is element in "RDD(key,value)"
	2nd _ is element within the "(key,Value)"
		i.e. _._1 is key
			 _._2 is value
			 
	to sort descending, sortBy (_._1, false) //will sort in descending order.

------
Save results to HDFS

<RDD>.saveAsTextFile ("complete HDFS path + filename")

Note:
val loc_list = List(10,20,30,40,50,60,70,90,100)
val rdd_list = sc.parallelize(loc_list)
rdd_list.saveAsTextFile("HDFS PATH/FILE1") //o/p can be seen in FILE1/part-00000

now,
val rdd_list2 = sc.parallelize(loc_list,3)
rdd_list.saveAsTextFile("HDFS PATH/FILE2") //o/p will be stored in part-00000, part-00001, part-00002 i.e. 1 file for each partition  

if we want to create one file for the 3 partitions above,
val rdd_list2 = sc.parallelize(loc_list,3)
sc.parallelize(rdd_list2.collect).saveAsTextFile("...../FILE3")   //sc.parallelize(rdd_list2.collect) will create is RDD
																  //o/p will be stored in part-00000

-------
to convert a tuple into String
val res_str = res_tuple.map (x => x._1 + "\t" + x._2)  //converts () to tab delimited string ex (hyd,2000) -> (hyd	2000)

NOTE: In SPARK, we cannot convert Tuples directly to List....tuples will have to be converted to string and then added to list1

val data = sc.textFile(" file ")
val pair = data.map { x => 
			val arr = x.split(",")
			dno = arr(4)
			sex = arr(3)
			sal = arr(2)
			val myKey = (dno,sex)
			(myKey,sal)
			}
			
val res3 = pair.reduceByKey(_+_)
			
((12,m),70000)
((12,f),90000)

to convert this data to 
12	m	70000
12	f	90000

val r4 = res4.map(x => x._1._1 + "\t" + x._1._2 + "\t" + x._2)
o/p:
12	m	70000
12	f	90000

-----------------------------------------------------
aggregations on multiple columns using groupByKey:

in case of previous aggregations (reduceByKey), we could only achieve one aggregation...in order to do multiple aggregations
we use groupByKey.

why reduceByKey is better than groupByKey?

emp:
101,aaa,70000,m,12
102,bbb,90000,f,12
103,ccc,10000,m,11
104,ddd,20000,f,13
105,eee,90000,f,14
106,fff,40000,m,15

dept:
11,marketing,hyd
12,HR,del
13,finance,hyd
14,admin,del
15,accounts,hyd

val data = sc.textFile("emp file")
val arr = data.map (x => x.split(","))
val pair1 = arr.map( x => (x(3),x(2).toInt)) // Array ((m,70000),(f,90000),......)
val grp = pair.groupByKey()

o/p: i.e. grp.collect
Array((f,CompactBuffer(90000,20000,90000)),(m,CompactBuffer(70000,10000,40000)))

val res = grp.map { x => 
			val sex = x._1
			val cb = x._2
			val total = cb.sum
			val count = cb.size
			val avg = total/count
			val max = cb.max
			val min = cb.min
			val r = sex + "," + total + "," + count + "," + avg + "," + max + "," + min
			r}
 
res.collect.foreach(println)

--
val pair2 = arr.map( x => ((x(4),x(3)),x(2)))  // Array (((11,m),50000),((11,f),10000))
val grp2 = pair2.groupByKey() // Array (((11,m),CompactBuffer(50000)),((11,f),CompactBuffer(50000,60000)),.....)
val res2 = grp2.map { x => 
			val k = x._1
			val dno = k._1
			val sex = k._2
			val cb = x._2
			val total = cb.sum
			val count = cb.size
			val avg = total/count
			val max = cb.max
			val min = cb.min
			(dno,sex,total,count,avg,max,min) 
			}

--			
//select sum(sal) from emp

val salary = arr.map ( x => x(2).toInt )
val tot = salary.sum
OR
val tot = salary.reduce(_+_)

NOTE: reduceByKey only works on key,value pair...while reduce can work on single data too.

//select sum(sal),avg(sal),count(*)....
val salary = arr.map ( x => x(2).toInt )
val tot = salary.sum
val cnt = salary.count
val avg = (tot/cnt).toInt
val max = salary.max
val min = salary.min

better way is :
val salary = arr.map ( x => x(2).toInt )
val tot = salary.reduce(_+_)
val cnt = salary.count
val avg = (tot/cnt).toInt
val max = salary.reduce( Math.max(_+_))
val min = salary.min( Math.min(_+_))

val res = (tot,cnt,avg,max,min)
val res2 = List(tot,cnt,avg,max,min).mkString("\t")

why is 2nd approach the better way,
consider an example,
val lst = sc.parallelize(List(10,20,30,40,50,20,30,40,40,10),2)
this creates a RDD having 2 partitions,
P1 = List (10,20,30,40,50)
P2 = List (20,30,40,40,10)

lst.sum //will load the entire list to client machine where the sum operation will take place.
		// incase of bigger datasets, this becomes a problem if clinet machine's ram cannot take the data.
		//also, due to this approach, this does not support parallel processing
		
lst.reduce(_+_) //operation will be executed at cluster machines separately i.e. separate for P1 and P2
				//P1 result will be 150 , P2 result will be 140 => will be collected into any one slave of spark 
				//cluster which has ram and aggregated  to give final result i.e. List(150,140) = 290
				//290 will now be loaded to client.
		

--------------
Spark transformations:

1) val res = data.map { x =>
			 val w = x.trim.split(",")
			 val id = w(0)
			 val name = w(1).toLowerCase
			 val fc = name.slice(0,1).toUpperCase
			 val rc = name.slice(1,name.size)
			 val sal = w(2).toInt
			 val grade = if (sal > 70000) "A" else
							if (sal >50000) "B" else
								if (sal > 30000) "C"
			 val dno = w(4).toInt
			 val dname = dno match {
				case 11 => "Marketing"
				case 12 => "HR"
				case other => "Others"}
			var sex = w(3).toLowerCase
			sex = if (sex == "f") "Female" else "male"
			List (id,fc+rc,w(2),grade,sex,dname).mkString("\t")
			}
2) filter operation
	//select sum(sal) from emp where sex = "m"
	
	def isMale(x:String) : Boolean = {
		val w = x.split(",")
		val sex = w(3).toLowerCase
		sex == "m"
	}
	
	val males = data.filter( x => isMale(x))
	val sales = males.map (x => x.split(",")(2).toInt)
	val res = sales.reduce(_+_) //sum of the salaries of males
	
	val fems = data.filter ( x => !isMale(x))
	val sales_fems = males.map (x => x.split(",")(2).toInt)
	val res = sales_fems.reduce(_+_) //sum of the salaries of females
	val max_sal = sales_fems.reduce(Math.max(_,_))  //max salary of female group
	
	NOTE: all scala functions CANNOT BE applied on RDD. 
	ex, 
	reduce is available in scala and RDD, but reduceByKey is available only for RDD
	count is for RDD and size for scala,
	but withinmap and flatMap, we can apply all scala functions.
	
--------------------------
Merging RDDs:
val L1 = List(10,20,40,50,50,30,40)
val L2 = List(50,60,70)

val RDD1 = sc.parallelize(L1)
val RDD2 = sc.parallelize(L2)

val UN_RDD = RDD1.union(RDD2) //Array(10,20,40,50,50,30,40,50,60,70)

NOTE: This is UnionAll where even the duplicates are merged.

val RDD3 = sc.parallelize(List(90,100,40))
val UN2_RDD = RDD1.union(RDD2,RDD3) //THIS will error
val UN2_RDD = RDD1.union(RDD2).union(RDD3)

other wayto merge
val MER_RDD = RDD1 ++ RDD2 ++ RDD3 //will also merge 3 RDDs, this is available in scala too. Union is not available in scala.

How about having only distinct values from union?
val L1 = List(10,20,40,50,50,30,40)
val L2 = List(50,60,70)

val RDD1 = sc.parallelize(L1)
val RDD2 = sc.parallelize(L2)

val UN_RDD = RDD1.union(RDD2) //Array(10,20,40,50,50,30,40,50,60,70)

val dist_UN_RDD = UN_RDD.distinct //Array(10,20,40,50,30,60,70)

--
val x = sc.parallelize(List("A","B","c","D"))
val y = sc.parallelize(List("A","c","M","N"))

val z = x ++ y //Array(A,B,c,D,A,c,M,N)
z.distinct.collect //Array(B,N,D,M,A,c)

----------
Cross joins: each element of left side RDD will join with each element of Right side RDD.
can be accomplished via cartesian.

ex,
val pair1 = sc.parallelize(Array(("p1",10000),("p2",10000),("p1",20000),("p2",50000),("p3",60000))
val pair2 = sc.parallelize(Array(("p1",20000),("p2",50000),("p1",10000))

val cr = pair1.cartesian(pair2)  //there will be 15 elements i.e. tuples

cr.collect.foreach(println)

o/p:
("p1",10000),("p1",20000)
("p1",10000),("p2",50000)
("p1",10000),("p1",10000)
("p2",10000),("p1",20000)
("p2",10000),("p2",50000)
("p2",10000),("p1",10000)
("p1",20000),("p1",20000)
("p1",20000),("p2",50000)
("p1",20000),("p1",10000)
("p2",50000),("p1",20000)
("p2",50000),("p2",50000)
("p2",50000),("p1",10000)
("p3",60000),("p1",20000)
("p3",60000),("p2",50000)
("p3",60000),("p1",10000)
			
ex2,
val RDD1 = sc.parallelize(List(10,20,30,40))
val RDD2 = sc.parallelize(List(100,200))

val cart = RDD1.cartesian(RDD2)  //this will have 8 elements, each is a tuple

ex3,
emp:
101,aaa,70000,m,12
102,bbb,90000,f,12
103,ccc,10000,m,11
104,ddd,20000,f,13
105,eee,90000,f,14
106,fff,40000,m,15

dept:
11,marketing,hyd
12,HR,del
13,finance,hyd
14,admin,del
15,accounts,hyd

val data = sc.textFile ("emp")
val dpair = data.map{ x => 
			val w = x.split(",")
			val dno= w(4)
			val sal = w(2).toInt
			(dno,sal)
			}
//for each dept, calculate total salary
val dres = dpair.reduceByKey(_+_)

val dres2 = dres //copy

val res = dres.cartesian(dres2)

o/p:
((12,26000),(12,26000))	
((12,26000),(13,22000))
((12,26000),(11,14000))
((13,22000),(12,26000))
((13,22000),(13,22000))
((13,22000),(11,14000))
((11,14000),(12,26000))
((11,14000),(13,22000))
((11,14000),(11,14000))	

val cr2 = cr.map { x => 
		val t1 = x._1
		val t2 = x._2
		val dno1 =t1._1
		val tot1 = t1._2
		val dno2 =t2._1
		val tot2 = t2._2
		(dno1,dno2,tot1,tot2)
		}

i.e cartesian can be used for compare data between rows by makeing a copy of the data.

-------
sample transformation (very much used by data scientists):
val a = sc.parallelize (1 to 1000)
a.sample(true, 0.1,4).collect //takes sample from the original data
                               //1st par (true/false). true -> allow duplicate values i.e. repetation allowed ; 
							   //					   false -> Do not allw duplicate values i.e. repeatation NOT allowed.
							   //2nd par - how much fraction of data i.e. size of the sample
							   //3rd par - randoom seed i.e. number to pick random numbers for the sample.
							   //          its random sample, no gurranty u can see the same number in next run.
							   
example,
val a = sc.parallelize (1 to 1000)
a.sample(true, 0.1,4).collect

o/p: 
Array(13, 27, 41, 49, 55, 70, 76, 83,.,.......270,270........)

example 2,
val a = sc.parallelize (1 to 1000)
a.sample(false, 0.1,4).collect
							   		
o/p:
Array(12, 31, 39, 53, 78, 99, 113, 114, 118, 138, 151, 155......)

--------------------------
Partitioning (Increasing or Decreasing) in SPARK

1) Increase the number of partitions:
	repartition(<no of partitions>)   //this is the ONLY WAY

	Example,
	val a = sc.parallelize(1 to 100)
	a.forEachPartition ( x => Println(x.toList)) // to view partition and see the members
	val b = a.repartition(10)

	benefit of increasing the number of partitions is that it can allow multiple partitions to be processed in parallel.

2) Decrease the number of partitions:
	a) Repartition():
		val a = sc.parallelize (1 to 1000,10)
		val b = a.repartition(3)
		
	b) Coalesce():
		val a = sc.parallelize (1 to 1000,10)
		val b = a.coalesce(3)

	NOTE: In order to reduce the number of partitions, always use "coalesce" coz, for repartition, 
		  Spark will internally shuffle the whole data in the entire cluster, 
		  but coalesce will minimize data movement i.e. it will retain 5 partitions and add the data from other 5 
		  to these retained 5 partitions.
		  
		  
<rdd>.partitions.length to get the number of partitions.


Question) if i reduce partition from 10 to 5, doesnt it happen that we get an error require partition 
			more than 5 according to data size? 
Answer)   no, your partition size gets adjusted until u hit the maximum memory available limit

NOTE: data within partitions is distributed equally.
	
---------------------------------------------------------
mapValues:
when you just want to transform the values and keep the keys as-is, it's recommended to use mapValues.
mapValues is only applicable for PairRDDs, meaning RDDs of the form RDD[(A, B)]
operates on the value only (i.e. the second part of the tuple).

consider an example to calculate the average of following,

val inputrdd = sc.parallelize(Seq(("maths", 50), ("maths", 60), ("english", 65)))
val mapped = inputrdd.mapValues(mark => (mark, 1));  
mapped.collect    //Array((maths,(50,1)), (maths,(60,1)), (english,(65,1)))
val reduced = mapped.reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)) 
reduced.collect	 //Array((english,(65,1)), (maths,(110,2)))
val average = reduced.mapValues( x => x._1/x._2)
average.collect  //Array((english,65), (maths,55))

this can also be done by,
val average = reduced.map { x =>
                           val temp = x._2
                           val total = temp._1
                           val count = temp._2
                           (x._1, total / count)
                           }

average.collect() //Array((english,65), (maths,55))

example 2,
sb_sparkCore_assgmnt_ds/fakefriends.csv:
0,Will,33,385
1,Jean-Luc,26,2
2,Hugh,55,221
3,Deanna,40,465
4,Quark,68,21

val rdd_friends = sc.textFile("sb_sparkCore_assgmnt_ds/fakefriends.csv")
val rdd_friends_split = rdd_friends.map { x =>
						val arr = x.split(",")
						val age = arr(2).toInt
						val num = arr(3).toInt
						(age,num)
						}
val rdd_friends_calc = rdd_friends_split.mapValues(x => (x,1))  // Array((33,(385,1)), (26,(2,1)),....)

val rdd_friends_collect = rdd_friends_calc.reduceByKey((x,y) => (x._1 + y._1,x._2 + y._2)) //Array((34,(1473,6)), (52,(3747,11)), (56,(1840,6))...

val rdd_friends_avg = rdd_friends_collect.mapValues( x => x._1/x._2) //Array((34,245), (52,340), (56,306), (66,276)

other way to calculate,
val rdd_friends = sc.textFile("sb_sparkCore_assgmnt_ds/fakefriends.csv")
val rdd_friends_split = rdd_friends.map { x =>
						val arr = x.split(",")
						val age = arr(2).toInt
						val num = arr(3).toInt
						(age,num)
						}

val rdd_friends_grp = rdd_friends_split.groupByKey()
o/p: Array((34,CompactBuffer(221, 319, 116, 346, 48, 423)), (56,CompactBuffer(444, 354, 343, 15, 371, 313)),....)

val rdd_friends_avg1 = rdd_friends_grp.map { x => 
						val sum = x._2.sum
						val count = x._2.size
						val avg = sum/count
						(x._1,avg)
						}
o/p: Array((34,245), (56,306), (52,340), (66,276),.......)
-------------------------------
Broadcast variables: <http://www.sparktutorials.net/spark-broadcast-variables---what-are-they-and-how-do-i-use-them>
if we have a spark cluster and 3 machines in the cluster
m1 = have some block of data of file1
m2 = have some block of data of file1
m3 = have some block of data of file1
driver = working on a separate machine m5
 
when we create RDD, data will be in RAM of each machine i.e. m1,m2,m3

if we want to do lookUp table or join,
all the data will be brought to one machine, the join will be performed and sent to driver 
i.e it will consume a lot of n/w bandwidth.

other way to do it is via a broadcast variable
i.e. we can create a Broadcast variable in driver and 
ask driver to read data (in m1,m2,m3) and create a hashMap which will have reference of data on each cluster machines,
the BV can be sent to all cluster machines where executer runs.
This will save bandwidth as we do not have to shuffle the data across cluster

NOTE: BV resides in driver and is pushed from driver to executer machines.

However, files being used for broadcast variables is preferred to be <= 64 mb coz larger files will degrade the performance.

BV features,
1) Broadcast variables are designed to be shared throughout a cluster and, 
	at the same time have to be able to fit in memory on one machine.
	
2) broadcast variables are immutable, so they cannot be changed later on (in case take a look at accumulators).


Thus broadcast variables are: 
• Immutable
• Distributed to the cluster
• Fit in memory

broadcast variables are a great case for "static look up tables". i.e. small tables that might have some metadata about one of your tables

example,
checkin table might look like this: (consider, the data is huge) 
| UserId | Neighborhood | 
|---------+--------------| 
| 234 | 1 | 
| 567 | 2 | 
| 234 | 3 | 
| 532 | 2 |
.
.
.

neighborhoods table would look like: 
| NeighborhoodId | Name | 
|----------------+----------------| 
| 1 | Mission | 
| 2 | SOMA | 
| 3 | Sunset | 
| 4 | Haight Ashbury |

So performing a standard join is going to take forever due to shuffle. Instead, neighborhood table is going to be really quite small, 
the smarter thing to do is to ship around that small table to each node in the cluster.


how to create a broadcast table,
val hoods = Seq((1, "Mission"), (2, "SOMA"), (3, "Sunset"), (4, "Haight Ashbury"))
val checkins = Seq((234, 1),(567, 2), (234, 3), (532, 2), (234, 4))
val hoodsRdd = sc.parallelize(hoods)
val checkRdd = sc.parallelize(checkins)

val broadcastedHoods = sc.broadcast(hoodsRdd.collectAsMap()) //Map(2 -> SOMA, 4 -> Haight Ashbury, 1 -> Mission, 3 -> Sunset) will be broadcasted

val checkinsWithHoods = checkRdd.mapPartitions({row =>
 row.map(x => (x._1, x._2, broadcastedHoods.value.getOrElse(x._2, -1)))
}, preservesPartitioning = true)

o/p: Array((234,1,Mission), (567,2,SOMA), (234,3,Sunset), (532,2,SOMA), (234,4,Haight Ashbury))

---

reference: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-broadcast.html

BV allows programmer to keep a read-only variable cached on all worker node rather thn shipping a copy of it with tasks.

Driver (spark application) ----> sc.broadcast(m) ----> worker node1 |-> m -- (task1, task2 in executer)
                                    |----------------> worker node2 |-> m -- (task1, task2 in executer)
									|----------------> worker node3 |-> m -- (task1, task2 in executer)
									
BV is usually used when same data is required across multiple tasks.

to use BV in a stored transformation:
1) create BV with sc.broadcast.
2) use "value" method to access the shared value.

BV variables are faster as the value is copied to executers only once. If normal variable is used, it is copied each time
for multiple tasks.

Value method: is the only way to access the value of a broadcast variable.
example,
val b = sc.broadcast(1)
b.value  	//Int = 1
b.unpersist //Before destroying a broadcast variable, you may want to unpersist it.
b.destroy 	//When you are done with a broadcast variable, you should destroy it to release memory

NOTE: You can only access the value until it is destroyed after which you will see SparkException exception in the logs

BV used in place of static mapping:
example, ()
	val pws = Map("Apache Spark" -> "http://spark.apache.org/", "Scala" -> "http://www.scala-lang.org/")

	O/p: Map(Apache Spark -> http://spark.apache.org/, Scala -> http://www.scala-lang.org/)

	val websites = sc.parallelize(Seq("Apache Spark", "Scala")).map(pws).collect

	O/p: websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/)

here, the pws map variable will be send to the executers multiple times for each task requirring the variable.

	If used as BV,
	val pwsB = sc.broadcast(pws)
	val websites = sc.parallelize(Seq("Apache Spark", "Scala")).map(pwsB.value).collect
	// websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/)

	here the BV variable will be spawned to multiple executers, so that it can execute many tasks using pws map variable. 
	
BV as map-side join i.e. join using a map. Here is the smaller RDD is transformed to a map (done via collectAsMap) and broadcast
to join (or compute) with a bigger RDD.
example,
	checkin table might look like this: (consider, the data is huge) 
	| UserId | Neighborhood | 
	|---------+--------------| 
	| 234 | 1 | 
	| 567 | 2 | 
	| 234 | 3 | 
	| 532 | 2 |
	.
	.
	.

	neighborhoods table would look like: 
	| NeighborhoodId | Name | 
	|----------------+----------------| 
	| 1 | Mission | 
	| 2 | SOMA | 
	| 3 | Sunset | 
	| 4 | Haight Ashbury |
	
	val hoods = Seq((1, "Mission"), (2, "SOMA"), (3, "Sunset"), (4, "Haight Ashbury"))
	val checkins = Seq((234, 1),(567, 2), (234, 3), (532, 2), (234, 4))
	val hoodsRdd = sc.parallelize(hoods)
	val checkRdd = sc.parallelize(checkins)

	val broadcastedHoods = sc.broadcast(hoodsRdd.collectAsMap()) 
	i.e. //Map(2 -> SOMA, 4 -> Haight Ashbury, 1 -> Mission, 3 -> Sunset) will be broadcasted

	val checkinsWithHoods = checkRdd.mapPartitions({row =>
	 row.map(x => (x._1, x._2, broadcastedHoods.value.getOrElse(x._2, -1)))
	}, preservesPartitioning = true)

	o/p: Array((234,1,Mission), (567,2,SOMA), (234,3,Sunset), (532,2,SOMA), (234,4,Haight Ashbury))
	 
 
---------------------------------------------------
Scala getOrElse method: is applicable only on maps

example 1,
val m = Map("foo" -> Array(1, 2, 3))
def myDefault = {
  println("called-default")
  Array(4, 5, 6)
}
val a1 = m.getOrElse("foo", myDefault)  // myDefault not called
val a2 = m.getOrElse("bar", myDefault)  // myDefault called
val a3 = m.getOrElse("baz", myDefault)  // myDefault called
a2 == a3  // false!!

example 2,
val hoods = Seq((1, "Mission"), (2, "SOMA"), (3, "Sunset"), (4, "Haight Ashbury"))
val checkins = Seq((234, 1),(567, 2), (234, 3), (532, 2), (234, 4),(100,5))
val hoodsRdd = sc.parallelize(hoods)
val checkRdd = sc.parallelize(checkins)
val checkinsWithHoods = checkRdd.map(x => (x._1, x._2, broadcastedHoods.value.getOrElse(x._2, -1))

o/p: Array((234,1,Mission), (567,2,SOMA), (234,3,Sunset), (532,2,SOMA), (234,4,Haight Ashbury),(100,5,-1))


BV Assignment:

Running without using BV :

    val sc = new SparkContext("local[*]", "PopularMoviesNicer")  
    
    // Create a broadcast variable of our ID -> movie name map
    var nameDict = sc.broadcast(loadMovieNames)
    
    // Read in each rating line
    val lines = sc.textFile("../ml-100k/u.data")
    
    // Map to (movieID, 1) tuples
    val movies = lines.map(x => (x.split("\t")(1).toInt, 1))
    
    // Count up all the 1's for each movie
    val movieCounts = movies.reduceByKey( (x, y) => x + y )
    
    // Flip (movieID, count) to (count, movieID)
    val flipped = movieCounts.map( x => (x._2, x._1) )
    
    // Sort
    val sortedMovies = flipped.sortByKey()
    
    // Fold in the movie names from the broadcast variable
    val sortedMoviesWithNames = sortedMovies.map( x  => (nameDict.value(x._2), x._1) )
    
    // Collect and print results
    val results = sortedMoviesWithNames.collect()

Run using BV variable:
val rdd_data = sc.textFile("sb_sparkCore_assgmnt_ds/u.data")
val rdd_item = sc.textFile("sb_sparkCore_assgmnt_ds/u.item")

val rdd_item_split = rdd_item.map { x =>
val arr = x.split('|')
val mv_id = arr(0)
val mv_name = arr(1)
(mv_id,mv_name)
}

val nameDict = sc.broadcast(rdd_item_split.collectAsMap())
val movies = rdd_data.map(x => (x.split("\t")(1).toInt, 1))
val movieCounts = movies.reduceByKey( (x, y) => x + y )
val flipped = movieCounts.map( x => (x._2, x._1) )
val sortedMovies = flipped.sortByKey(false)
val sortedMoviesWithNames = sortedMovies.map( x  => (nameDict.value(x._2.toString), x._1) )

-------------
Scala BV and get method:

val flights = sc.parallelize(List(
("SEA", "JFK", "DL", "418",  "7:00"),
("SFO", "LAX", "AA", "1250", "7:05"),
("SFO", "JFK", "VX", "12",   "7:05"),
("JFK", "LAX", "DL", "424",  "7:10"),
("LAX", "SEA", "DL", "5737", "7:10")))

val airports = sc.parallelize(List(
("JFK", "John F. Kennedy International Airport", "New York", "NY"),
("LAX", "Los Angeles International Airport", "Los Angeles", "CA"),
("SEA", "Seattle-Tacoma International Airport", "Seattle", "WA"),
("SFO", "San Francisco International Airport", "San Francisco", "CA")))

val airlines = sc.parallelize(List(
("AA", "American Airlines"), 
("DL", "Delta Airlines"), 
("VX", "Virgin America")))

val air = airports.map{case(a, b, c, d) => (a, c)}
o/p: Array[(String, String)] = Array((JFK,New York), (LAX,Los Angeles), (SEA,Seattle), (SFO,San Francisco))

val air = airports.map{case(a, b, c, d) => (a, c)}
Array[(String, String)] = Array((JFK,New York), (LAX,Los Angeles), (SEA,Seattle), (SFO,San Francisco))

flights.map{case(a, b, c, d, e) => 
(airportsMap.value.get(a).get, 
airportsMap.value.get(b).get, 
airlinesMap.value.get(c).get, d, e)}.collect

O/p: 
res1: Array[(String, String, String, String, String)] = Array((Seattle,New York,Delta Airlines,418,7:00), 
(San Francisco,Los Angeles,American Airlines,1250,7:05), (San Francisco,New York,Virgin America,12,7:05), 
(New York,Los Angeles,Delta Airlines,424,7:10), (Los Angeles,Seattle,Delta Airlines,5737,7:10))

-------------
CHECK RDD Tranformations and Actions in same folder.
