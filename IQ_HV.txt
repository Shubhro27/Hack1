hdfs dfs -ls "/user/shubhro2705854012/hive_shubhro"
hdfs dfs -get "/user/shubhro2705854012/hive_shubhro/customer_information.txt"
------------------------
4000001,Kristina,Chung,55,Pilot
4000002,Paige,Chen,74,Teacher
4000003,Sherri,Melton,34,Firefighter
4000004,Gretchen,Hill,66,Computer hardware engineer
------------------------
CREATE DATABASE sb_hive_customer_info;
USE sb_hive_customer_info;

CREATE EXTERNAL TABLE sb_cus_info_unp(customer_id INT,first_name STRING,last_name STRING,customer_age INT,cust_prof STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

LOAD DATA INPATH '/user/shubhro2705854012/hive_shubhro/customer_information.txt' OVERWRITE INTO TABLE sb_cus_info_unp;

set hive.exec.dynamic.partition=true;       
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions = 1000;
set hive.exec.max.dynamic.partitions.pernode=100;

CREATE EXTERNAL TABLE sb_cus_info_par(customer_id INT,first_name STRING,last_name STRING,customer_age INT)
PARTITIONED BY (cust_prof STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

INSERT INTO sb_cus_info_par PARTITION(cust_prof) SELECT * FROM sb_cus_info_unp;

-------
Buck:
CREATE TABLE sb_cust_info_buck (customer_id INT,first_name STRING,last_name STRING,customer_age INT)
PARTITIONED BY (cust_prof STRING)
CLUSTERED BY (customer_age) INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

INSERT INTO sb_cust_info_buck PARTITION(cust_prof) SELECT * FROM sb_cus_info_unp;

--
CREATE TABLE sb_cust_info_buck2 (customer_id INT,first_name STRING,last_name STRING,customer_age INT,cust_prof STRING)
CLUSTERED BY (customer_age) INTO 10 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

INSERT INTO sb_cust_info_buck2 SELECT * FROM sb_cus_info_unp;

################################################################
1st link: https://www.dezyre.com/article/hive-interview-questions-and-answers-for-2018/246
2nd link: https://data-flair.training/blogs/apache-hive-interview-questions-2018/
3rd link: https://intellipaat.com/interview-question/hive-interview-questions/

------------------------------------------------------------------
Hive Architecture:

Hive Client [(Thrift API -> Thrift client) OR (JDBC API -> Hive JDBC Driver) OR (ODBC API -> Hive ODBC Driver)]
-> Hive Services [(CLI),(Hive Web Interface),(Hive server) -> Hive Driver -> metastore -> Derby Db]
-> Resource Manager [MapRV1, MapRV2, YARN, TEZ]
-> distributed Storage [HDFS]				

Is Hive suitable to be used for OLTP (online transaction processing) systems? 
No, it is not suitable for OLTP system since it does not offer insert and update at the row level.

Where does the data of a Hive table gets stored?
in cloudx lab, the DB will be created in /apps/hive/warehouse/, in cloudera it will be created in /hive/warehouse
However, we can explicitely provide location by configuring hive-site.xml property: hive.metastore.warehouse.dir 

----------------------------------------------------------------
How Hive distributes the rows into buckets?

Basically, hash_function depends on the column data type. 
for int data type, hash_function (bucketing_column) modulo (num_of_buckets)
for string and bucketing within partition, the hashing function is more complex.
----------------------------------------------------------------
In Dynamic partition, the data is read and partitioned thru the map-reduce job based on the field specified in the table to be used for partitioning.

Hive Metastore: stores metadata for Hive tables (like their schema and location) and partitions in a relational database.
                "It provides client access to this information by using metastore service API."
				
				Hive metastore consists of two fundamental units:
				1) metastore service to provide access to metastore to various hive services*.
				2) disk storage of hive metadata (i.e. not in HDFS)
				
				3 modes of Hive metastore deployment:
				1) Embedded Metastore. (DEFAULT)
				2) local Metastore.
				3) Remote Metastore.
				
				Embedded M: both metastore service and Hive service run in the same JVM by using an embedded Derby database.
				            Limitation of this mode is that only ONE hive session can be open at a time as only one embedded derby DB can access disk to get the metadata.
							
							the architecture will look like:
							drive (hive service JVM) -> Metastore -> Derby
							
				local M: this mode helps overcome the limited for embedded M by allowing multuple hive sessions i.e. multiple users can access metastore at the same time.
						 This is achieved by using a JDBC compliant DB like mySQL. In this model, mySQL runs on a different server (i.e. different machines) 
						 than (hive service+metastore service).
						 
						 Architecture will look like:
						 drive -> Metastore ->
                                             |
											 --> mySQL
											 |
						 drive -> Metastore ->
						 
				Remote M: in this mode (unlike embedded and local), metastore service runs on different JVM's than Hive service.
				          Mutiple metastore services can be configured so that different processes can communicate with metastore services via thrift network API.
						  Mutiple metastore servers help provide more availability and is more manageable.
						  to configure this mode, in hive-site.xml, change hive.metastore.uris property to point to the trift server location
						

				NOTE:
				When running Hive in embedded mode, it creates a local metastore. 
				When you run the query, it first checks whether a metastore already exists or not. 
				The property javax.jdo.option.ConnectionURL defined in the hive-site.xml has a default value jdbc: derby: databaseName=metastore_db; create=true.
				which implies that embedded derby will be used as the Hive metastore and the location of the metastore is metastore_db which will be created only if it does not 
				exist already.
				The location metastore_db is a relative location so when you run queries from different directories it gets created at all places from wherever you launch hive.
				This property can be altered in the hive-site.xml file to an absolute path.
				
Why does Hive not store metadata information in HDFS?
to achieve low latency we use RDBMS. Because  HDFS read/write operations are time-consuming processes.
				
---------
How will you read and write HDFS files in Hive?
i)   TextInputFormat- This class is used to read data in plain text file format.
ii)  HiveIgnoreKeyTextOutputFormat- This class is used to write data in plain text file format.
iii) SequenceFileInputFormat- This class is used to read data in hadoop SequenceFile format.
iv)  SequenceFileOutputFormat- This class is used to write data in hadoop SequenceFile format.
	
--------
Hive query processor: service which converts SQL to a graph of map reduce jobs so jobs can be executed in the order of dependencies.
components include:
1) parser
2) semantic analyzer
3) type checking
4) logical plan generation
5) optimizer
6) physical plan generation
7) execution engine
8) operators 	
9) UDF and UDAF

-------
"Describe" and "describe extended"...difference?

		Describe									Describe Extended
1		describe works for both						DE works only on tables
		DB and tables.
2		D when used on DB will give                 DE on DB fails with 
		the DB name, location, creator,             "ParseException line 1:18 cannot recognize input near 'DATABASE'"
3		D when used on table will give				DE when used on table will give table information (name, DB, owner, create time, last access)
		field names, field data types,				field schema, bucketing columns (if any), partition information, table type (managed/external)	
		partition information (if any)
		
NOTE: to see the tables within a DB, use SHOW TABLES
	  hive query to view all the databases whose name begins with “db”: SHOW DATABASES LIKE ‘db.*’

-------
How to run UNIX commands within Hive shell?
prepend the unix command with a "!". example, to know the current working directory in Unix, we can use "!pwd;"

------
Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive:
SORT BY – Data is ordered at each of ‘N’ reducers where the reducers can have overlapping range of data.

ORDER BY- This is similar to the SORT BY in SQL where total ordering of data takes place by passing it to a single reducer.
		  guarantees global ordering, but unacceptable for large datasets. You end up one sorted file as output.

DISTRIBUTE BY – It is used to distribute the rows among the reducers. Rows that have the same distribute by columns will go to the same reducer.
				example,
				SELECT col1, col2 FROM t1 DISTRIBUTE BY col1
				SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC				

CLUSTER BY- It is a combination of DISTRIBUTE BY and SORT BY where each of the N reducers gets non overlapping range of data which is then sorted by those ranges at the respective 
            reducers.This gives you global ordering. example,
			SELECT col1, col2 FROM t1 CLUSTER BY col1
			
------
explain explode() in Hive?

A lateral view with explode() can be used to convert collection(like array, map....) into separate rows using the query


I/p data: (explod_data)
Amar:"Btech","Mtech"
Amala:"Bsc","Msc","Mtech"
Akash:"Btech","Mba"

CREATE TABLE explod_tab(name STRING, degr array<STRING>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY ','
LINES TERMINATED BY '\n';

LOAD DATA LOCAL INPATH 'explod_data' OVERWRITE INTO TABLE explod_tab;

select * from explod_tab

	o/p:
	Amar    ["\"Btech\"","\"Mtech\""]
	Amala   ["\"Bsc\"","\"Msc\"","\"Mtech\""]
	Akash   ["\"Btech\"","\"Mba\""]

select name, myq from explod_tab lateral view explode(degr) q as myq;

	O/p:
	Amar    "Btech"
	Amar    "Mtech"
	Amala   "Bsc"
	Amala   "Msc"
	Amala   "Mtech"
	Akash   "Btech"
	Akash   "Mba"
	
----------
How can you prevent a large job from running for a long time?

This can be achieved by setting the MapReduce jobs to execute in strict mode set hive.mapred.mode=strict;
The strict mode ensures that the queries on partitioned tables cannot execute without defining a WHERE clause.

---------
Explain about the different types of join in Hive.
HiveQL has 4 different types of joins –
JOIN- Similar to Outer Join in SQL
FULL OUTER JOIN – Combines the records of both the left and right outer tables that fulfil the join condition.
LEFT OUTER JOIN- All the rows from the left table are returned even if there are no matches in the right table.
RIGHT OUTER JOIN-All the rows from the right table are returned even if there are no matches in the left table.

--------
Indexing in Hive table:
index acts as a reference to the records, so whenever we query a large dataset (terabytes and petabytes of data), intead of searching the entire data,
it looks for the index first and then go the required column and perform the query.

Indexing is preferred when,
1) if DS is large
2) if query execution takes considerable time.
3) build on the columns on which you frequently perform operations

Indexing should not be done when,
1) Building more number of indexes also degrade the performance of your query.
2) Type of index to be created should be identified prior to its creation else it degrades query performance.

NOTE: Indexes are maintained in a separate table, so it does not impact the data in the original table. 
      Indexes can also be partitioned for large datasets.
	  
Hive allows 2 types of indexing:
1) compact indexing: Compact indexing stores the pair of indexed column’s value and its blockid.
2) bitmap indexing : Bitmap indexing stores the combination of indexed column value and list of rows as a bitmap (combination of value and list of rows as a digital image).

In Hive, we can, Create index, Show index, Alter index, Drop index

creating index:
STEP-1:
	CREATE INDEX index_name
	 ON TABLE table_name (columns,....)
	 AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'   //built in handler implies we are creating a compact index.
	 WITH DEFERRED REBUILD;                                             // allows the index to be altered at later stages.
	 
	//To create a bitmap index,
	CREATE INDEX index_name
	 ON TABLE table_name (age)
	 AS 'BITMAP'
	 WITH DEFERRED REBUILD;
  
	Above will create an index, but to complete the creation, we will use ALTER INDEX command to launch mapreduce job for creation.
	
STEP-2:
	ALTER INDEX index_name on table_name REBUILD;             // will complete our REBUILDED index creation for the table. Compact Index
	ALTER INDEX index_name on table_name REBUILD;  // bitmap index
	
	example,
	create table olympic(athelete STRING,age INT,country STRING,year STRING,closing STRING,sport STRING,gold INT,silver INT,bronze INT,total INT) 
	row format delimited 
	fields terminated by '\t' stored as textfile;
	
	load data local inpath "...." overwrite into table olympic;
	
	CREATE INDEX olympic_index
	ON TABLE olympic (age)
	AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
	WITH DEFERRED REBUILD;
	
	ALTER INDEX olympic index on olympic REBUILD;
	
	//create a bitmap index on the same column
	CREATE INDEX olympic_index_bitmap
	ON TABLE olympic (age)
	AS BITMAP
	WITH DEFERRED REBUILD;	
 
	ALTER INDEX olympic_index_bitmap ON olympic REBUILD;
	
Showing indexes:
	SHOW FORMATTED INDEX ON olympic; //will show index name, table name, column name, index type
	
Drop Index:
	DROP INDEX IF EXISTS <index_name> ON <table_name>;
	example, DROP INDEX IF EXISTS olympic_index ON olympic;
	
------------------
Will the reducer work or not if you use “Limit 1” in any HiveQL query?  NO.

-----------------
How will you optimize Hive performance?

There are various ways to run Hive queries faster -
Using Apache Tez execution engine
Using vectorization
Using ORCFILE
Do cost based query optimization.


What is TEZ?
	TEZ is a DAG architecture which works very similar to spark in the sense that:
	1) Execute the plan but no need to read data from disk.
	2) Once ready to do some calculations (similar to actions in spark), get the data from disk and perform all steps and produce output.

	Intermediate results are stored in memory (not written to disks). 
	On top of that there is vectorization (process batch of rows instead of one row at a time). All this adds to efficiencies in query time.

	TEZ was introduced by Hortonworks as an alternative to mapreduce, though having a very high response time than traditional map reduce.
	Hive and pig can use this framework to,
	1) express query logic efficiently
	2) execute it with high performance.

what is ORC file?
	ORC is abbreviation for Optimized Row Columnar, provides an efficient way of storing Hive data.
	The filestructure is such that file contains:
	1) groups of row data called stripes.
	2) info about all the stripes of a file in file footer. Contains info like number of rows per stripe, each columns data type etc.

	Note: default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS.

	HiveQL Syntax,
	1) CREATE TABLE ... STORED AS ORC
	2) ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC
	3) SET hive.default.fileformat=Orc

	example,
	create table Addresses (
	  name string,
	  street string,
	  city string,
	  state string,
	  zip int
	) stored as orc tblproperties ("orc.compress"="NONE");  //this implies no High level compression. Other values are, ZLIB, SNAPPY.
	
what is vectorization?
	allows Hive to process a batch of rows together instead of processing one row at a time (default in Hive). 
	Introduced in hive-0.13, helps to improve performance of operations like scans, aggregations, filters and joins.
	Vectorized query execution streamlines operations by processing a block of 1024 rows at a time. 
	Within the block, each column is stored as a vector (an array of a primitive data type).

	In order to enable vector execution, enable following property - set hive.vectorized.execution.enabled = true;