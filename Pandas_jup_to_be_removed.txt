pandas

Pandas is the primary package for performing data analysis tasks in Python. pandas derives its name from PANel Data AnalysiS and is the fundamental package that provides relational data structures (think Excel, SQL type) and a host of capabilities to play with those data structures. It is the most widely used package in Python for data analysis tasks, and is very good to work with cross sectional, time series, and panel data analysis. Python sits on top of NumPy and can be used with NumPy arrays and the functions in NumPy. How is pandas suited for a researcher’s needs:

Has a tabular data structure that can hold both homogenous and heterogenous data.
Very good indexing capabilities that makes data alignment and merging easy.
Good time series functionality. No need to use different data structures for time series and cross sectional data. Allows for both ordered and unordered time-series data.
A host of statistical functions developed around NumPy and pandas that makes a researcher’s task easy and fast.
Programming is lot simpler and faster.
Easily handles data manipulation and cleaning.
Easy to expand and shorten data sets. Comprehensive merging, joins, and group by functionality to join multiple data sets.
Installing pandas

In order to check if pandas is installed, go to Package Manager and type pandas. By default, pandas already comes installed with a distribution of Canopy. If the package is not installed, click on Install.

Importing pandas

In order to be able to use NumPy, first import it using import statement

import pandas as pd                         # This will import pandas into your workspace
import numpy as np                          # We will be using numpy functions so import numpy

Data Structures in pandas

There are two basic data structures in pandas: Series and DataFrame

Series: It is similar to a NumPy 1-dimensional array. In addition to the values that are specified by the programmer, pandas attaches a label to each of the values. If the labels are not provided by the programmer, then pandas assigns labels ( 0 for first element, 1 for second element and so on). A benefit of assigning labels to data values is that it becomes easier to perform manipulations on the dataset as the whole dataset becomes more of a dictionary where each value is associated with a label.

series1 = pd.Series([10,20,30,40])
series1

0    10
1    20
2    30
3    40
dtype: int64

series1.values
array([10, 20, 30, 40], dtype=int64)

series1.index
RangeIndex(start=0, stop=4, step=1)

If you want to specify custom index values rather than the default ones provided, you can do so using the following command
series2 = pd.Series([10,20,30,40,50], index=[1,2,3,4,5])
series2

1    10
2    20
3    30
4    40
5    50
dtype: int64

The ways of accesing elements in a Series object are similar to what we have seen in NumPy, and you can perform NumPy operations on Series data arrays.


series2[6]                  # How to print 30 from the series
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-41-e8a1ac399b3a> in <module>()
----> 1 series2[6]                  # How to print 30 from the series

C:\Users\tejks\AppData\Local\Continuum\Anaconda3\lib\site-packages\pandas\core\series.py in __getitem__(self, key)
    581         key = com._apply_if_callable(key, self)
    582         try:
--> 583             result = self.index.get_value(self, key)
    584 
    585             if not lib.isscalar(result):

C:\Users\tejks\AppData\Local\Continuum\Anaconda3\lib\site-packages\pandas\indexes\base.py in get_value(self, series, key)
   1978         try:
   1979             return self._engine.get_value(s, k,
-> 1980                                           tz=getattr(series.dtype, 'tz', None))
   1981         except KeyError as e1:
   1982             if len(self) > 0 and self.inferred_type in ['integer', 'boolean']:

pandas\index.pyx in pandas.index.IndexEngine.get_value (pandas\index.c:3332)()

pandas\index.pyx in pandas.index.IndexEngine.get_value (pandas\index.c:3035)()

pandas\index.pyx in pandas.index.IndexEngine.get_loc (pandas\index.c:4018)()

pandas\hashtable.pyx in pandas.hashtable.Int64HashTable.get_item (pandas\hashtable.c:6610)()

pandas\hashtable.pyx in pandas.hashtable.Int64HashTable.get_item (pandas\hashtable.c:6554)()

KeyError: 6

series2['three']            # Another way to print 30 from the series

series2[['one', 'three', 'five']]

series2[[0,1,3]]

series2 + 4            # Add 4 to every element

series2 ** 3           # Cube every element 

series2[series2>30]    # Find all values greater than 30

np.sqrt(series2)       # Get the square roots of all the values

If you have a dictionary, you can create a Series data structure from that dictionary. Suppose you are interested in EPS values for firms and the values come from different sources and is not clean. In that case you dont have to worry about cleaning and aligning those values.


years = [91, 90, 92, 93, 94, 95, 96, 97]
f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11, 96:13}
firm1 = pd.Series(f1,index=years)                     # Extra element in VALUE not an issue
firm1

f2 = {90:14,92:9, 93:13, 94:5}                      
firm2 = pd.Series(f2,index=years)                   # Extra element in the KEY is handled as NaN
firm2
NaN stands for missing or NA values in pandas. Make use of isnull() function to find out if there are any missing values in the data structure.


pd.isnull(firm2)
A key feature of Series data is structures is that you don't have to worry about data alignment. For example, if we have run a word count program on two different files and we have the following data structures


dict1 = {'finance': 10, 'earning': 5, 'debt':8}
dict2 = {'finance' : 8, 'compensation':4, 'earning': 9}
count1 = pd.Series(dict1)
count2 = pd.Series(dict2)
print (count1)
print (count2)
If we want to calculate the sum of common words in combined files, then we dont have to worry about data alignment. If we want to include all words, then we can take care of NaN values and compute the sum. By default, Series data structure ignores NaN values. NaN values stand for missing data values.


count1+count2                                # Sum values on key based retrieval. Non matching keys become NaN
Data Frame

DataFrame is a tabular data structure in which data is laid out in rows and column format (similar to a CSV and SQL file), but it can also be used for higher dimensional data sets. The DataFrame object can contain homogenous and heterogenous values, and can be thought of as a logical extension of Series data structures. In contrast to Series, where there is one index, a DataFrame object has one index for column and one index for rows. This allows flexibility in accessing and manipulating data.


data = pd.DataFrame({'price':[95, 25, 85, 41, 78],
                     'ticker':['AXP', 'CSCO', 'DIS', 'MSFT', 'WMT'],
                     'company':['American Express', 'Cisco', 'Walt Disney','Microsoft', 'Walmart']})
data
If a column is passed with no values, it will simply have NaN values

In order to access a column, simply mention the column name


data['company']                          # Print only company names

data.company                            # Another way to print only company names

print (data.ix[2])                              # Print the row corresponding to the given Index

data.ix[data.ticker=='DIS']            # Print the row corresponding to the ticker that is 'DIS'
In order to add additional columns


data['Year'] = 2014                       # Add additional column Year. Populate value of 2014 in all the rows.
data

data['pricesquared'] = data.price**2      # Square all the prices and add another column 'pricesquared'.
data

del data['pricesquared']                  # Delete column 'pricesquared'
data

data['pricesquared'] = np.NaN             # Create column 'pricesquared' with NaN values
data

data['sequence'] = np.arange(2014,2024,2)         # Add column 'sequence' and populate sequence of numbers from 1 to 5
data
You can use NumPy functions inside DataFrame objects.


dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])  # Create array of 3*3 dimensionality using standard normally distributed random vales
dataframe

np.abs(dataframe)                   # Absolute value

f = lambda x:x.max()-x.min()        # We have option to use a powerful lambda function feature. Get the range in a column
abs(dataframe).apply(f)

abs(dataframe).apply(f,axis=1)           # Apply lambda function on the row and not column as we did in the previous example

def f(x):
    return pd.Series([np.mean(x), x.max(), x.min()], index=['mean','max','min'])
print(dataframe)
dataframe.apply(f,axis=1)

print(dataframe)
dataframe.sum()                          # Column wise sum

dataframe.sum(axis=1)                    # Row wise sum

print(dataframe)
dataframe.cumsum()                 # Get cumulative sum

dataframe.describe()
If you have non-numeric data, then applying describe function would produce statistics such as count, unique, frequency. In addition to this, you can also calculate skewness (skew), kurtosis (kurt), percent changes, difference, and other statistics.

Missing Data

Pandas have a number of features to deal with missing data. We have seen an example of the case of descriptive statistics, where missing values are not taken into account while calculating the descriptive statistics. Missing data is denoted by NaN.


years = [90, 91, 92, 93, 94, 95]
f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}
firm1 = pd.Series(f1,index=years)
firm1
f2 = {90:14,92:9, 93:13, 94:5}
firm2 = pd.Series(f2,index=years)
firm2
f3 = {93:10, 94:12, 95: 13}
firm3 = pd.Series(f3,index=years)
firm3
df3 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)
df3
df3.Firm1 = firm1
df3.Firm2 = firm2
df3.Firm3 = firm3
df3

firm2

nadeleted = firm2.dropna()
nadeleted

df3
In case of DataFrame, if you use dropna, it deletes entire row by default. Another way is to drop only those rows that are all NA. If you want to drop columns, pass axis=1


cleandf3 = df3.dropna(axis=1)          # Drop row if any value in the row is NaN
cleandf3

df3

clean2 = df3.dropna(how='all')   # Drop row if all values in the row are NaN
clean2

columndrop = df3.dropna(axis=1)   # Drop column if any value in the column is NaN
columndrop

df3

thresholddf = df3.dropna(axis=1,thresh=2)    # Define threshold. In this case row having >= 2 NaN, drop the row
thresholddf

fillna1 = df3.fillna(0)      # Fill all NaN values with 0
fillna1

fillna2 = df3.fillna({'Firm1':8, 'Firm2': 10, 'Firm3':14})   # We can specify what value should be replaced
fillna2

df3

fillna3 = df3.fillna(method='ffill')    # Forward fill NaNs for 1 level. Check Firm 2 -> 14 & 5
fillna3

fillna4 = df3.fillna(method='bfill')    # Back fill NaNs till 2 levels before. Check how Firm3 behaves
fillna4

#print(df3.mean())
print (df3)
fillna5 = df3.fillna(df3.mean())  # We will substitute empty values with Mean of the Firm in concern
fillna5

​
-----------------------------------
Dataframes

Slide Type

your_local_path="C:/Users/tejks/Desktop/ML/practice/"
Slide Type

import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
Slide Type

arr = np.linspace(0,15,40)
print (arr)
add = arr+1
add
[  0.           0.38461538   0.76923077   1.15384615   1.53846154
   1.92307692   2.30769231   2.69230769   3.07692308   3.46153846
   3.84615385   4.23076923   4.61538462   5.           5.38461538
   5.76923077   6.15384615   6.53846154   6.92307692   7.30769231
   7.69230769   8.07692308   8.46153846   8.84615385   9.23076923
   9.61538462  10.          10.38461538  10.76923077  11.15384615
  11.53846154  11.92307692  12.30769231  12.69230769  13.07692308
  13.46153846  13.84615385  14.23076923  14.61538462  15.        ]
array([  1.        ,   1.38461538,   1.76923077,   2.15384615,
         2.53846154,   2.92307692,   3.30769231,   3.69230769,
         4.07692308,   4.46153846,   4.84615385,   5.23076923,
         5.61538462,   6.        ,   6.38461538,   6.76923077,
         7.15384615,   7.53846154,   7.92307692,   8.30769231,
         8.69230769,   9.07692308,   9.46153846,   9.84615385,
        10.23076923,  10.61538462,  11.        ,  11.38461538,
        11.76923077,  12.15384615,  12.53846154,  12.92307692,
        13.30769231,  13.69230769,  14.07692308,  14.46153846,
        14.84615385,  15.23076923,  15.61538462,  16.        ])
Slide Type

sin_arr = np.sin(arr)
sin_arr
array([ 0.        ,  0.37520265,  0.69558279,  0.91432829,  0.99947728,
        0.93858826,  0.740558  ,  0.43432157,  0.06462451, -0.31451512,
       -0.64769957, -0.88624579, -0.99529876, -0.95892427, -0.78243716,
       -0.49162472, -0.12897884,  0.2525127 ,  0.5971085 ,  0.85445817,
        0.98695921,  0.97525132,  0.82104518,  0.54687254,  0.19279396,
       -0.1894546 , -0.54402111, -0.81909832, -0.97449348, -0.98750113,
       -0.85622067, -0.59983405, -0.25580306,  0.12560445,  0.48865933,
        0.78031406,  0.95795369,  0.9956225 ,  0.88781655,  0.65028784])
Slide Type

plt.plot(arr,sin_arr)
​
[<matplotlib.lines.Line2D at 0x6036f98>]

Slide Type
Selecting on certain criteria

Slide Type

test_score = np.array([[65,55,45,63],[28,92,88,65]] )
print(test_score)
passing_score = test_score > 50
print(passing_score)
test_score[passing_score]
[[65 55 45 63]
 [28 92 88 65]]
[[ True  True False  True]
 [False  True  True  True]]
array([65, 55, 63, 92, 88, 65])
Slide Type

x=2
X=2
x_123 =2
# Uncomment
123x = 2                  #invalid token
_x =2
  File "<ipython-input-50-94eb86cd99a4>", line 5
    123x = 2                  #invalid token
       ^
SyntaxError: invalid syntax


Slide Type
Pandas

Pandas is the primary package for performing data analysis tasks in Python. pandas derives its name from PANel Data AnalysiS and is the fundamental package that provides relational data structures (think Excel, SQL type) and a host of capabilities to play with those data structures. It is the most widely used package in Python for data analysis tasks, and is very good to work with cross sectional, time series, and panel data analysis. Python sits on top of NumPy and can be used with NumPy arrays and the functions in NumPy. How is pandas suited for a researcher’s needs:

Has a tabular data structure that can hold both homogenous and heterogenous data.
Very good indexing capabilities that makes data alignment and merging easy.
Good time series functionality. No need to use different data structures for time series and cross sectional data. Allows for both ordered and unordered time-series data.
A host of statistical functions developed around NumPy and pandas that makes a researcher’s task easy and fast.
Programming is lot simpler and faster.
Easily handles data manipulation and cleaning.
Easy to expand and shorten data sets. Comprehensive merging, joins, and group by functionality to join multiple data sets.
Installing pandas

In order to check if pandas is installed, go to Package Manager and type pandas. By default, pandas already comes installed with a distribution of Canopy. If the package is not installed, click on Install.

Importing pandas

In order to be able to use NumPy, first import it using import statement

Slide Type

import pandas as pd #this will import pandas into your workspace
Slide Type

years = [90, 91, 92, 93, 94, 95]
f1 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}
firm1 = pd.Series(f1,index=years)
print(firm1)
f2 = {90:14,92:9, 93:13, 94:5}
firm2 = pd.Series(f2,index=years)
print(firm2)
f3 = {93:10, 94:12, 95: 13}
firm3 = pd.Series(f3,index=years)
print(firm3)
df1 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)
df1
df1.Firm1 = firm1
df1.Firm2 = firm2
df1.Firm3 = firm3
df1
​
90     8
91     9
92     7
93     8
94     9
95    11
dtype: int64
90    14.0
91     NaN
92     9.0
93    13.0
94     5.0
95     NaN
dtype: float64
90     NaN
91     NaN
92     NaN
93    10.0
94    12.0
95    13.0
dtype: float64
Firm1	Firm2	Firm3
90	8	14.0	NaN
91	9	NaN	NaN
92	7	9.0	NaN
93	8	13.0	10.0
94	9	5.0	12.0
95	11	NaN	13.0
Slide Type

dft = df1.T
dft
del dft[90]
dft
​
91	92	93	94	95
Firm1	9.0	7.0	8.0	9.0	11.0
Firm2	NaN	9.0	13.0	5.0	NaN
Firm3	NaN	NaN	10.0	12.0	13.0
Slide Type

years = [1988, 1989, 1990, 1991, 1992, 1993, 1994]
f1 = {1990:8, 1992:11}
firm1 = pd.Series(f1,index=years)
firm1
f2 = {1990:5,1992:3}
firm2 = pd.Series(f2,index=years)
firm2
f3 = {1990:4, 1992:2}
firm3 = pd.Series(f3,index=years)
firm3
df9 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years)
df9
df9.Firm1 = firm1
df9.Firm2 = firm2
df9.Firm3 = firm3
print (df9)
print (df9.fillna(method='ffill'))
#print (df9.interpolate())
#print(df9.interpolate().fillna(method='bfill'))
​
​
      Firm1  Firm2  Firm3
1988    NaN    NaN    NaN
1989    NaN    NaN    NaN
1990    8.0    5.0    4.0
1991    NaN    NaN    NaN
1992   11.0    3.0    2.0
1993    NaN    NaN    NaN
1994    NaN    NaN    NaN
      Firm1  Firm2  Firm3
1988    NaN    NaN    NaN
1989    NaN    NaN    NaN
1990    8.0    5.0    4.0
1991    8.0    5.0    4.0
1992   11.0    3.0    2.0
1993   11.0    3.0    2.0
1994   11.0    3.0    2.0
Slide Type
You can pass a number of data structures to DataFrame such as a ndarray, lists, dict, Series, and another DataFrame. You can also reindex to confirm to data to a new index. Reindexing is a powerful feature that allows you to access data in a number of different ways, and also to confirm data to some new time series or other index.

Slide Type

print (df1)
reindexdf1 = df1.reindex([88,89,90,91,92,93,94,95,96,97,98])
reindexdf1
    Firm1  Firm2  Firm3
90      8   14.0    NaN
91      9    NaN    NaN
92      7    9.0    NaN
93      8   13.0   10.0
94      9    5.0   12.0
95     11    NaN   13.0
Firm1	Firm2	Firm3
88	NaN	NaN	NaN
89	NaN	NaN	NaN
90	8.0	14.0	NaN
91	9.0	NaN	NaN
92	7.0	9.0	NaN
93	8.0	13.0	10.0
94	9.0	5.0	12.0
95	11.0	NaN	13.0
96	NaN	NaN	NaN
97	NaN	NaN	NaN
98	NaN	NaN	NaN
Slide Type

print(df9)
reindexdf9 = df9.reindex(np.arange(1988,1998))
reindexdf9
      Firm1  Firm2  Firm3
1988    NaN    NaN    NaN
1989    NaN    NaN    NaN
1990    8.0    5.0    4.0
1991    NaN    NaN    NaN
1992   11.0    3.0    2.0
1993    NaN    NaN    NaN
1994    NaN    NaN    NaN
Firm1	Firm2	Firm3
1988	NaN	NaN	NaN
1989	NaN	NaN	NaN
1990	8.0	5.0	4.0
1991	NaN	NaN	NaN
1992	11.0	3.0	2.0
1993	NaN	NaN	NaN
1994	NaN	NaN	NaN
1995	NaN	NaN	NaN
1996	NaN	NaN	NaN
1997	NaN	NaN	NaN


Slide Type

years1 = [90, 91, 92, 93, 94, 95]
f4 = {90:8, 91:9, 92:7, 93:8, 94:9, 95:11}
firm4 = pd.Series(f4,index=years1)
f5 = {90:14,91:12, 92:9, 93:13, 94:5, 95:8}
firm5 = pd.Series(f5,index=years1)
f6 = {90:8, 91: 9, 92:9,93:10, 94:12, 95: 13}
firm6 = pd.Series(f6,index=years1)
df2 = pd.DataFrame(columns=['Firm1','Firm2','Firm3'],index=years1)
df2.Firm1 = firm4
df2.Firm2 = firm5
df2.Firm3 = firm6
df2
Firm1	Firm2	Firm3
90	8	14	8
91	9	12	9
92	7	9	9
93	8	13	10
94	9	5	12
95	11	8	13
Slide Type

reindexdf2 = df2.reindex([88,89,90,91,92,93,94,95,96,97,98],fill_value=0)
reindexdf2
​
Firm1	Firm2	Firm3
88	0	0	0
89	0	0	0
90	8	14	8
91	9	12	9
92	7	9	9
93	8	13	10
94	9	5	12
95	11	8	13
96	0	0	0
97	0	0	0
98	0	0	0
Slide Type
You can use NumPy functions inside DataFrame objects.

Slide Type

# Generate random sample from a standard normal distribution
np.random.seed(2) # what is this doing?
dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])
dataframe
​
# Try tab for the other distributions
#np.random.
one	two	three
0	-0.416758	-0.056267	-2.136196
1	1.640271	-1.793436	-0.841747
2	0.502881	-1.245288	-1.057952
Slide Type

dataframe=np.abs(dataframe)
dataframe
one	two	three
0	0.416758	0.056267	2.136196
1	1.640271	1.793436	0.841747
2	0.502881	1.245288	1.057952
Slide Type

np.random.seed(2)
dataframe = pd.DataFrame(np.random.randn(3,3),columns=['one','two','three'])
dataframe
one	two	three
0	-0.416758	-0.056267	-2.136196
1	1.640271	-1.793436	-0.841747
2	0.502881	-1.245288	-1.057952
Slide Type

dataframe.sort_values(by='two')
one	two	three
1	1.640271	-1.793436	-0.841747
2	0.502881	-1.245288	-1.057952
0	-0.416758	-0.056267	-2.136196
Slide Type
Hierarchical Indexing

Hierarchical indexing allows you to have index on an index (multiple index). It is an important feature of pandas using which you can select subsets of data and perform independent analyses on them. For example, suppose you have firm prices data and the data is indexed by firm name. On top of that, you can index firms by industry. Thus, industry becomes an index on top of firms. You can then perform analyses either on individual firm, or on group of firms in an industry, or on the whole dataset.

Slide Type

import numpy as np
import pandas as pd
h_i_data = pd.Series(np.random.randn(10),index=[['Ind1','Ind1','Ind1','Ind1','Ind2','Ind2','Ind2','Ind3','Ind3','Ind3'],
                                              [1,2,3,4,1,2,3,1,2,3]])
h_i_data
Ind1  1   -0.153495
      2   -0.269057
      3    2.231367
      4   -2.434768
Ind2  1    0.112727
      2    0.370445
      3    1.359634
Ind3  1    0.501857
      2   -0.844214
      3    0.000010
dtype: float64
Slide Type

h_i_data['Ind3']
1    0.501857
2   -0.844214
3    0.000010
dtype: float64
Slide Type

h_i_data['Ind1':'Ind2']
Ind1  1   -0.153495
      2   -0.269057
      3    2.231367
      4   -2.434768
Ind2  1    0.112727
      2    0.370445
      3    1.359634
dtype: float64
Slide Type

h_i_data[['Ind1','Ind3']]
Ind1  1   -0.153495
      2   -0.269057
      3    2.231367
      4   -2.434768
Ind3  1    0.501857
      2   -0.844214
      3    0.000010
dtype: float64
Slide Type

h_i_data[:,3]
Ind1    2.231367
Ind2    1.359634
Ind3    0.000010
dtype: float64
Slide Type

h_i_data[:,4]
Ind1   -2.434768
dtype: float64
Slide Type

print (h_i_data)
h_i_data.unstack(level=0) # pivot an index
Ind1  1   -0.153495
      2   -0.269057
      3    2.231367
      4   -2.434768
Ind2  1    0.112727
      2    0.370445
      3    1.359634
Ind3  1    0.501857
      2   -0.844214
      3    0.000010
dtype: float64
Ind1	Ind2	Ind3
1	-0.153495	0.112727	0.501857
2	-0.269057	0.370445	-0.844214
3	2.231367	1.359634	0.000010
4	-2.434768	NaN	NaN
Slide Type

h_i_data.unstack().stack()
Ind1  1   -0.153495
      2   -0.269057
      3    2.231367
      4   -2.434768
Ind2  1    0.112727
      2    0.370445
      3    1.359634
Ind3  1    0.501857
      2   -0.844214
      3    0.000010
dtype: float64
Slide Type

h_i_data.unstack(level=1) # pivot an index
1	2	3	4
Ind1	-0.153495	-0.269057	2.231367	-2.434768
Ind2	0.112727	0.370445	1.359634	NaN
Ind3	0.501857	-0.844214	0.000010	NaN
Slide Type

h_i_data.sum()
0.8745052250481244
Slide Type

h_i_data.sum(level=1)
1    0.461089
2   -0.742826
3    3.591010
4   -2.434768
dtype: float64
Slide Type

h_i_data.sum(level=0)
Ind1   -0.625953
Ind2    1.842805
Ind3   -0.342347
dtype: float64
Slide Type

h_i_data
Slide Type
IO in pandas

In this section, we will focus on I/O from text files, csv, excel, and sql files as well as getting data from web such as Yahoo! Finance. Using functions in pandas, you can read data as a DataFrame object.

Reading a csv file

Slide Type

roedatacsv = pd.read_csv(your_local_path+'roedata_Sc.csv')
#roedatacsv
roedatacsv.head(10)
Industry Name,Number of firms,ROE
0	Advertising,65,16.51%
1	Aerospace/Defense,95,21.60%
2	Air Transport,25,42.68%
3	Apparel,70,17.87%
4	Auto & Truck,26,22.05%
5	Auto Parts,75,17.54%
6	Bank,7,15.03%
7	Banks (Regional),721,9.52%
8	Beverage ,47,27.62%
9	Beverage (Alcoholic),19,18.28%
Slide Type

location_df = roedatacsv['Industry Name,Number of firms,ROE'].apply(lambda x: pd.Series(x.split(',')))
#location_df = location_df.Dataframe(columns=['Industry Name','Number of firms','ROE'])
#df1 = location_df.Dataframe(columns=['Firm1','Firm2','Firm3'])
location_df.columns = ['Industry Name','Number of firms','ROE']
location_df
Industry Name	Number of firms	ROE
0	Advertising	65	16.51%
1	Aerospace/Defense	95	21.60%
2	Air Transport	25	42.68%
3	Apparel	70	17.87%
4	Auto & Truck	26	22.05%
5	Auto Parts	75	17.54%
6	Bank	7	15.03%
7	Banks (Regional)	721	9.52%
8	Beverage	47	27.62%
9	Beverage (Alcoholic)	19	18.28%
10	Biotechnology	349	6.77%
11	Broadcasting	30	74.10%
12	Brokerage & Investment Banking	49	9.25%
13	Building Materials	37	6.78%
14	Business & Consumer Services	179	12.48%
15	Cable TV	16	26.62%
16	Chemical (Basic)	47	8.80%
17	Chemical (Diversified)	10	24.33%
18	Chemical (Specialty)	100	22.10%
19	Coal & Related Energy	45	-14.66%
20	Computer Services	129	39.46%
21	Computer Software	273	21.53%
22	Computers/Peripherals	66	24.55%
23	Construction	18	3.62%
24	Diversified	20	13.06%
25	Educational Services	40	-0.13%
26	Electrical Equipment	135	13.67%
27	Electronics	191	7.98%
28	Electronics (Consumer & Office)	26	25.66%
29	Engineering	56	7.61%
...	...	...	...
67	Railroad	10	19.85%
68	Real Estate (Development)	22	-4.29%
69	Real Estate (General/Diversified)	11	1.02%
70	Real Estate (Operations & Services)	47	19.84%
71	Recreation	70	18.03%
72	Reinsurance	3	6.05%
73	Restaurant	84	27.46%
74	Retail (Automotive)	30	30.79%
75	Retail (Building Supply)	7	23.12%
76	Retail (Distributors)	87	12.92%
77	Retail (General)	21	17.68%
78	Retail (Grocery and Food)	21	10.91%
79	Retail (Internet)	47	18.36%
80	Retail (Special Lines)	137	17.29%
81	Rubber& Tires	4	45.41%
82	Semiconductor	104	13.14%
83	Semiconductor Equip	51	-1.21%
84	Shipbuilding & Marine	14	0.37%
85	Shoe	14	24.27%
86	Steel	37	-6.67%
87	Telecom (Wireless)	28	-15.63%
88	Telecom. Equipment	131	15.67%
89	Telecom. Services	82	5.78%
90	Thrift	223	-79.47%
91	Tobacco	12	214.71%
92	Transportation	22	14.75%
93	Trucking	28	16.01%
94	Utility (General)	20	7.34%
95	Utility (Water)	20	9.95%
96	Total Market	7766	15.68%
97 rows × 3 columns

Slide Type
If the file does not have a header, then you can either let pandas assign default headers or you can specify custom headers. If you want industry name to be the index of DataFrame, you can achieve that.

Slide Type

roedatacsv = pd.read_csv(your_local_path+'roedata.csv', index_col = 'Industry Name' )
roedatacsv
Number of firms	ROE
Industry Name		
Advertising	65	16.51%
Aerospace/Defense	95	21.60%
Air Transport	25	42.68%
Apparel	70	17.87%
Auto & Truck	26	22.05%
Auto Parts	75	17.54%
Bank	7	15.03%
Banks (Regional)	721	9.52%
Beverage	47	27.62%
Beverage (Alcoholic)	19	18.28%
Biotechnology	349	6.77%
Broadcasting	30	74.10%
Brokerage & Investment Banking	49	9.25%
Building Materials	37	6.78%
Business & Consumer Services	179	12.48%
Cable TV	16	26.62%
Chemical (Basic)	47	8.80%
Chemical (Diversified)	10	24.33%
Chemical (Specialty)	100	22.10%
Coal & Related Energy	45	-14.66%
Computer Services	129	39.46%
Computer Software	273	21.53%
Computers/Peripherals	66	24.55%
Construction	18	3.62%
Diversified	20	13.06%
Educational Services	40	-0.13%
Electrical Equipment	135	13.67%
Electronics	191	7.98%
Electronics (Consumer & Office)	26	25.66%
Engineering	56	7.61%
...	...	...
Railroad	10	19.85%
Real Estate (Development)	22	-4.29%
Real Estate (General/Diversified)	11	1.02%
Real Estate (Operations & Services)	47	19.84%
Recreation	70	18.03%
Reinsurance	3	6.05%
Restaurant	84	27.46%
Retail (Automotive)	30	30.79%
Retail (Building Supply)	7	23.12%
Retail (Distributors)	87	12.92%
Retail (General)	21	17.68%
Retail (Grocery and Food)	21	10.91%
Retail (Internet)	47	18.36%
Retail (Special Lines)	137	17.29%
Rubber& Tires	4	45.41%
Semiconductor	104	13.14%
Semiconductor Equip	51	-1.21%
Shipbuilding & Marine	14	0.37%
Shoe	14	24.27%
Steel	37	-6.67%
Telecom (Wireless)	28	-15.63%
Telecom. Equipment	131	15.67%
Telecom. Services	82	5.78%
Thrift	223	-79.47%
Tobacco	12	214.71%
Transportation	22	14.75%
Trucking	28	16.01%
Utility (General)	20	7.34%
Utility (Water)	20	9.95%
Total Market	7766	15.68%
97 rows × 2 columns

Slide Type

roedatacsv = pd.read_csv(your_local_path+'roedata.csv', usecols = ['Industry Name','ROE'] )
roedatacsv # usecols is used to selectively get the columns that we require
Slide Type

# Import only selected rows
roedatacsv = pd.read_csv(your_local_path+'roedata.csv',nrows=50)
roedatacsv
Industry Name	Number of firms	ROE
0	Advertising	65	16.51%
1	Aerospace/Defense	95	21.60%
2	Air Transport	25	42.68%
3	Apparel	70	17.87%
4	Auto & Truck	26	22.05%
5	Auto Parts	75	17.54%
6	Bank	7	15.03%
7	Banks (Regional)	721	9.52%
8	Beverage	47	27.62%
9	Beverage (Alcoholic)	19	18.28%
10	Biotechnology	349	6.77%
11	Broadcasting	30	74.10%
12	Brokerage & Investment Banking	49	9.25%
13	Building Materials	37	6.78%
14	Business & Consumer Services	179	12.48%
15	Cable TV	16	26.62%
16	Chemical (Basic)	47	8.80%
17	Chemical (Diversified)	10	24.33%
18	Chemical (Specialty)	100	22.10%
19	Coal & Related Energy	45	-14.66%
20	Computer Services	129	39.46%
21	Computer Software	273	21.53%
22	Computers/Peripherals	66	24.55%
23	Construction	18	3.62%
24	Diversified	20	13.06%
25	Educational Services	40	-0.13%
26	Electrical Equipment	135	13.67%
27	Electronics	191	7.98%
28	Electronics (Consumer & Office)	26	25.66%
29	Engineering	56	7.61%
30	Entertainment	85	17.45%
31	Environmental & Waste Services	108	8.28%
32	Farming/Agriculture	29	4.82%
33	Financial Svcs.	76	15.50%
34	Financial Svcs. (Non-bank & Insurance)	17	6.45%
35	Food Processing	97	15.94%
36	Food Wholesalers	18	17.74%
37	Furn/Home Furnishings	36	12.99%
38	Healthcare Equipment	193	11.87%
39	Healthcare Facilities	47	26.29%
40	Healthcare Products	58	5.52%
41	Healthcare Services	126	13.14%
42	Heathcare Information and Technology	125	9.45%
43	Heavy Construction	46	22.00%
44	Homebuilding	32	32.64%
45	Hotel/Gaming	89	4.50%
46	Household Products	139	21.68%
47	Information Services	71	22.24%
48	Insurance (General)	26	4.08%
49	Insurance (Life)	27	6.08%
Slide Type

capm_dem_data = pd.read_table(your_local_path+'capm_dem.txt',nrows = 50, delimiter=' ',header = None)
capm_dem_data
Slide Type

capm_dem_data = pd.read_table(your_local_path+'capm_dem.txt', delimiter=' ',header = None)
capm_dem_data
0	1	2	3
0	195710	880211	-0.012605	0.003871
1	195710	880212	-0.008511	0.007406
2	195710	880216	0.008584	0.001411
3	195710	880217	-0.004255	0.002414
4	195710	880218	0.000000	0.002845
5	195710	880219	0.008547	0.004753
6	195710	880222	0.012712	0.006375
7	195710	880223	-0.008368	0.001864
8	195710	880224	-0.008439	0.004237
9	195710	880225	-0.004255	0.005164
10	195710	880226	-0.008547	0.001747
11	195710	880229	0.012931	0.011278
12	195710	880301	-0.008511	0.001305
13	195710	880302	0.008584	0.005652
14	195710	880303	-0.017021	0.002895
15	195710	880304	0.004329	0.004586
16	195710	880307	-0.017241	0.003740
17	195710	880308	-0.008772	0.006258
18	195710	880309	0.008850	0.008291
19	195710	880310	-0.008772	-0.004363
20	195710	880311	-0.008850	0.000796
21	195710	880314	0.013393	0.002721
22	195710	880315	0.008811	0.001341
23	195710	880316	0.004367	0.004319
24	195710	880317	0.008696	0.005612
25	195710	880318	-0.008621	0.003249
26	195710	880321	-0.004348	-0.004265
27	195710	880322	-0.008734	0.002760
28	195710	880323	-0.004405	0.003259
29	195710	880324	-0.017699	-0.006748
...	...	...	...	...
244	811710	880419	-0.005865	0.004033
245	811710	880420	0.002950	-0.004894
246	811710	880421	0.002941	-0.000031
247	811710	880422	0.002933	0.003857
248	811710	880425	0.013216	0.002482
249	811710	880426	0.017595	0.004963
250	811710	880427	-0.014409	0.002056
251	811710	880428	0.008772	0.001157
252	811710	880429	-0.026087	0.003796
253	811710	880502	-0.008929	-0.001114
254	811710	880503	-0.006006	0.002655
255	811710	880504	0.003021	-0.000176
256	811710	880505	-0.003012	-0.001624
257	811710	880506	-0.018127	0.000376
258	811710	880509	-0.003077	-0.005489
259	811710	880510	-0.006173	0.000986
260	811710	880511	-0.012422	-0.012381
261	811710	880512	0.009434	0.002235
262	811710	880513	0.012461	0.005115
263	811710	880516	0.003077	0.000834
264	811710	880517	-0.018405	-0.001624
265	811710	880518	0.003125	-0.010956
266	811710	880519	0.006231	-0.001865
267	811710	880520	0.012384	0.003239
268	811710	880523	-0.003058	-0.003542
269	811710	880524	0.024540	0.001700
270	811710	880525	0.008982	0.001598
271	811710	880526	0.000000	0.003151
272	811710	880527	0.002967	-0.000359
273	811710	880531	0.038462	0.006501
274 rows × 4 columns

Slide Type

#crsp_data = pd.read_table(your_local_path+'crsp.output', sep='\s+',header = None)
crsp_data = pd.read_table(your_local_path+'crsp.output', sep='\s+', header = None)
crsp_data
0	1	2	3	4	5	6	7	8	9	10	11	12	13
0	00036110	19840831	19890929	3.311923	22.62500	0.97	0.597	-0.172	0.248	0.153	0.193	0.081	0.163	135.908
1	00036110	19850830	19900928	0.378167	22.62500	1.51	0.640	-0.028	0.185	0.149	0.187	0.095	0.148	136.474
2	00036110	19860829	19910930	0.104652	23.50000	1.27	0.446	0.178	0.058	0.157	0.142	0.112	0.129	213.827
3	00036110	19870831	19920930	-0.391107	37.37500	1.50	0.342	0.371	-0.097	0.157	0.101	0.122	0.078	392.400
4	00036110	19880831	19930930	-0.408846	24.62500	1.34	0.385	0.286	-0.697	0.187	0.044	0.163	0.018	390.429
5	00036110	19890831	19940930	-0.547627	34.62500	1.56	0.309	0.248	-0.525	0.193	0.018	0.163	0.015	554.796
6	00036110	19900831	19950929	0.002083	15.87500	1.60	0.743	0.185	-0.263	0.187	0.019	0.148	0.012	255.111
7	00036110	19910830	19960930	0.639713	16.25000	0.93	0.750	0.058	0.112	0.142	0.030	0.129	0.019	258.229
8	00088610	19840131	19890228	0.928572	13.75000	0.95	0.464	0.176	0.302	0.289	0.139	0.221	0.182	78.760
9	00088610	19850131	19900228	3.300000	13.25000	0.57	0.533	-0.017	0.298	0.189	0.154	0.147	0.151	75.750
10	00088610	19860131	19910228	2.375000	22.50000	1.37	0.373	0.076	0.177	0.124	0.171	0.143	0.134	128.970
11	00088610	19870130	19920228	0.884146	23.50000	1.39	0.298	0.206	0.114	0.120	0.205	0.162	0.143	202.499
12	00088610	19880129	19930226	1.388489	17.25000	1.18	0.338	0.267	0.070	0.123	0.188	0.166	0.142	224.543
13	00088610	19890131	19940228	5.226804	13.37500	1.30	0.533	0.302	0.099	0.139	0.174	0.182	0.146	175.855
14	00088610	19900131	19950228	5.068027	17.50000	1.25	0.477	0.298	0.144	0.154	0.157	0.151	0.150	231.805
15	00088610	19910131	19960229	5.489796	26.25000	1.73	0.384	0.177	0.171	0.171	0.201	0.134	0.157	349.020
16	00095710	19840131	19890228	0.433400	23.25000	2.26	0.724	0.107	-0.067	0.108	0.119	0.118	0.115	81.259
17	00095710	19850131	19900228	0.835788	25.00000	2.12	0.683	0.081	0.029	0.109	0.103	0.108	0.106	89.875
18	00095710	19860131	19910228	0.511377	22.12500	1.38	0.794	-0.012	0.127	0.136	0.086	0.107	0.094	80.690
19	00095710	19870130	19920228	1.198015	23.75000	1.53	0.767	-0.066	0.144	0.138	0.084	0.114	0.084	88.872
20	00095710	19880129	19930226	1.433394	20.00000	1.30	0.934	-0.113	0.149	0.127	0.076	0.116	0.074	76.300
21	00103010	19860530	19910628	-0.556604	13.25000	1.01	1.023	0.524	-0.156	0.097	0.082	0.136	0.059	54.259
22	00103110	19910131	19960229	3.483710	7.75000	0.81	0.978	0.054	0.306	0.199	0.132	0.180	0.132	37.084
23	00105510	19840330	19890428	4.260434	16.75000	1.70	0.776	-0.052	0.235	0.218	0.324	0.167	0.252	298.016
24	00105510	19850329	19900430	1.102349	27.00000	2.72	0.510	0.098	0.120	0.196	0.298	0.161	0.240	536.868
25	00105510	19860331	19910430	0.886910	38.16666	1.80	0.297	0.248	0.115	0.202	0.244	0.159	0.201	1144.113
26	00105510	19870331	19920430	1.204124	25.75000	1.96	0.431	0.314	0.099	0.246	0.202	0.189	0.153	1035.099
27	00105510	19880331	19930430	1.944473	16.25000	1.16	0.418	0.282	0.130	0.303	0.173	0.230	0.140	1314.999
28	00105510	19890331	19940429	1.653188	17.00000	1.35	0.466	0.235	0.185	0.324	0.192	0.252	0.157	1377.935
29	00105510	19900330	19950428	2.715672	14.75000	1.00	0.586	0.120	0.247	0.298	0.223	0.240	0.190	1197.582
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
661	05527050	19870331	19920430	1.543598	8.56250	0.79	65.589	0.098	-0.124	0.075	0.283	-0.065	0.094	94.239
662	05527050	19880331	19930430	1.775276	8.00000	1.00	84.524	0.130	-0.112	0.092	0.279	-0.018	0.077	88.048
663	05527050	19890331	19940429	1.422740	9.81250	1.14	60.302	0.150	-0.025	0.113	0.254	0.051	0.077	107.996
664	05527050	19900330	19950428	0.833697	12.81250	1.24	53.617	0.153	0.091	0.100	0.188	0.049	0.034	141.014
665	05527050	19910328	19960430	0.611784	12.12500	0.46	41.533	-0.009	0.247	0.218	0.075	0.077	0.027	133.448
666	05534B10	19840330	19890428	0.750622	23.25000	2.78	0.853	0.062	-0.014	0.083	0.136	0.136	0.114	4999.633
667	05534B10	19850329	19900430	0.632114	29.12500	3.05	0.703	0.087	0.004	0.077	0.179	0.114	0.113	6800.163
668	05534B10	19860331	19910430	0.805334	28.50000	3.02	0.757	0.104	0.012	0.084	0.191	0.113	0.107	6907.032
669	05534B10	19870331	19920430	0.621981	32.37500	2.78	0.707	0.033	0.040	0.099	0.180	0.101	0.108	8588.310
670	05534B10	19880331	19930430	0.572517	30.75000	3.01	0.829	0.025	0.035	0.121	0.138	0.106	0.081	8371.872
671	05534B10	19890331	19940429	0.570449	31.00000	2.59	0.876	-0.014	-0.396	0.136	0.036	0.114	0.036	8953.513
672	05534B10	19900330	19950428	0.201547	35.12500	3.38	0.778	0.004	-0.296	0.179	-0.056	0.113	0.004	10597.177
673	05534B10	19910328	19960430	0.476158	36.00000	3.02	0.792	0.012	-0.202	0.191	-0.076	0.107	0.007	10983.996
674	05544210	19880429	19930528	6.724358	8.25000	0.47	0.662	-0.059	0.270	0.322	0.016	0.185	0.129	26.433
675	05544210	19890428	19940531	2.073009	12.75000	1.02	0.355	0.088	0.155	0.066	0.025	0.145	0.121	40.991
676	05544210	19900430	19950531	2.713574	14.50000	1.32	0.346	0.261	0.115	0.039	0.035	0.128	0.116	43.239
677	05544210	19910430	19960531	1.302734	26.25000	1.64	0.165	0.353	0.088	0.007	0.060	0.111	0.117	80.194
678	05560710	19840330	19890428	-0.472315	19.50000	1.66	0.778	0.105	-0.120	0.195	-0.068	0.171	0.058	100.308
679	05560710	19880331	19930430	0.921570	6.37500	0.85	0.269	-0.111	0.062	0.005	0.028	-0.025	0.052	33.494
680	05560710	19890331	19940429	2.184617	8.75000	0.92	0.294	-0.120	0.148	-0.068	0.021	-0.058	0.041	46.209
681	05560710	19900330	19950428	4.072203	7.75000	0.96	0.434	-0.058	0.249	-0.125	0.026	-0.054	0.048	41.222
682	05652510	19850329	19900430	0.999144	10.87500	1.06	1.211	-0.069	0.100	-0.050	0.059	-0.022	0.056	15.823
683	05652510	19860331	19910430	0.100793	15.37500	0.92	0.893	0.061	0.125	-0.062	0.062	-0.020	0.062	22.448
684	05652510	19870331	19920430	0.128704	20.50000	0.80	0.686	0.010	0.046	-0.030	0.055	-0.004	0.049	30.156
685	05652510	19900330	19950428	0.606721	18.50000	1.54	0.907	0.100	-0.005	0.059	0.053	0.056	0.054	28.564
686	05652510	19910328	19960430	0.823062	17.87500	1.45	0.927	0.125	0.138	0.062	0.044	0.062	0.070	30.388
687	05654310	19860331	19910430	1.244162	22.75000	1.98	0.852	0.031	0.310	0.121	0.066	0.049	0.100	22.614
688	05654310	19870331	19920430	1.318356	23.12500	0.59	0.828	-0.075	0.360	0.120	0.080	0.046	0.064	22.986
689	05709710	19840330	19890428	0.720602	38.50000	3.58	0.556	0.201	-0.035	0.203	-0.008	0.201	0.072	238.199
690	05709710	19850329	19900430	-0.628028	30.25000	2.54	0.500	0.159	-0.200	0.180	-0.037	0.202	0.000	NaN
691 rows × 14 columns

Slide Type
Handling missing values

Some types of missing values are automatically identified by pandas as NaN while importing the data. Those types are NA, NULL, -1.#IND. Additionally, you can also specify a list of missing values.

Slide Type

roemissing = pd.read_csv(your_local_path+'roemissing.csv', na_values=['NULL',-999, 'RP'] )
roemissing
Slide Type

roemissing = pd.read_csv(your_local_path+'roemissing.csv', na_values={'Number of firms':[-999],'ROE':['10000.00%']} )
roemissing
Industry Name	Number of firms	ROE
0	Advertising	65	16.51%
1	Aerospace/Defense	95	21.60%
2	Air Transport	25	42.68%
3	Apparel	70	17.87%
4	Auto & Truck	26	22.05%
5	Auto Parts	75	17.54%
6	Bank	7	15.03%
7	Banks (Regional)	NaN	9.52%
8	Beverage	47	NaN
9	Beverage (Alcoholic)	19	18.28%
10	Biotechnology	10000.00%	6.77%
11	Broadcasting	30	74.10%
12	Brokerage & Investment Banking	49	9.25%
13	Building Materials	37	6.78%
14	Business & Consumer Services	NaN	12.48%
15	Cable TV	RP	26.62%
16	Chemical (Basic)	47	NaN
17	Chemical (Diversified)	10	145.00%
18	Chemical (Specialty)	100	22.10%
19	Coal & Related Energy	45	-14.66%
20	Computer Services	129	39.46%
21	Computer Software	273	NaN
22	Computers/Peripherals	66	24.55%
23	Construction	18	3.62%
24	Diversified	20	13.06%
25	Educational Services	40	-0.13%
26	Electrical Equipment	135	13.67%
27	Electronics	191	7.98%
28	Electronics (Consumer & Office)	26	25.66%
29	Engineering	56	7.61%
...	...	...	...
67	Railroad	10	19.85%
68	Real Estate (Development)	22	-4.29%
69	Real Estate (General/Diversified)	11	1.02%
70	Real Estate (Operations & Services)	47	19.84%
71	Recreation	70	18.03%
72	Reinsurance	3	6.05%
73	Restaurant	84	27.46%
74	Retail (Automotive)	30	30.79%
75	Retail (Building Supply)	7	23.12%
76	Retail (Distributors)	87	12.92%
77	Retail (General)	21	17.68%
78	Retail (Grocery and Food)	21	10.91%
79	Retail (Internet)	47	18.36%
80	Retail (Special Lines)	137	17.29%
81	Rubber& Tires	4	45.41%
82	Semiconductor	104	13.14%
83	Semiconductor Equip	51	-1.21%
84	Shipbuilding & Marine	14	0.37%
85	Shoe	14	24.27%
86	Steel	37	-6.67%
87	Telecom (Wireless)	28	-15.63%
88	Telecom. Equipment	131	15.67%
89	Telecom. Services	82	5.78%
90	Thrift	223	-79.47%
91	Tobacco	12	214.71%
92	Transportation	22	14.75%
93	Trucking	28	16.01%
94	Utility (General)	20	7.34%
95	Utility (Water)	20	9.95%
96	Total Market	7766	15.68%
97 rows × 3 columns

Slide Type
Writing Data

Slide Type

roedata = pd.read_csv(your_local_path+'roedata.csv')
roedata
roedata.to_csv(your_local_path+'roedatawrite89.csv')
Slide Type

roedata = pd.read_csv(your_local_path+'roedata.csv')
roedata.to_csv(your_local_path+'roedatawrite97.csv',  columns=['Industry Name','ROE'])
Slide Type
Merging Data

Slide Type

import pandas as pd
Slide Type

help(pd.read_csv)
Help on function read_csv in module pandas.io.parsers:

read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None)
    Read CSV (comma-separated) file into DataFrame
    
    Also supports optionally iterating or breaking of the file
    into chunks.
    
    Additional help can be found in the `online docs for IO Tools
    <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.
    
    Parameters
    ----------
    filepath_or_buffer : str, pathlib.Path, py._path.local.LocalPath or any object with a read() method (such as a file handle or StringIO)
        The string could be a URL. Valid URL schemes include http, ftp, s3, and
        file. For file URLs, a host is expected. For instance, a local file could
        be file ://localhost/path/to/table.csv
    sep : str, default ','
        Delimiter to use. If sep is None, will try to automatically determine
        this. Separators longer than 1 character and different from '\s+' will be
        interpreted as regular expressions, will force use of the python parsing
        engine and will ignore quotes in the data. Regex example: '\r\t'
    delimiter : str, default ``None``
        Alternative argument name for sep.
    delim_whitespace : boolean, default False
        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be
        used as the sep. Equivalent to setting ``sep='\+s'``. If this option
        is set to True, nothing should be passed in for the ``delimiter``
        parameter.
    
        .. versionadded:: 0.18.1 support for the Python parser.
    
    header : int or list of ints, default 'infer'
        Row number(s) to use as the column names, and the start of the data.
        Default behavior is as if set to 0 if no ``names`` passed, otherwise
        ``None``. Explicitly pass ``header=0`` to be able to replace existing
        names. The header can be a list of integers that specify row locations for
        a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not
        specified will be skipped (e.g. 2 in this example is skipped). Note that
        this parameter ignores commented lines and empty lines if
        ``skip_blank_lines=True``, so header=0 denotes the first line of data
        rather than the first line of the file.
    names : array-like, default None
        List of column names to use. If file contains no header row, then you
        should explicitly pass header=None
    index_col : int or sequence or False, default None
        Column to use as the row labels of the DataFrame. If a sequence is given, a
        MultiIndex is used. If you have a malformed file with delimiters at the end
        of each line, you might consider index_col=False to force pandas to _not_
        use the first column as the index (row names)
    usecols : array-like, default None
        Return a subset of the columns. All elements in this array must either
        be positional (i.e. integer indices into the document columns) or strings
        that correspond to column names provided either by the user in `names` or
        inferred from the document header row(s). For example, a valid `usecols`
        parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Using this parameter
        results in much faster parsing time and lower memory usage.
    squeeze : boolean, default False
        If the parsed data only contains one column then return a Series
    prefix : str, default None
        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...
    mangle_dupe_cols : boolean, default True
        Duplicate columns will be specified as 'X.0'...'X.N', rather than 'X'...'X'
    dtype : Type name or dict of column -> type, default None
        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}
        (Unsupported with engine='python'). Use `str` or `object` to preserve and
        not interpret dtype.
    engine : {'c', 'python'}, optional
        Parser engine to use. The C engine is faster while the python engine is
        currently more feature-complete.
    converters : dict, default None
        Dict of functions for converting values in certain columns. Keys can either
        be integers or column labels
    true_values : list, default None
        Values to consider as True
    false_values : list, default None
        Values to consider as False
    skipinitialspace : boolean, default False
        Skip spaces after delimiter.
    skiprows : list-like or integer, default None
        Line numbers to skip (0-indexed) or number of lines to skip (int)
        at the start of the file
    skipfooter : int, default 0
        Number of lines at bottom of file to skip (Unsupported with engine='c')
    nrows : int, default None
        Number of rows of file to read. Useful for reading pieces of large files
    na_values : str or list-like or dict, default None
        Additional strings to recognize as NA/NaN. If dict passed, specific
        per-column NA values.  By default the following values are interpreted as
        NaN: `''`, `'#N/A'`, `'#N/A N/A'`, `'#NA'`, `'-1.#IND'`, `'-1.#QNAN'`, `'-NaN'`, `'-nan'`, `'1.#IND'`, `'1.#QNAN'`, `'N/A'`, `'NA'`, `'NULL'`, `'NaN'`, `'nan'`.
    keep_default_na : bool, default True
        If na_values are specified and keep_default_na is False the default NaN
        values are overridden, otherwise they're appended to.
    na_filter : boolean, default True
        Detect missing value markers (empty strings and the value of na_values). In
        data without any NAs, passing na_filter=False can improve the performance
        of reading a large file
    verbose : boolean, default False
        Indicate number of NA values placed in non-numeric columns
    skip_blank_lines : boolean, default True
        If True, skip over blank lines rather than interpreting as NaN values
    parse_dates : boolean or list of ints or names or list of lists or dict, default False
    
        * boolean. If True -> try parsing the index.
        * list of ints or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3
          each as a separate date column.
        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as
            a single date column.
        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result
          'foo'
    
        Note: A fast-path exists for iso8601-formatted dates.
    infer_datetime_format : boolean, default False
        If True and parse_dates is enabled, pandas will attempt to infer the format
        of the datetime strings in the columns, and if it can be inferred, switch
        to a faster method of parsing them. In some cases this can increase the
        parsing speed by ~5-10x.
    keep_date_col : boolean, default False
        If True and parse_dates specifies combining multiple columns then
        keep the original columns.
    date_parser : function, default None
        Function to use for converting a sequence of string columns to an array of
        datetime instances. The default uses ``dateutil.parser.parser`` to do the
        conversion. Pandas will try to call date_parser in three different ways,
        advancing to the next if an exception occurs: 1) Pass one or more arrays
        (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the
        string values from the columns defined by parse_dates into a single array
        and pass that; and 3) call date_parser once for each row using one or more
        strings (corresponding to the columns defined by parse_dates) as arguments.
    dayfirst : boolean, default False
        DD/MM format dates, international and European format
    iterator : boolean, default False
        Return TextFileReader object for iteration or getting chunks with
        ``get_chunk()``.
    chunksize : int, default None
        Return TextFileReader object for iteration. `See IO Tools docs for more
        information
        <http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_ on
        ``iterator`` and ``chunksize``.
    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
        For on-the-fly decompression of on-disk data. If 'infer', then use gzip,
        bz2, zip or xz if filepath_or_buffer is a string ending in '.gz', '.bz2',
        '.zip', or 'xz', respectively, and no decompression otherwise. If using
        'zip', the ZIP file must contain only one data file to be read in.
        Set to None for no decompression.
    
        .. versionadded:: 0.18.1 support for 'zip' and 'xz' compression.
    
    thousands : str, default None
        Thousands separator
    decimal : str, default '.'
        Character to recognize as decimal point (e.g. use ',' for European data).
    lineterminator : str (length 1), default None
        Character to break file into lines. Only valid with C parser.
    quotechar : str (length 1), optional
        The character used to denote the start and end of a quoted item. Quoted
        items can include the delimiter and it will be ignored.
    quoting : int or csv.QUOTE_* instance, default None
        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of
        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).
        Default (None) results in QUOTE_MINIMAL behavior.
    escapechar : str (length 1), default None
        One-character string used to escape delimiter when quoting is QUOTE_NONE.
    comment : str, default None
        Indicates remainder of line should not be parsed. If found at the beginning
        of a line, the line will be ignored altogether. This parameter must be a
        single character. Like empty lines (as long as ``skip_blank_lines=True``),
        fully commented lines are ignored by the parameter `header` but not by
        `skiprows`. For example, if comment='#', parsing '#empty\na,b,c\n1,2,3'
        with `header=0` will result in 'a,b,c' being
        treated as the header.
    encoding : str, default None
        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python
        standard encodings
        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_
    dialect : str or csv.Dialect instance, default None
        If None defaults to Excel dialect. Ignored if sep longer than 1 char
        See csv.Dialect documentation for more details
    tupleize_cols : boolean, default False
        Leave a list of tuples on columns as is (default is to convert to
        a Multi Index on the columns)
    error_bad_lines : boolean, default True
        Lines with too many fields (e.g. a csv line with too many commas) will by
        default cause an exception to be raised, and no DataFrame will be returned.
        If False, then these "bad lines" will dropped from the DataFrame that is
        returned. (Only valid with C parser)
    warn_bad_lines : boolean, default True
        If error_bad_lines is False, and warn_bad_lines is True, a warning for each
        "bad line" will be output. (Only valid with C parser).
    
    Returns
    -------
    result : DataFrame or TextParser

Slide Type

left_frame = pd.DataFrame({'key': range(5), 
                           'left_value': ['a', 'b', 'c', 'd', 'e']})
right_frame = pd.DataFrame({'key': range(2, 7), 
                           'right_value': ['f', 'g', 'h', 'i', 'j']})
print(left_frame)
print()
print(right_frame)
   key left_value
0    0          a
1    1          b
2    2          c
3    3          d
4    4          e

   key right_value
0    2           f
1    3           g
2    4           h
3    5           i
4    6           j
Slide Type

pd.merge(left_frame, right_frame, on='key')
key	left_value	right_value
0	2	c	f
1	3	d	g
2	4	e	h
Slide Type

pd.merge(left_frame, right_frame, on='key', how='left')
key	left_value	right_value
0	0	a	NaN
1	1	b	NaN
2	2	c	f
3	3	d	g
4	4	e	h
Slide Type

pd.merge(left_frame, right_frame, on='key', how='right')
key	left_value	right_value
0	2.0	c	f
1	3.0	d	g
2	4.0	e	h
3	5.0	NaN	i
4	6.0	NaN	j
Slide Type

pd.merge(left_frame, right_frame, on='key', how='outer')
key	left_value	right_value
0	0.0	a	NaN
1	1.0	b	NaN
2	2.0	c	f
3	3.0	d	g
4	4.0	e	h
5	5.0	NaN	i
6	6.0	NaN	j
Slide Type

pd.concat([left_frame, right_frame])
key	left_value	right_value
0	0	a	NaN
1	1	b	NaN
2	2	c	NaN
3	3	d	NaN
4	4	e	NaN
0	2	NaN	f
1	3	NaN	g
2	4	NaN	h
3	5	NaN	i
4	6	NaN	j
Slide Type

pd.concat([left_frame, right_frame], axis=1)
key	left_value	key	right_value
0	0	a	2	f
1	1	b	3	g
2	2	c	4	h
3	3	d	5	i
4	4	e	6	j
Slide Type

​

​
