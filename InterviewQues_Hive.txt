hdfs dfs -ls "/user/shubhro2705854012/hive_shubhro"
hdfs dfs -get "/user/shubhro2705854012/hive_shubhro/customer_information.txt"
------------------------
4000001,Kristina,Chung,55,Pilot
4000002,Paige,Chen,74,Teacher
4000003,Sherri,Melton,34,Firefighter
4000004,Gretchen,Hill,66,Computer hardware engineer
------------------------
CREATE DATABASE sb_hive_customer_info;
USE sb_hive_customer_info;

CREATE EXTERNAL TABLE sb_cus_info_unp(customer_id INT,first_name STRING,last_name STRING,customer_age INT,cust_prof STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

LOAD DATA INPATH '/user/shubhro2705854012/hive_shubhro/customer_information.txt' OVERWRITE INTO TABLE sb_cus_info_unp;

set hive.exec.dynamic.partition=true;       
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions = 1000;
set hive.exec.max.dynamic.partitions.pernode=100;

CREATE EXTERNAL TABLE sb_cus_info_par(customer_id INT,first_name STRING,last_name STRING,customer_age INT)
PARTITIONED BY (cust_prof STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

INSERT INTO sb_cus_info_par PARTITION(cust_prof) SELECT * FROM sb_cus_info_unp;

-------
Buck:
CREATE TABLE sb_cust_info_buck (customer_id INT,first_name STRING,last_name STRING,customer_age INT)
PARTITIONED BY (cust_prof STRING)
CLUSTERED BY (customer_age) INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

INSERT INTO sb_cust_info_buck PARTITION(cust_prof) SELECT * FROM sb_cus_info_unp;

--
CREATE TABLE sb_cust_info_buck2 (customer_id INT,first_name STRING,last_name STRING,customer_age INT,cust_prof STRING)
CLUSTERED BY (customer_age) INTO 10 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

INSERT INTO sb_cust_info_buck2 SELECT * FROM sb_cus_info_unp;

################################################################
1st link: https://www.dezyre.com/article/hive-interview-questions-and-answers-for-2018/246
2nd link: https://data-flair.training/blogs/apache-hive-interview-questions-2018/
3rd link: https://intellipaat.com/interview-question/hive-interview-questions/
4th link: https://www.edureka.co/blog/interview-questions/hive-interview-questions/
5th Link: https://data-flair.training/blogs/hive-interview-questions-and-answers-for-experience/
6th Link: https://www.tutorialspoint.com/hive/hive_interview_questions.htm
------------------------------------------------------------------
Hive Architecture:

Hive Client [(Thrift API -> Thrift client) OR (JDBC API -> Hive JDBC Driver) OR (ODBC API -> Hive ODBC Driver)]
-> Hive Services [(CLI),(Hive Web Interface),(Hive server) -> Hive Driver -> metastore -> Derby Db]
-> Resource Manager [MapRV1, MapRV2, YARN, TEZ]
-> distributed Storage [HDFS]				

Is Hive suitable to be used for OLTP (online transaction processing) systems? 
No, it is not suitable for OLTP system since it does not offer insert and update at the row level.

Where does the data of a Hive table gets stored?
in cloudx lab, the DB will be created in /apps/hive/warehouse/, in cloudera it will be created in /hive/warehouse
However, we can explicitely provide location by configuring hive-site.xml property: hive.metastore.warehouse.dir 

----------------------------------------------------------------
How Hive distributes the rows into buckets?

Basically, hash_function depends on the column data type. 
for int data type, hash_function (bucketing_column) modulo (num_of_buckets)
for string and bucketing within partition, the hashing function is more complex.

What will happen in case you have not issued the command:  
‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive in Apache Hive 0.x or 1.x?

The command:  ‘SET hive.enforce.bucketing=true;’ allows one to have the correct number of reducer while using ‘CLUSTER BY’ 
clause for bucketing a column. In case it’s not done, one may find the number of files that will be generated in the table 
directory to be not equal to the number of buckets. As an alternative, one may also set the number of reducer equal to the 
number of buckets by using set mapred.reduce.task = num_bucket.

----------------------------------------------------------------
In Dynamic partition, the data is read and partitioned thru the map-reduce job based on the field specified in the table to be used for partitioning.

How to recover partitions in easy fashion. Here is the scenario
Have 'n' partitions on existing external table 't'
Dropped table 't'
Recreated table 't' // Note : same table but with excluding some column
How to recover the 'n' partitions that existed for table 't' in step #1 ?

An. 
1) manually alter table to add 'n' partition by writing some script. 
2) When the partitions directories still exist in the HDFS, we can run: MSCK REPAIR TABLE table_name;
3) In case of internal/ managed tables you can recover the data from .TRASH directory

------------------------------------------------------------------

Hive Metastore: stores metadata for Hive tables (like their schema and location) and partitions in a relational database.
                "It provides client access to this information by using metastore service API."
				
				Hive metastore consists of two fundamental units:
				1) metastore service to provide access to metastore to various hive services*.
				2) disk storage of hive metadata (i.e. not in HDFS)
				
				3 modes of Hive metastore deployment:
				1) Embedded Metastore. (DEFAULT)
				2) local Metastore.
				3) Remote Metastore.
				
				Embedded M: both metastore service and Hive service run in the same JVM by using an embedded Derby database.
				            Limitation of this mode is that only ONE hive session can be open at a time as only one embedded derby DB can access disk to get the metadata.
							
							the architecture will look like:
							drive (hive service JVM) -> Metastore -> Derby
							
				local M: this mode helps overcome the limited for embedded M by allowing multuple hive sessions i.e. multiple users can access metastore at the same time.
						 This is achieved by using a JDBC compliant DB like mySQL. In this model, mySQL runs on a different server (i.e. different machines) 
						 than (hive service+metastore service).
						 
						 Architecture will look like:
						 drive -> Metastore ->
                                             |
											 --> mySQL
											 |
						 drive -> Metastore ->
						 
				Remote M: in this mode (unlike embedded and local), metastore service runs on different JVM's than Hive service.
				          Mutiple metastore services can be configured so that different processes can communicate with metastore services via thrift network API.
						  Mutiple metastore servers help provide more availability and is more manageable.
						  to configure this mode, in hive-site.xml, change hive.metastore.uris property to point to the trift server location
						

				NOTE:
				When running Hive in embedded mode, it creates a local metastore. 
				When you run the query, it first checks whether a metastore already exists or not. 
				The property javax.jdo.option.ConnectionURL defined in the hive-site.xml has a default value jdbc: derby: databaseName=metastore_db; create=true.
				which implies that embedded derby will be used as the Hive metastore and the location of the metastore is metastore_db which will be created only if it does not 
				exist already.
				The location metastore_db is a relative location so when you run queries from different directories it gets created at all places from wherever you launch hive.
				This property can be altered in the hive-site.xml file to an absolute path.
				
Why does Hive not store metadata information in HDFS?
to achieve low latency we use RDBMS. Because  HDFS read/write operations are time-consuming processes.

Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient?
1) While we use derby metastore, Then lock file would be there in case of the abnormal exit.
	Hence, do remove the lock file rm metastore_db/*.lck
	
2) Run hive in Debug mode, hive -hiveconf hive.root.logger=DEBUG,console
				
---------
How will you read and write HDFS files in Hive?
i)   TextInputFormat- This class is used to read data in plain text file format.
ii)  HiveIgnoreKeyTextOutputFormat- This class is used to write data in plain text file format.
iii) SequenceFileInputFormat- This class is used to read data in hadoop SequenceFile format.
iv)  SequenceFileOutputFormat- This class is used to write data in hadoop SequenceFile format.
	
--------
Hive query processor: service which converts SQL to a graph of map reduce jobs so jobs can be executed in the order of dependencies.
components include:
1) parser
2) semantic analyzer
3) type checking
4) logical plan generation
5) optimizer
6) physical plan generation
7) execution engine
8) operators 	
9) UDF and UDAF

-------
"Describe" and "describe extended"...difference?

		Describe									Describe Extended
1		describe works for both						DE works only on tables
		DB and tables.
2		D when used on DB will give                 DE on DB fails with 
		the DB name, location, creator,             "ParseException line 1:18 cannot recognize input near 'DATABASE'"
3		D when used on table will give				DE when used on table will give table information (name, DB, owner, create time, last access)
		field names, field data types,				field schema, bucketing columns (if any), partition information, table type (managed/external)	
		partition information (if any)
		
NOTE: to see the tables within a DB, use SHOW TABLES
	  hive query to view all the databases whose name begins with “db”: SHOW DATABASES LIKE ‘db.*’

-------
How to run UNIX commands within Hive shell?
prepend the unix command with a "!". example, to know the current working directory in Unix, we can use "!pwd;"

------
Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive:
SORT BY – Data is ordered at each of ‘N’ reducers where the reducers can have overlapping range of data.

ORDER BY- This is similar to the SORT BY in SQL where total ordering of data takes place by passing it to a single reducer.
		  guarantees global ordering, but unacceptable for large datasets. You end up one sorted file as output.

DISTRIBUTE BY – It is used to distribute the rows among the reducers. Rows that have the same distribute by columns will go to the same reducer.
				example,
				SELECT col1, col2 FROM t1 DISTRIBUTE BY col1
				SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC				

CLUSTER BY- It is a combination of DISTRIBUTE BY and SORT BY where each of the N reducers gets non overlapping range of data which is then sorted by those ranges at the respective 
            reducers.This gives you global ordering. example,
			SELECT col1, col2 FROM t1 CLUSTER BY col1
			
------
explain explode() in Hive?

A lateral view with explode() can be used to convert collection(like array, map....) into separate rows using the query


I/p data: (explod_data)
Amar:"Btech","Mtech"
Amala:"Bsc","Msc","Mtech"
Akash:"Btech","Mba"

CREATE TABLE explod_tab(name STRING, degr array<STRING>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY ','
LINES TERMINATED BY '\n';

LOAD DATA LOCAL INPATH 'explod_data' OVERWRITE INTO TABLE explod_tab;

select * from explod_tab

	o/p:
	Amar    ["\"Btech\"","\"Mtech\""]
	Amala   ["\"Bsc\"","\"Msc\"","\"Mtech\""]
	Akash   ["\"Btech\"","\"Mba\""]

select name, myq from explod_tab lateral view explode(degr) q as myq;

	O/p:
	Amar    "Btech"
	Amar    "Mtech"
	Amala   "Bsc"
	Amala   "Msc"
	Amala   "Mtech"
	Akash   "Btech"
	Akash   "Mba"
	
----------
How can you prevent a large job from running for a long time?

This can be achieved by setting the MapReduce jobs to execute in strict mode set hive.mapred.mode=strict;
The strict mode ensures that the queries on partitioned tables cannot execute without defining a WHERE clause.

---------
Explain about the different types of join in Hive.
HiveQL has 4 different types of joins –
JOIN- Similar to Outer Join in SQL
FULL OUTER JOIN – Combines the records of both the left and right outer tables that fulfil the join condition.
LEFT OUTER JOIN- All the rows from the left table are returned even if there are no matches in the right table.
RIGHT OUTER JOIN-All the rows from the right table are returned even if there are no matches in the left table.

--------
Indexing in Hive table:
index acts as a reference to the records, so whenever we query a large dataset (terabytes and petabytes of data), intead of searching the entire data,
it looks for the index first and then go the required column and perform the query.

Indexing is preferred when,
1) if DS is large
2) if query execution takes considerable time.
3) build on the columns on which you frequently perform operations

Indexing should not be done when,
1) Building more number of indexes also degrade the performance of your query.
2) Type of index to be created should be identified prior to its creation else it degrades query performance.

NOTE: Indexes are maintained in a separate table, so it does not impact the data in the original table. 
      Indexes can also be partitioned for large datasets.
	  
Hive allows 2 types of indexing:
1) compact indexing: Compact indexing stores the pair of indexed column’s value and its blockid.
2) bitmap indexing : Bitmap indexing stores the combination of indexed column value and list of rows as a bitmap (combination of value and list of rows as a digital image).

In Hive, we can, Create index, Show index, Alter index, Drop index

creating index:
STEP-1:
	CREATE INDEX index_name
	 ON TABLE table_name (columns,....)
	 AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'   //built in handler implies we are creating a compact index.
	 WITH DEFERRED REBUILD;                                             // allows the index to be altered at later stages.
	 
	//To create a bitmap index,
	CREATE INDEX index_name
	 ON TABLE table_name (age)
	 AS 'BITMAP'
	 WITH DEFERRED REBUILD;
  
	Above will create an index, but to complete the creation, we will use ALTER INDEX command to launch mapreduce job for creation.
	
STEP-2:
	ALTER INDEX index_name on table_name REBUILD;             // will complete our REBUILDED index creation for the table. Compact Index
	ALTER INDEX index_name on table_name REBUILD;  // bitmap index
	
	example,
	create table olympic(athelete STRING,age INT,country STRING,year STRING,closing STRING,sport STRING,gold INT,silver INT,bronze INT,total INT) 
	row format delimited 
	fields terminated by '\t' stored as textfile;
	
	load data local inpath "...." overwrite into table olympic;
	
	CREATE INDEX olympic_index
	ON TABLE olympic (age)
	AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
	WITH DEFERRED REBUILD;
	
	ALTER INDEX olympic index on olympic REBUILD;
	
	//create a bitmap index on the same column
	CREATE INDEX olympic_index_bitmap
	ON TABLE olympic (age)
	AS BITMAP
	WITH DEFERRED REBUILD;	
 
	ALTER INDEX olympic_index_bitmap ON olympic REBUILD;
	
Showing indexes:
	SHOW FORMATTED INDEX ON olympic; //will show index name, table name, column name, index type
	
Drop Index:
	DROP INDEX IF EXISTS <index_name> ON <table_name>;
	example, DROP INDEX IF EXISTS olympic_index ON olympic;
	
------------------
Will the reducer work or not if you use “Limit 1” in any HiveQL query?  NO.

-----------------
How will you optimize Hive performance?

There are various ways to run Hive queries faster -
Using Apache Tez execution engine
Using vectorization
Using ORCFILE
Do cost based query optimization.


What is TEZ?
	TEZ is a DAG architecture which works very similar to spark in the sense that:
	1) Execute the plan but no need to read data from disk.
	2) Once ready to do some calculations (similar to actions in spark), get the data from disk and perform all steps and produce output.

	Intermediate results are stored in memory (not written to disks). 
	On top of that there is vectorization (process batch of rows instead of one row at a time). All this adds to efficiencies in query time.

	TEZ was introduced by Hortonworks as an alternative to mapreduce, though having a very high response time than traditional map reduce.
	Hive and pig can use this framework to,
	1) express query logic efficiently
	2) execute it with high performance.

what is ORC file?
	ORC is abbreviation for Optimized Row Columnar, provides an efficient way of storing Hive data.
	The filestructure is such that file contains:
	1) groups of row data called stripes.
	2) info about all the stripes of a file in file footer. Contains info like number of rows per stripe, each columns data type etc.

	Note: default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS.

	HiveQL Syntax,
	1) CREATE TABLE ... STORED AS ORC
	2) ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC
	3) SET hive.default.fileformat=Orc

	example,
	create table Addresses (
	  name string,
	  street string,
	  city string,
	  state string,
	  zip int
	) stored as orc tblproperties ("orc.compress"="NONE");  //this implies no High level compression. Other values are, ZLIB, SNAPPY.
	
what is vectorization?
	allows Hive to process a batch of rows together instead of processing one row at a time (default in Hive). 
	Introduced in hive-0.13, helps to improve performance of operations like scans, aggregations, filters and joins.
	Vectorized query execution streamlines operations by processing a block of 1024 rows at a time. 
	Within the block, each column is stored as a vector (an array of a primitive data type).

	In order to enable vector execution, enable following property - set hive.vectorized.execution.enabled = true
	
--------------------------------------------------
Change settings within Hive Session:
using the SET command.
example, hive> SET hive.enforce.bucketing=true;

to view the current value of a variable, we use "set", example,
	hive> SET hive.enforce.bucketing;
	hive.enforce.bucketing=true

NOTE: this list will not include defaults of Hadoop.

to view defaults of hadoop from within hive shell, we use, SET -v

--------------------------------------------------
Hive Variables:
A variable in hive that is created in Hive variable and referenced from Hive scripting languages.

Creating a hive variable in hive shell:
Method-1:
	1) hive -hiveconf myvar=2016-01-01 (this has to be done outside hive shell, it will launch Hive shell)
	2) set myvar; (o/p: myvar=2016-01-01)

Method-2:
	1) within hive CLI, set myvar=2061-01-20
	2) set myvar; (o/p: myvar1=2061-01-01)

Method-3:
	1) set hivevar:tablename=mytable;
	2) select * from ${tablename} OR select * from ${hivevar:tablename}
	3) In case I set up a local variable from Hive shell as,
		set tablename=newtable;
		then,
		select * from ${tablename} -- uses 'newtable'
		select * from ${hivevar:tablename} -- still uses the original 'mytable'
		
Writing Hive scripts (.sql files or .hql files)
1) file should be saved with .sql extension.
	example,(tryHiveScript.sql)
	CREATE TABLE explod_tab1(name STRING, degr array<STRING>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ':' COLLECTION ITEMS TERMINATED BY ',' LINES TERMINATED BY '\n';
	DESCRIBE explod_tab1;
	LOAD DATA LOCAL INPATH 'explod_data' OVERWRITE INTO TABLE explod_tab1;
	SELECT * FROM explod_tab1;

2) for running, we will use the following command
	hive -f tryHiveScript.sql
	
3) passing variables to hive script from command line:
	 hive -hiveconf DATE='20160101' -f myHive.hql
	 
	 
precedence order of HIVE configuration:
SET Command in HIVE
The command line –hiveconf option
Hive-site.XML
Hive-default.xml
Hadoop-site.xml
Hadoop-default.xm
	 
------------------------------------------------------------------------
Hive Functions: CONCATENATE, TRIM and REVERSE (used with SELECT)

1) concatenate: concatenates a number of strings separated by ','
	a) CONCAT ('Intellipaat','-','is','-','a','-','eLearning',’-’,’provider’);
		o/p: Intellipaat-is-a-eLearning-provider
		
	b) Concatenate with delimiter:
		CONCAT_WS (':',’Intellipaat’,’is’,’a’,’eLearning’,‘provider’);
		
2) trim: trims the trailing and leading spaces
	a) TRIM('   Shubhro     '): o/p is Shubhro
	b) LTRIM
	c) RTRIM
	
3) reverse : REVERSE('abcd') : o/p is dcba

Other string functions can be found in http://www.folkstalk.com/2011/11/string-functions-in-hive.html.

--------------------------------------------------------------------
change the column data type in Hive:
	ALTER TABLE table_name CHANGE <column_name> <column_name> <new_datatype>;
	example,
	ALTER TABLE employee CHANGE salary salary BIGINT
	will change datatype of salary from INT to BIGINT

what is RLIKE?
	Its full form is Right-Like.It helps to examine the two substrings. 
	i.e, if the substring of A matches with B then it evaluates to true.
	example,
	‘Intellipaat’ RLIKE ‘tell’ will return True
	‘Intellipaat’ RLIKE ‘^I.*’ will also return True
	
---------------------------------------------------------------------
Suppose, I have a lot of small CSV files present in /input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

One can use the SequenceFile format which will group these small files together to form a single sequence file. The steps that will be followed in doing so are as follows:

Create a temporary table:
CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING)

ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS TEXTFILE;

Load the data into temp_table:
LOAD DATA INPATH ‘/input’ INTO TABLE temp_table;

Create a table that will store data in SequenceFile format:
CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING)

ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS SEQUENCEFILE;

Transfer the data from the temporary table into the sample_seqfile table:
INSERT OVERWRITE TABLE sample SELECT * FROM temp_table;

Hence, a single SequenceFile is generated which contains the data present in all of the input files and therefore, the problem of having lots of small files is finally eliminated.

-----------------------------------------------
Regular expressions in Hive:

Reference: https://stackoverflow.com/questions/46060475/regular-expression-in-hive


regexp_extract(string subject, string pattern, int index)
*****You must provide the 3rd argument, the group index to extract.

Returns the string extracted using the pattern.

NOTE:
1) using '\s' as the second argument will match the letter s; '\\s' is necessary to match whitespace, etc
2) The 'index' parameter is the Java regex Matcher group() method index. Details in 		
	https://docs.oracle.com/javase/8/docs/api/java/util/regex/Matcher.html

example,
	select REGEXP_EXTRACT( 'Hello, my name is Ben. Please visit' , 'Ben', 0)
	o/p: Ben


We can use the following to replace:
	regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)
	example,
	regexp_replace("foobar", "oo|ar", "") returns 'fb.'
	
	
2nd option is RLIKE (reference: https://codereview.stackexchange.com/questions/172748/regex-in-hive-ql-rlike)

performance: These queries will certainly have to read all rows, which costs more CPU cycles than evaluating the regexes.

example,

	I/p:
		groups     user     age
		X124       john     23
		XY22#AB    mike     33
		AB         peter    21
		X122#XY01  francis  43

	Problem:
	select 
	  count(*)
	from 
	  users
	where
	  groups not rlike '^(X[Y1-9][0-9]{2,2})(#X[Y1-9][0-9]{2,2})*$'

--------------------------------------------------------------------
Hive UDF's in Python

https://acadgild.com/blog/hive-udf-python