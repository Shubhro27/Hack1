Link-1: https://www.dezyre.com/article/pig-interview-questions-and-answers-for-2018/244
Link-2: https://intellipaat.com/interview-question/pig-interview-questions/
Link-3: https://mindmajix.com/apache-pig-interview-questions
-------------------------------------------------------------------------------------------------
Modes of Execution for Apache Pig:
1) Local Mode   : pig -x local
2) Cluster Mode : pig

Interviewer may also expect,
1) grunt shell mode.
2) batch/script mode.
3) embedded mode.

Pig programming is done via pig latin. To execute a statement, Pig requires an execution engine. So
Pig Engine (converts statements into MapReduce jobs) -> map Reduce Engine then executes this set of Map-reduce jobs.

Is Pig Latin strongly typed?
If we have defined a schema for the data item, Pig will expect the data to come in the same format as defines, So strongly typed
If however, we skip the schema, PIG allows semi structured data to be processed, but it is coder's overhead to remember and maintain the positioning of data. 

Difference between Pig and Hive:
Pig								 		Hive
Pig is a Yahoo Product.			 		Hive is a facebook product.
Pig is a dataflow language.				Hive supports a data warehouse architecture.
Pig resides on client machine.   		Hive resides on server machine.
Pig supports semi structured format		Hive is strictly structured.
Pig implements DAG						Hive executes the code at run time
PLatin is procedural					HiveQL is declarative language
PLatin does not require a schema		HiveQL requires a Schema
PLatin has nested relational model		HiveQL has flat relational Model

Difference between Pig and Mapreduce:
Pig										MR
Pig is a scripting language.			MR is a compiled language.
High level of abstraction.				Low level of Abstraction.
Pig uses less lines of coding.			More lines of coding.
Pig implements DAG						MR executes the code at run time

Difference between Pig and SQL
Pig														SQL
Pig resides on HDFS 									SQL resides on DB server
PIG implements DAG, scripting language					SQL is compiled language
Pig built-in can split a data processing stream			SQL cannot
Pig build in allows loading the data for operations		SQL requires interrogating the DB for every transaction.
--------------------------------------------------------------------------------------------------
Comments in PIG can be put in "/*.....*/

Pig single line and multiline commands:
Single line: if we have a single line command, PIG will directly execute it.
Multi line : we will have to store it in a pig script and execute it via,
			 pig -x mapreduce *.pig  //for cluser mode
			 pig -x local *.pig      //for local mode 
--------------------------------------------------------------------------------------------------
What are the execution plans in PIG:
Pig scripts are based on interpreter checking.
1) Logical Plan  : is produced after semantic checking and basic parsing but no data processing takes place during creation of this plan.
				   Each statement in the script has its own logical plan which is formed after syntax checking on the operators. If an issue is encountered,
				   an exception is thrown.
2) Physical Plan : A physical plan is a series of map-reduce jobs, although it does not have any reference on the execution path.
				    Load and store functions usually get resolved in the physical plan.
					
Interesting Fact:
During the creation of physical plan, cogroup logical operator is converted into 3 physical operators namely –Local Rearrange, Global Rearrange and Package.
--------------------------------------------------------------------------------------------------
Operators In Pig:

1) Arithmetic Operators:  Addition, substraction, multiplication, modulus, division etc. Operates on 2 atoms in a tuple or bag.
2) Comparision Operators: ==, >, <......
3) Type Construction Operator: Used to construct a complex data type  from scalar data types
	a) Tuple constructor operator: (). ex, (Raju, 30)
	b) Bag constructor operator: {}  . ex, {(Raju, 30), (Mohammad, 45)}
	c) Map constructor operator: []  . ex, [name#Raja, age#30]
4) Relational Operators: it takes an expression and applies them to all the records in the data item. example, foreach, order by, group, distinct, Filter etc.
	Categories,
	a) Loading and Storing
	b) Filtering: FILTER, DISTINCT, FOREACH
	c) Grouping and joining: JOIN, GROUP, COGROUP, CROSS
	d) Sorting: ORDER, LIMIT
	e) Combining/Splitting: UNON, SPLIT
	f) Diagnostic: DESCRIBE, EXPLAIN.......
--------------------------------------------------------------------------------------------------
Mapfile and bloomsMapFile in Hadoop:

mapFile: is a directory which comprises of 2 sequential files: data file and index file. It appends key-Value pairs as key and offset as index.
		 This allows for a fast lookup, as instead of scanning all the records till a key is found, we scan the index, which has lesser entries.
		 
BloomMapFile: is a class which extends MapFile. It uses HBASE table format and expedites the search for a file/block than mapFile.
--------------------------------------------------------------------------------------------------
Complex datatypes in PIG:
1) Maps (KeyValue pairs separated by #)
2) Tuples (uneditable array of heterogeneous elements). Encapsulated within ()
3) Bag (group of tuples): encapsulated within {}

Map in more details:
Usually maps are not preferred in PIG. Limitation is one can not lookup variable key in the Map in Pig i.e. key needs to be constant.
ex, e.g. myMap#'keyFoo' is allowed but myMap#$SOME_VARIABLE is not allowed.

We need maps as, usually Hadoop data are the dumps of different data sources from Traditional languages. 
If original data sources contain Maps, the HDFS data would contain a corresponding Map. example,HttpRequest header#'clientIp'.

Handling map datatype:
1) adding in schema as map[chararray]
	example,
		a = LOAD 'pigtest.csv' using PigStorage('|') AS (employee_id:int, email:chararray, name:tuple(first_name:chararray, middle_name:chararray, last_name:chararray), 
														 project_list:bag{project: tuple(project_name:chararray)}, skills:map[chararray]) ;

		b = FOREACH a GENERATE employee_id, email, name.first_name, project_list, skills#'programming' ;

		dump b

2) Using TOMAP() [post version 0.10]
	example,
		I/p:
			001,Robin,22,newyork
			002,BOB,23,Kolkata
			003,Maya,23,Tokyo
			004,Sara,25,London 
			005,David,23,Bhuwaneshwar 
			006,Maggy,22,Chennai
			
		code:
			emp_data = LOAD 'employee_details.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, city:chararray);
			tomap = FOREACH emp_data GENERATE TOMAP(name, age);
			DUMP tomap;
			
		O/p:
			([Robin#22])
			([BOB#23])
			([Maya#23])
			([Sara#25]) 
			([David#23])
			([Maggy#22])
--------------------------------------------------------------------------------------------------
CO-GROUP Example,
	GROUP-BY Operates on only one relation (Bag), CO-GROUP works on more than one relation.
	example,
	1st file: student_details.txt
		001,Rajiv,Reddy,21,9848022337,Hyderabad
		002,siddarth,Battacharya,22,9848022338,Kolkata
		003,Rajesh,Khanna,22,9848022339,Delhi
		004,Preethi,Agarwal,21,9848022330,Pune
		005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar
		006,Archana,Mishra,23,9848022335,Chennai
		007,Komal,Nayak,24,9848022334,trivendram
		008,Bharathi,Nambiayar,24,9848022333,Chennai

	2nd file: employee_details.txt
		001,Robin,22,newyork 
		002,BOB,23,Kolkata 
		003,Maya,23,Tokyo 
		004,Sara,25,London 
		005,David,23,Bhuwaneshwar 
		006,Maggy,22,Chennai
		
	STEP-1: load the files into relations:
	student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',') 
	as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray); 
  
	employee_details = LOAD 'hdfs://localhost:9000/pig_data/employee_details.txt' USING PigStorage(',') as (id:int, name:chararray, age:int, city:chararray);
	
	STEP-2: cogroup then on the basis of age,
	cogroup_data = COGROUP student_details by age, employee_details by age;
	
	O/p: (DUMP cogroup_data)
		(21,{(4,Preethi,Agarwal,21,9848022330,Pune), (1,Rajiv,Reddy,21,9848022337,Hyderabad)}, {    })  
		(22,{ (3,Rajesh,Khanna,22,9848022339,Delhi), (2,siddarth,Battacharya,22,9848022338,Kolkata) },{ (6,Maggy,22,Chennai),(1,Robin,22,newyork) })  
		(23,{(6,Archana,Mishra,23,9848022335,Chennai),(5,Trupthi,Mohanthy,23,9848022336 ,Bhuwaneshwar)},{(5,David,23,Bhuwaneshwar),(3,Maya,23,Tokyo),(2,BOB,23,Kolkata)}) 
		(24,{(8,Bharathi,Nambiayar,24,9848022333,Chennai),(7,Komal,Nayak,24,9848022334, trivendram)}, { })  
		(25,{   },{(4,Sara,25,London)})

--------------------------------------------------------------------
How to execute shell commands from within Grunt shell:
1) sh command: example, "sh ls" will list all the files /pig/bin directory.
			   NOTE: with sh, we cannot execute the commands that are a part of the shell environment (example - cd).
2) fs command: we can invoke any FsShell (file system shell) commands from the Grunt shell.
			   example, fs -ls.
			   
How to run a pig script from grunt shell:
1) run command: syntax is run [-param param-name = <param-value>][-param_file file_name] script
	example,
	i/p file: student.txt
				001,Rajiv,Hyderabad
				002,siddarth,Kolkata
				003,Rajesh,Delhi
				
	sample_script.pig:
		student = LOAD 'hdfs://localhost:9000/pig_data/student.txt' USING PigStorage(',') as (id:int,name:chararray,city:chararray);
		
	run this script:
		grunt> run /sample_script.pig
		
	to check the o/p:
		grunt> Dump;

		(1,Rajiv,Hyderabad)
		(2,siddarth,Kolkata)
		(3,Rajesh,Delhi)
		
2) running pig script with params example,
	syntax is : run [-param param-name = <param-value>][-param_file file_name] script
	          OR run [-p = <param-value>][-m file_name] script
			  
	example,
	i/p file: student.txt
				001,Rajiv,Reddy,21,9848022337,Hyderabad
				002,siddarth,Battacharya,22,9848022338,Kolkata
				003,Rajesh,Khanna,22,9848022339,Delhi
				004,Preethi,Agarwal,21,9848022330,Pune
				005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar
				006,Archana,Mishra,23,9848022335,Chennai
				007,Komal,Nayak,24,9848022334,trivendram
				008,Bharathi,Nambiayar,24,9848022333,Chennai
				
	params.init (file to hold all parameters)
				fileName='hdfs://horton/user/jgosalia/students.txt'
				cityName='Chennai'
				
	filter.pig
			students = LOAD '$fileName' USING PigStorage(',') AS (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray);
			students = FILTER students BY city == '$cityName';
			DUMP students;
			
			
	Run command:
	1) pig -param fileName='hdfs://horton/user/jgosalia/students.txt' -param cityName='Chennai' filter.pig
	2) pig -param_file params.init filter.pig
		
What is difference between run command and exec command:
with run, the statements from the script are available in the command history.

------------------------------------------------------------------------
What are Pig diagnostic operators:

By diagnostic operators (also called exception handling operators), we mean operators which verify the result of a LOAD command.
there are 4 operators as such,
1) Dump operator
2) Describe operator : view the schema of a relation.
3) Explanation operator : display the logical, physical, and MapReduce execution plans of a relation.
						  so, the output of "explain student;" will comprise of:
						  LogicalPlanOptimizer, "New Logical Plan", "Physical Plan", "map-Reduce Plan".
4) Illustration operator : "illustrate student;" provides step-by-step execution of a sequence of statements.

When is Explain utility used and when is Illustrate Preferred?
Explain Utility: to debug error or optimize PigLatin scripts. Explain can be applied to an alias of a script or complete script in grunt shell.

Illustrate Utility: if a script has a join or filter operation,illustrate takes a sample from the data and whenever it comes across operators like join or filter 
					that remove data, it ensures that only some records pass through and some do not.
					illustrate just shows the output of each stage but does not run any MapReduce task.

Explain plan format:
EXPLAIN [-script pigscript] [-out path] [-brief] [-dot] [-paramparam_name = param_value] [-param_filefile_name] alias;
out   : Used to specify the output path (directory)
brief :Does not expand nested plans
dot   : utility which will generate a directed-acyclic-graph (DAG) of the plans in any supported format (.gif, .jpg …).
Alias : name of a relation.
paramparam_name = param_value : used to see the parameters					
-------------------------------------------------------------------------
What are the kind of joins available in Pig:
1) self join
2) inner join
3) Outer join:
	a) Full outer join : grunt> outer_full = JOIN customers BY id FULL OUTER, orders BY customer_id;
	b) Left outer join : grunt> outer_left = JOIN customers BY id LEFT OUTER, orders BY customer_id;
	c) Right outer join: grunt> outer_right = JOIN customers BY id RIGHT, orders BY customer_id;

	
what is CROSS Operator: it provides the cartesian product of 2 relations. example, grunt> Relation3_name = CROSS Relation1_name, Relation2_name;

What is Union and what is split operator in PIG?
Union will merge the contents of 2 relations. example, grunt> Relation_name3 = UNION Relation_name1, Relation_name2;
Split will split the content of a relation into 2 or more relations based on a condition.
example, SPLIT Relation1_name INTO Relation2_name IF (condition1), Relation2_name (condition2),
		 SPLIT student_details into student_details1 if age<23,student_details2 if (22<age and age>25);
		 
------------------------------------------------------------------------
What is difference between COUNT() and COUNT_STAR()
	COUNT() ignores NULL elements but COUNT_STAR() considers NULL elements (tuples).
	example,
	1) Input file has
		, , , , , , , 
		001,Rajiv,Reddy,21,9848022337,Hyderabad,89 
		002,siddarth,Battacharya,22,9848022338,Kolkata,78 
		003,Rajesh,Khanna,22,9848022339,Delhi,90 
		004,Preethi,Agarwal,21,9848022330,Pune,93 
		005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar,75 
		006,Archana,Mishra,23,9848022335,Chennai,87 
		007,Komal,Nayak,24,9848022334,trivendram,83 
		008,Bharathi,Nambiayar,24,9848022333,Chennai,72
		
	2) load the file into a relation:
		grunt> student_details = LOAD 'hdfs://localhost:9000/pig_data/student_details.txt' USING PigStorage(',')
				as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray, gpa:int);
				
	3) group:
		grunt> student_group_all = Group student_details All;
		
		o/p: 
			(all,{(8,Bharathi,Nambiayar,24,9848022333,Chennai,72),
			(7,Komal,Nayak,24,9848022 334,trivendram,83),
			(6,Archana,Mishra,23,9848022335,Chennai,87),
			(5,Trupthi,Mohan thy,23,9848022336,Bhuwaneshwar,75),
			(4,Preethi,Agarwal,21,9848022330,Pune,93),
			(3 ,Rajesh,Khanna,22,9848022339,Delhi,90),
			(2,siddarth,Battacharya,22,9848022338,Ko lkata,78),
			(1,Rajiv,Reddy,21,9848022337,Hyderabad,89),
			( , , , , , , )}) 
			
	4) grunt> student_count = foreach student_group_all  Generate COUNT_STAR(student_details.gpa);
	
		o/p: 9 // i.e. it counted the NULL tuple too.
		
What does TOKENIZE do?
	It splits a string (based on delimter) and returns a bag of tuples where each tuple is the split string.
	grunt> TOKENIZE(expression [, 'field_delimiter'])
	example,
		1) I/p file: 
				001,Rajiv Reddy,21,Hyderabad
				002,siddarth Battacharya,22,Kolkata 
				003,Rajesh Khanna,22,Delhi 
				004,Preethi Agarwal,21,Pune 
				005,Trupthi Mohanthy,23,Bhuwaneshwar 
				006,Archana Mishra,23 ,Chennai 
				007,Komal Nayak,24,trivendram
				008,Bharathi Nambiayar,24,Chennai 			
				
		2) Load the file into a relation.

		3) grunt> student_name_tokenize = foreach student_details  Generate TOKENIZE(name);
			O/p:
			({(Rajiv),(Reddy)})
			({(siddarth),(Battacharya)})
			({(Rajesh),(Khanna)})
			({(Preethi),(Agarwal)})
			({(Trupthi),(Mohanthy)})
			({(Archana),(Mishra)})
			({(Komal),(Nayak)})
			({(Bharathi),(Nambiayar)})

Can we load zipped file in PIG?
	Yes, we can do it with both BinStorage() and TextLoader() 
	example,
	grunt> data = LOAD 'hdfs://localhost:9000/pig_data/employee.txt.zip' USING PigStorage(','); 
	grunt> data = LOAD 'hdfs://localhost:9000/pig_data/employee.txt.zip' USING TextLoader;
	grunt> store data INTO 'hdfs://localhost:9000/pig_Output/data.bz' USING PigStorage(',');

What is TOP() used for in PIG?
It is used to get the top N tuples in a BAG based on a field value.This returns a bag containing required columns.
grunt> TOP(topN,column,relation)

example,
1) Input file
		001,Robin,22,newyork 
		002,BOB,23,Kolkata 
		003,Maya,23,Tokyo 
		004,Sara,25,London 
		005,David,23,Bhuwaneshwar 
		006,Maggy,22,Chennai 
		007,Robert,22,newyork 
		008,Syam,23,Kolkata 
		009,Mary,25,Tokyo 
		010,Saran,25,London 
		011,Stacy,25,Bhuwaneshwar 
		012,Kelly,22,Chennai
		
2) load and group
	grunt> emp_group = Group emp_data BY age;
	
	O/p:
		(22,{(12,Kelly,22,Chennai),(7,Robert,22,newyork),(6,Maggy,22,Chennai),(1,Robin, 22,newyork)}) 
		(23,{(8,Syam,23,Kolkata),(5,David,23,Bhuwaneshwar),(3,Maya,23,Tokyo),(2,BOB,23, Kolkata)}) 
		(25,{(11,Stacy,25,Bhuwaneshwar),(10,Saran,25,London),(9,Mary,25,Tokyo),(4,Sara, 25,London)})

3)  get the top two records of each group arranged based on "id"
	grunt> data_top = FOREACH emp_group { 
					   top = TOP(2, 0, emp_data);      #0 is for ID position.
					   GENERATE top; 
					}
					
	O/p:
	({(7,Robert,22,newyork),(12,Kelly,22,Chennai)}) 
	({(5,David,23,Bhuwaneshwar),(8,Syam,23,Kolkata)}) 
	({(10,Saran,25,London),(11,Stacy,25,Bhuwaneshwar)})

----------------------------------------------------------------------------------------
Embedding Pig Script from Unix Shell, Java and Python:
1) Unix shell: 
	a) pig  -f /home/Scripts/PigScripts/pig_dcnt$x.pig --param timestamp=$timestamp1
	   example,
			#!/bin/sh
			x=1
			while [ $x -le 3 ]
			do
				echo "pig_dcnt$x.pig will be  run"
				pig  -f /home/Scripts/PigScripts/pig_dcnt$x.pig --param timestamp=$timestamp1
				x=$(( $x + 1 ))
			done
			
	b) 
		example,
			#!/bin/bash
			pig pig_script_file1.pig
			pig pig_script_file2.pig

		NOTE: used exec command (example, exec pig_script_file1.pig) in shell script but it just enters into grunt shell and not executing the the pig scripts.

2) Java: Pig provides a PigServer class, using which we can embedded Pig in Java or any other scripting languages. 
		 Details: https://acadgild.com/blog/embedding-pig-java/
	a) create an object: 
		PigServer pigServer = new PigServer(ExecType.MAPREDUCE);  //ExecType gives the mode to run PIG (i.e. local or mapreduce mode)
	b) "runQuery" runs your Pig script:
		runQuery(pigServer);
	c) PigServer provides default methods to register pig queries:
		i.e to include a pigLatin script, use pigServer.registerQuery method provided by the PigServer.
			pigServer.registerQuery("input1 = LOAD '/input' as (line:chararray);"); 
			pigServer.registerQuery("words = foreach input1 generate FLATTEN(TOKENIZE(line)) as word;");
			pigServer.registerQuery("word_groups = group words by word;");
			pigServer.registerQuery("word_count = foreach word_groups generate group, COUNT(words);");
			pigServer.registerQuery("ordered_word_count = order word_count by group desc;");
			pigServer.registerQuery("store ordered_word_count into '/wct_output';");
	d) Using the Properties class, we need to give the HDFS path
		example, props.setProperty("fs.default.name", "hdfs://localhost:9000");
		
	NOTE: setup MAVEN before running.
	
3) Python: pig -embedded jython myjob.py OR USE Python UDFs from within PIG.
		   also check https://www.safaribooksonline.com/library/view/programming-pig/9781449317881/ch09.html

----------------------------------------------------------------------------------------------------
What are all stats classes in the org.apache.pig.tools.pigstats package?
PigStats encapsulates the statistics collected from a running script.
Stat classes are in the package,
PigStats
JobStats
OutputStats
InputStats.		   
		   