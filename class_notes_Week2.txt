what is res -> temproproary storage refrence to the variables


collections:
by default array in scala is immutable (i.e. fixed length).
we can create array with mixed data types ex 
val a = Array("Scala",10, 20.30)  //this will create an Array[Any]
a(1) = 40  

to make it mutable (i.e. give dynamic length array), import scala.collection.mutable.ArrayBuffer
then, var cars = new ArrayBuffer[String]()
cars+ = "BMW"
cars+ = "Merc"

cars.trimEnd(1) - will delete last element
cars.trimEnd(2) - will delete second last element
cars.insert(2,"Maruti")  //will add Maruti to 2nd index of the array i.e. cars[2]

println(cars.length)

cars.insert(1,"Maruti","Volvo","Bentley")

cars.remove(3) //will remove 3rd index

negative index is not supported.
Multi dimensional arrays are supported but a package have to be imported

------------------
Map:

By default scala creates immutable map

val mapping = MAP("NY" -> "New York", "NJ" -> "New Jersey")

mapping.getOrElse("NY","???") //New York

what is a immutable map: in immutable, we can add Map elements, but we will not be able to edit map elements.


var states = scala.collection.mutable.Map("KA" -> "Karnataka", "TN" -> "Tamil Nadu")
states+ = ("AP" -> "Andra Pradesh") //this is added to the end
states- = ("AP")  //this will be now removed

----------------------
Tuple:
var Tupex = ("AP",1,2,"lllllll")
Tupex._1   //index startes from 1.


---------------------
List:

Lists are immutable.

var num = List(1,2,3,4)
num.head = 1
num.tail = List(2,3,4)
num.last //4
num.head.tail //will error
num.tail.head // 2
num.sum //will add

in List, we cannot change value of an element.

------------------------------------------------------
what is id() function?

Anonymous functions: functions we want to use only once

def square (x:Int) : Int = x * x is example of proper function making it reusable.

val square = List.map (x => x * x) is example of a anonymous functions. (map here is a higher order function, x => x * x is anonymous function)

numbers = List (1,2,3,4,5)
numbers.map(x => x + 1).map (y => y * y).map (j => j -1).map(z => -z) i.e it operates from left to right

filter HOF : returns a boolean value for each element passed to it


---------------------------------------------------------

Spark is a programming engine/framework, Hadoop is file storage mechanism

Spark was created in 2009-2011. Spark is a accidental byproduct of Mesos (cluster management framework)
Spark was created to test the power of Mesos

Mesos power was supersceded by YARN and mesos was taken over by Apache.

Databricks company was created by founders of Spark and till date, Databricks are leader.

Databricks has the best documentation on Spark.

spark.apache.org is better for documentation.

major releases of spark starts from 1. In our course, we do 1.6.3 (as majority of orgs are using this version, major changes in V2 is present in 1.6.3)

in spark 2, major difference in performance, not in programming.

There is also a spark standalone

------------------------------------------------------------------------
- DAY2
------------------------------------------------------------------------
USP of spark: 1st 3 are technical reason and 4th is functional reason
1 In memory computing
2 Easy to programming
3 Unified engine for all kinds of workload : 
4 Most active open source project

The real reason of adapting is NOT that Spark is fast. It is the 3rd point.

hadoop came in 2004 and since then the BD world was on top of Hadoop. The problem was
that it had only map-reduce programming workload which was batch processing (slow, no real time result)

other type of workload is realtime processing (interactive processing). Primary was Machine learning ans streaming (realtime data processing)

hadoop layers
a HDFS
b YARN
c MAP REDUCE

SPARK is lightweight map reduce. True power of SPARK is in-buit libs for streaming, graph processing, ML etc

Flink is a competitor of Spark. spark gives near real-time processing and flink almost real-time

Spark jobs can be.
Scheduled (via OOZY or SPARK Built in) 
Monitored (SPARK UI)
Distributed (i.e. supports distributed processing)


Spark core : language API used for programming. Lowest level of abstraction. It uses RAM or harddisk.
				Scala, python, R, java 8 are used for programming.
				
Spark libraries: SPARK SQL, streaming (for data in motion/real time), GraphX, MLlib (ML workloads) 
				  are libs which sit on top of core and come with the installation.
				  

Ways of installing SPARK: local mode (i.e. on a single machine) applicable for development and testing,
						  standalone mode i.e. installing spark in its own cluster i.e. no hadoop cluster required, created by Sameer farooqi
						  Mesos 
						  YARN Mode i.e. hadoop mode [MOST POPULAR MODE]
						  
SPARK can read data from any file system (RDBMS, Local mode, file system)

reason why MR is slow: is coz it reads and writes data to hard disc i.e. lot of I/OOZY
						MR does not allow iterative processing i.e. intermediate result processing
						
SPARK works on data without intermediate read and write to hard disk.

this is done by RAM but spark can use hard disk also. enough RAM helps improve performance of SPARK.

Raghu, UPX, for SPARK is there a SDLC, i.e. dev and testing in local mode and then deploying it in PROD version? What is the tool used to deploy to PROD?
Yes SBT tool is used to deploy in PROD

NOTE: SPARK is to replace MR, it does not work on map-reduce.

--
SPARK RDD (missed around 1 hr 15 mins)

When driver pgm starts it checks with YARN to allocate executor (resources to run program i.e. HDD+vcore) on all data nodes

In SPARK you store/represent data as RDD (in leyman terms, it is a pointer to data).

i.e. val x  = abc.txt
I am creating a RDD to point to abc.txt. So this will load data to RAM

so user sees complete data, data actually resides in RAM of 3 cluster machines. 

RDD is immutable, so do data manipulation, I will create a new RDD.

so x is a RDD having data in abc.txt
a = filter x data by some criteria //new RDD.

x is base RDD.

In Spark, we can write transformation (produce RDD by manipulating data) and action (triggers a DAG and actually show result).

SPARK is fault tolerent, so if one cluster crash, it can handle the data.

Partitions in SPARK: number of blocks where data is stored.

We can reduce or expand partitons in SPARK.

By default, all transformations in SPARK are LAZY unless we call an Action

Raghu, who holds the DAG logic? is it the driver program? DAG is created by Spark Core.

Once action is executed, data is drained from RAM. If a new action is called, SPARK will again load data, run transformations
and show data.
TO optimize, we can cache a RDD before running multiple actions without SPARK have to load data and run same steps.

caching will be available only till the program runs and is over. This is how SPark works coz we look for immediate viewing
i.e. realtime data

We cannot have to actions running at the same time








 						  
				  





















