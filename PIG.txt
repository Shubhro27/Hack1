hadoop streaming: how to see application logs?

link: https://stackoverflow.com/questions/7894770/hadoop-streaming-how-to-see-application-logs

Hadoop streaming uses STDIN/STDOUT for passing the key/value pairs between the mappers and reducers, so the log messages have to be written to a specific log file - check the sample code and the python logging documentation for more details.

http://www.mechanicalcat.net/richard/log/Python/Simple_usage_of_Python_s_logging_module

---------------------------------------------------------------------------------------------------
Apache Pig:
Is a platform that runs on the top of Hadoop to analyze large data sets.
Pig reads the input files from HDFS, uses HDFS to store intermediate data between MapReduce jobs and writes the output to HDFS.

Pig's language is called Pig Latin (PL scripts are turned into MapReduce jobs and executed on Hadoop Cluster)

In Pig a single element (example, fields) is an "ATOM".
a collection of "Atoms" is a "TUPLE".
a collection of "Tuples" is a "BAG".

Pig is primarily used to perform data analytics without writing map-reduce code. It enables:
a) refering to elements by position or name.
b) handling datasets with many elements.
c) Grouping data and Joining Datasets.
d) loading non-delimited data into patterns.
e) major use of pig is for processing massive data sets as it is build on top of Hadoop, thus able to scale across multiple servers.
f) Pig focus on data flow (rather than SQL logic), allows joins, sorting, grouping and aggregation for combining data from multiple sources.
.....

Pig can be extended using custom user defined functions (UDF)








