KafkaStructuredReceiver.consumeMultiTopicsCGroupMapPartition(context, processData, storeData)

here,
context -> a map comprising of zone and sourcetype
		   example,
		      var context = Map[String, String]()
				context += "zone" -> zone
				context += "sourceType" -> sourceType
processData -> where the logic is written for a receiver. i.e. receiver -> DAO
storeData -> 
  def storeData(str: String, context: Map[String, String]): Unit = {
    logger.info(s"storeData end of processing : $str")
  }
  
----------------------------
creating a streaming context:
  def createStreamingContext(): StreamingContext = {
    
    var streamingcont: StreamingContext = null
    val isLocal: Boolean = true  //true for local i.e. development; false for cluster mode
    val appname = "abc"          //i.e. name of the spark job
    val interval = "5".toLong    //i.e. 5 secs microbatch.
      
    if (isLocal) {
	  /*IS CLUSTER*/
      val sc = new SparkConf().setAppName(appname).setMaster("local[*]")
      streamingcont = new StreamingContext(sc, Seconds(interval))
    } else {
	  /*IS LOCAL*/
      val sc = new SparkConf().setAppName(appname)
      streamingcont = new StreamingContext(sc, Seconds(interval))
    }
    streamingcont
  }
  
To create only a Spark Context:
 def createSparkContext(applicName: String): SparkContext = {

    val appname = applicName
    val conf = new SparkConf()
    val sc = conf.setAppName(appname) // for cluster mode 
	val sc = conf.setAppName(appname).setMaster("local") //for local mode
    new SparkContext(sc)
  }
  
To create Dstreams from incoming topic:
    val brokers = "b1.com,b2.com,b3.com,b4.com"  //broker URLs
    val groupId= "cg1.com,cg2.com"               // CONSUMERGROUPS
    try {
      val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> brokers,
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> groupId,
      "auto.offset.reset" -> "latest",
      "enable.auto.commit" -> (false: java.lang.Boolean))  
      
      val topicsSet = topicStr.split(",").toSet    //let topicStr be "A,B,C,D"
      
      var dstreams = KafkaUtils.createDirectStream[String, String](
      streamingContext, PreferConsistent, Subscribe[String, String](topicsSet, kafkaParams)).map(_.value())

      logger.info("Inside createDirectStream : Kafka Dstream got created ")

      dstreams

    } catch {
      case e: Exception =>
        logger.info("Inside createDirectStream : " + e.printStackTrace().toString())
        
        var dstreams =null
        return dstreams
    }
  
How to use a StreamingContext:
1) Reading from multiple topics:

		val topics_str = "A,B,C,D"
		val collection_str = "C1,C2,C3,C4"
		var requestParameterMapNew = = Map[String, String]()

		val topic = topics_str.split(",")     //forms Array[String]
		var collections = collection_str.split(",")

		var collectionmap = Map[String, String]()
		var zonemap = Map[String, String]()

		topic.foreach { x =>
		collectionmap += x -> collections(collectioncount)
		collectioncount = collectioncount + 1
		}

		topic.foreach { x => 
		val kafkaFeeds = dstreams //from above
			var parsedStream = kafkaFeeds.mapPartitions { line =>
				line.map { rowmap =>
				val collection = collectionmap(x)
				requestParameterMapNew += "collectionName" -> collection
				callback(rowmap, requestParameterMapNew)
				}
			}
		}

		parsedStream.foreachRDD { rdd =>
			rdd.foreachAsync { str =>
				storeData(str, requestParameterMapNew)
			}
		}

		ssc.start()
		ssc.awaitTermination()

NOTE: forEachAsync just iterate through values from all partitions one by one in an Async Manner

There also exists forEachPartitionAsync: ForEachPartition will allow you to run the custom code per partition
The custom code here will iterate through values from that partition one by one in Async manner 
NOTE: One partition will always run on one executor.

A usecase of forEachPartitionAsync: 
You want to save your result to database. Now as you know that opening closing DB connections are costly, 
one connection(or pool) per executor will be best. So you code would be,

rdd.forEachPartition(part => {
    db= mysql..blablabla
    part.forEach(record=> {
    db.save(record)
   })
   db.close()
})

i.e. If you have any costly pre-work to do before start processing the data use forEachParition. 
     If not just use forEach. Both are parallel.
	 i.e. foreachPartition just gives you the opportunity to do something outside of the looping of the iterator,

------------------------------------------------------------------
def receiveStreamMapPartion(sourceType: String, zone: String) {

val ssc = EdgeTfsSparkFactory.createStreamingContext(sourceType, zone)
logger.info("Inside receiveStreamMapPartion : Spark streaming context Retrieved")

val kafkaFeeds = EdgeTfsKafkaTemplate10.createDirectStreamConsumerGroup(ssc, sourceType, zone)
logger.info("Inside receiveStreamMapPartion : Kafka DStreams Retrieved")

kafkaFeeds.foreachRDD { rdd =>

  rdd.foreachPartition { partitionOfRecords =>
	try {
	  var topicSet = Properties.getKafkaTopics(sourceType, zone).split(",")
	  var zoneSet = Properties.getKafkaZones(sourceType, zone).split(",")

	  var zoneMap = (topicSet zip zoneSet).toMap
	  logger.info("Inside foreachPartition : zoneMap--->" + zoneMap)

	  partitionOfRecords.foreach(record =>
		{
		  logger.info("Inside ForEach RDD Process starts : Topic-->" + record.topic() + "<--->Zone-->" + zoneMap(record.topic))
		  logger.info("Inside ForEach RDD Process starts : Json-->" + record.value())

		  if (zoneMap.contains(record.topic)) {
			processCallback(record.value, sourceType, zoneMap(record.topic))  //is a function which comprises of the logic
		  }
		})
	} catch {
	  case e: Throwable => e.printStackTrace()
	}
  }
}

ssc.start()
ssc.awaitTermination()

}
------------------------------------------------------------------
difference between kafka earliest and latest offset values:
producer sends messages 1, 2, 3, 4
consumer receives messages 1, 2, 3, 4
consumer crashes/disconnects
producer sends messages 5, 6, 7
consumer comes back up and should receive messages starting from 5 instead of 7

1) When a consumer joins a consumer group it will fetch the last committed offset. so in this case, 
   it will restart to read from 5, 6, 7 if before crashing it committed the latest offset (so 4).
2) If there is no committed offset for the assigned partition, auto.offset.reset property is used 
   to re-read all the messages from the beginning (earliest) or just after the last one (latest).
   
   NOTE: auto.offset.reset config kicks in ONLY if your consumer group does not have a valid offset committed somewhere (ex in Kafka, Zookeeper)
   IF, You have messages in a topic and you start a consumer in a new consumer group group2. There is no offset stored anywhere and this time the auto.offset.reset config will decide whether to start from the beginning of the topic (earliest) or from the end of the topic (latest)
   
   There are 3 values for auto.offset.reset config
   a) earliest: automatically reset the offset to the earliest offset.
   b) latest: automatically reset the offset to the latest offset
   c) none: throw exception to the consumer if no previous offset is found for the consumer's group
   d) anything else: throw exception to the consumer.
   
--------------------------------------------------------------------
Spark Submit command:
nohup spark-submit --master spark://zld06006.vci.att.com:6066 --deploy-mode cluster --total-executor-cores 4 --executor-memory 4G --driver-memory 4G --conf spark.streaming.kafka.maxRatePerPartition=500 --conf spark.streaming.kafka.consumer.cache.enabled=false --class com.att.kepler.ta.receiver.EdgeTfsReceiver ATT_EDGE_TFS-0.1.jar &



--------------------------------------------------------------------
To be formatted
[Executor task launch worker for task 0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NoClassDefFoundError: com/att/raptorframework/dataaccess/mongo/MongoConnectionFactory$
	at com.att.kepler.dao.EdgeTfsMongoDao1$.<init>(EdgeTfsMongoDao1.scala:20)
	at com.att.kepler.dao.EdgeTfsMongoDao1$.<clinit>(EdgeTfsMongoDao1.scala)
	at com.att.kepler.ta.receiver.EdgeTfsReceiver$.processData(EdgeTFSReceiver.scala:90)
	at com.att.kepler.ta.receiver.EdgeTfsReceiver$$anonfun$main$1.apply(EdgeTFSReceiver.scala:29)
	at com.att.kepler.ta.receiver.EdgeTfsReceiver$$anonfun$main$1.apply(EdgeTFSReceiver.scala:29)
	at com.att.raptorframework.receiver.kafka.KafkaStructuredReceiver$$anonfun$consumeMultiTopicsCGroupMapPartition$2$$anonfun$1$$anonfun$apply$1.apply(KafkaStructuredReceiver.scala:68)
	at com.att.raptorframework.receiver.kafka.KafkaStructuredReceiver$$anonfun$consumeMultiTopicsCGroupMapPartition$2$$anonfun$1$$anonfun$apply$1.apply(KafkaStructuredReceiver.scala:59)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
	at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2173)
	at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2173)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.att.raptorframework.dataaccess.mongo.MongoConnectionFactory$
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 20 more
	
	
########################################################33333
Exceptiopn if mongo is down:
[Executor task launch worker for task 4] INFO org.mongodb.driver.cluster - No server chosen by WritableServerSelector from cluster description ClusterDescription{type=UNKNOWN, connectionMode=SINGLE, all=[ServerDescription{address=localhost:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 30000 ms before timing out
[cluster-ClusterId{value='5c17bc947d58f62e24bb63b8', description='null'}-localhost:27017] INFO org.mongodb.driver.cluster - Exception in monitor thread while connecting to server localhost:27017
com.mongodb.MongoSocketOpenException: Exception opening socket	




####################################################################
Job aborted due to stage failure: Task 1 in stage 12557.0 failed 4 times, most recent failure: Lost task 1.3 in stage 12557.0 (TID 49002, 130.6.140.178, executor 0): org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:765)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:602)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:585)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.<init>(CachedKafkaConsumer.scala:47)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer$.get(CachedKafkaConsumer.scala:157)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.<init>(KafkaRDD.scala:211)
	at org.apache.spark.streaming.kafka010.KafkaRDD.compute(KafkaRDD.scala:186)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:64)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:691)
	... 17 more

Driver stacktrace:



##################################################################
[DEV: 2018-Dec-18 03:50:20,037][WARN ][Executor task launch worker for task 50463]Removing server zlp21299.vci.att.com:29092 from bootstrap.servers as DNS resolution failed for zlp21299.vci.att.com
[DEV: 2018-Dec-18 03:50:20,037][WARN ][Executor task launch worker for task 50463]Removing server zlp21301.vci.att.com:29093 from bootstrap.servers as DNS resolution failed for zlp21301.vci.att.com
[DEV: 2018-Dec-18 03:50:20,037][WARN ][Executor task launch worker for task 50463]Removing server zlp21318.vci.att.com:29094 from bootstrap.servers as DNS resolution failed for zlp21318.vci.att.com
[DEV: 2018-Dec-18 03:50:20,037][ERROR][Executor task launch worker for task 50463]Exception in task 1.3 in stage 12933.0 (TID 50463)
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.(KafkaConsumer.java:765)
	at org.apache.kafka.clients.consumer.KafkaConsumer.(KafkaConsumer.java:602)
	at org.apache.kafka.clients.consumer.KafkaConsumer.(KafkaConsumer.java:585)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.(CachedKafkaConsumer.scala:47)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer$.get(CachedKafkaConsumer.scala:157)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.(KafkaRDD.scala:211)
	at org.apache.spark.streaming.kafka010.KafkaRDD.compute(KafkaRDD.scala:186)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:64)
	at org.apache.kafka.clients.consumer.KafkaConsumer.(KafkaConsumer.java:691)
	... 17 more
	
	
############################################################################3
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access


##################################################################################
java.lang.IllegalStateException: This consumer has already been closed.

1) --conf spark.kafka.consumer.cache.timeout=60m


./kafka-consumer-groups.sh --describe --group RTTATFSEdgeData4 --bootstrap-server zlp21299.vci.att.com:29092,zlp21301.vci.att.com:29093,zlp21318.vci.att.com:29094
#######################################################################################################3

java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access

Solutions:
1) add a config to spark-conf.properties or run command i.e. spark.streaming.kafka.consumer.cache.enabled=false
   example,
   nohup spark-submit --master spark://zld06006.vci.att.com:6066 --deploy-mode cluster --total-executor-cores 4 --executor-memory 4G --driver-memory 4G --conf spark.streaming.kafka.maxRatePerPartition=500 --conf spark.streaming.kafka.consumer.cache.enabled=false --class com.att.kepler.ta.receiver.EdgeTfsReceiver ATT_EDGE_TFS-0.1.jar &
   
2) When we have multiple operations happening on the same dstream
   example,
   val s1 = KafkaUtils.createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics1, kafkaParams)).map(record => {
    implicit val formats = DefaultFormats
    parse(record.value).extract[Sensors1]
  } 
  )      
  s1.print()
  s1.saveAsTextFiles("results/", "")
  
  so, there are 2 operations on s1. In such case Spark creates a graph of flows (i.e. 2).
  Spark will attempt to concurrently run both of these graphs, since they are independent of each other. Since Kafka uses a cached consumer approach, it is effectively trying to use the same consumer for both stream executions.
  
  Solution will be to cache the DStream before running the two queries. i.e.
    val dataFromKafka = KafkaUtils.createDirectStream[String, String](ssc, PreferConsistent, Subscribe[String, String](topics1, kafkaParams)).map(/* stuff */)
	val cachedStream = dataFromKafka.cache()
	cachedStream.print()
	cachedStream.saveAsTextFiles("results/", "")
	
3) Config can also be provided in code as : 

      val kafkaParams = Map[String, Object](
        "bootstrap.servers" -> brokers,
        "key.deserializer" -> classOf[StringDeserializer],
        "value.deserializer" -> classOf[StringDeserializer],
        "group.id" -> groupId,
        "auto.offset.reset" -> "latest",
        "enable.auto.commit" -> (false: java.lang.Boolean),
        "num.network.threads"-> "8",
		"spark.streaming.kafka.consumer.cache.enabled" -> false)

######################################################################################3
EDGE: Failed Jobs: 57
ITL: Failed Jobs: 28974
RC: NO Failed Jobs
DJ: NO Failed Jobs
WRDetails: NO Failed Jobs

EDGE:
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
java.lang.IllegalStateException: This consumer has already been closed.

----
java.lang.IllegalStateException: This consumer has already been closed.
	at org.apache.kafka.clients.consumer.KafkaConsumer.ensureNotClosed(KafkaConsumer.java:1610)
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1621)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:983)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.poll(CachedKafkaConsumer.scala:99)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:73)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:223)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:189)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
	at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2173)
	at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:2173)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

 
1 48692 2 FAILED PROCESS_LOCAL 
15 / 130.6.140.175


stdout

stderr
 2018/12/24 01:15:10 33 s   58 ms     java.lang.IllegalStateException: This consumer has already been closed. 

----


###############################################################################################
ITL:

Job aborted due to stage failure: Task 2 in stage 47403.0 failed 4 times, most recent failure: Lost task 2.3 in stage 47403.0 (TID 395777, 130.6.126.27, executor 0): org.apache.kafka.common.KafkaException: Failed to construct kafka consumer

Job aborted due to stage failure: Task 2 in stage 47403.0 failed 4 times, most recent failure: Lost task 2.3 in stage 47403.0 (TID 395777, 130.6.126.27, executor 0): org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:717)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:566)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:549)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.<init>(CachedKafkaConsumer.scala:47)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer$.get(CachedKafkaConsumer.scala:157)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.<init>(KafkaRDD.scala:206)
	at org.apache.spark.streaming.kafka010.KafkaRDD.compute(KafkaRDD.scala:181)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:60)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:654)
	... 20 more

Driver stacktrace:

