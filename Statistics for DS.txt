Stat is scientific approach to do data analysis and predictions. However statistics is a science of approximation (not accuracy)
Stats always gives values in a range.

In stat, predictions based on experience (i.e. no proof) is called hypothesis

Correlation value: relation between 2 factors used to calculate +ve or -ve dependencies.
positive dependency: as one increases, other will also increases. ex, if temperature increases, sale of cold drink increases.
negative dependency: as one increase, other will decrease.ex, if temperature decreases, sale of sweaters increase.

TO understand dependency between 2 variables, correlation proof can be used.
Where correlation is estimated by correlation efficient (depicted as "r")
r value will always be between -1 to +1 where 
a) if "r" is 0, there is no dependecy between variables.
b) if "r" is > 0, then positive dependency i.e. both variables flow in same directtion. ex corr(temperature, cold drink sales) = 0.8
c) if "r" is < 0, then negative dependency i.e. both variables flow in opposite direction. ex corr(temperature, sweater sales) = -0.78


----------------------------------------------------------------------------
Random variables:
a variable whose values are a numerical outcomes of Rand() function.

2 types of RV:
1) Discrete : these are integers (no floating point numbers). ex, outcomes of rolling a dice (any value b/w 1-6)
2) Continuous: it can take any value in a range.ex, bowling speed of a bowler in an over (i.e. it will be in a range)
----------------------------------------------------------------------------
Stat analyisis is subclassified as:
1) Descriptive stat: getting better sense of data i.e. describing data. ex, mean, median, mode
					 i.e. collect data, Present data, Summarize data. So we analyze over the entire data for a question
					and then come to conclusion.
2) Inferential stat: drawing conclusion abt something (ex, population) based on sample data.eg: probability.
					 i.e. we analyze over sample data and then come to conclusion.
					example, if we want to know the avg weight of females (age 20-35) in Pune, it will not be a wise choice to 
					collect entire data for entire population.
					Solution will be to collect sample. This process of collecting sample is "Sampling"
					example, collect sample for about 500 females and then apply avg on weight.
					based on this data conclude on entire population.
					When doing Inferencial stats, "confidence Interval" (CI) i.e. the %confidence that data is correct.
					ex,
					"the avg age of females (20-35) is 85 kg with 0.05 CI"
					CI (how it is calculated),this test should be done before coming to conclusion,
					if CI = 0.05
					confidence on data is (1 - CI) i.e. 1 - 0.05 = 0.95 => 95%
					In general, if data is increasing, CI should also increase.
				
					When collecting sample, usually Sampling bias happens, i.e. different people have different interpretations
					for same data.
					
					
Statistical analysis can be divided into:
1) Univariate Analysis 	 : Analysis over single variable. 
							ex, runs scored by Sachin T and Saurav G over 100 matches is given, 
							    among Sacin and Ganguly, whose performance is good? 
								as Statistical instrument is mean. i.e. applied on runs of each player 
								
							ex, MCT (measure of central tendency),Despertions, distributions.
2) MultiVariate Analysis : Analysis over multiple variables i.e. combination of variables.	
							ex,runs scored by Sachin T and Saurav G over 100 matches is given,
								what is dependency between 2 players? as Statistical instrument is correlation.
								correlation applies on combination of runs scored by each player. 
								i.e.corr(s,g) = -0.78 (i.e. -ve dependency)
							
							ex, covariance, correlation, linear and non linear regression, logistic regression.
							

Univariate analysis methods,
1) measure of central tendency (MCT): mean , mode, median. Used to explain a variable's performance with a single value.
	MCT however does not include sum, max, min, count because these do not help explain performance of a variable.
	MCT means tendency of a variable to be centered around a variable (usually the mean in a data set).
	
	following are used in MCT:
	a) Mean: (summing up values/number of data types) i.e. average. It can be used to determine which variables' value is 
			  close to mean. ex, Sachin's avg run is 70 => most of sachin's scores are close to 70.
			  
			  3 types of mean,
			  a) arithmetic mean: (sum of all elements/number of elements) i.e. avg.
					AM can be used wherever decimal is neglectable. 
					ex, if there is a list of weights like [50.25,50.50,49.25,50.75] and we need to determine who among
					these 3 participants is slim, we will concurr all of them are slim as the weights are similar, something
					decimal is not significant here.
					ex 2
			  b) geometric mean: GM is used whenever decimal is important.
					ex, if there is list of weight of gold jewellery like [1.5,1.75,1.25], we cannot ignore the change
					in decimals as each gram will add/deduct considerable money, so GM is used as decimal value is important.
					
					formula : n th root of product of all elements. ex, GM of [10,20,30] = cube root of (10 * 20 * 30)
			  
			  c) harmonic mean: HM is used when there is one or more outliers. HM allows us to neglect the outlier by 
					providing less weightage to outlier.
					Also, when the decimals have to be considered minutely, it is preferrable to use harmonic mean.
					ex, in a tournament, id Sachin's score is following in 4 matches [60,70,80,290], mean will be 125
					i.e. it will imply Sachin performed exceptional in the tournament, but the avg increased just because of score
					of 290 in 1 match (i.e. outlier) thus giving incorrect judgement.
					ex 2, when we are taking weight of chemical composition
					[0.00354678,0.00351111,0.0034999], although, if rounded off to 4 decimals i.e. [0.0035,0.0035,0.0034]
					still there will be considerable difference, so we need to consider decimals minutely.
					
					formula: n/sum of each element inversion => 1/ (1/x + 1/y + ....). ex, HM of [10,20,30]
					will be 1/(1/10 + 1/20 + 1/30) 
					
			variable bar represents mean. ex, ybar, xbar i.e a line over Y, X.
		
	b) Mode: most repeated value. ex, if a class has students from following background,[BE, BE,MBA, BE,BE,MCA], then mode is BE.
			 however, if the data is like, in a class of 100 students learning DS,[51 male students and 49 female students], then
			 using mode is not a very good idea to define that males students are preferring DS as career.
			 ex 2, among cancer patients, 
				1) if 100 are non smokers and 900 are smokers, we can say that "smoking cause cancer"
				2) if 501 are non smokers and 499 are smokers, we cannot infer if "smoking causes cancer"
			 
			 Note: Mode is used when the number of choices is less (ex, choice from yes or no).
				   Mode cannot be used when data is variable, in this case we create status (or category)
				   ex,
				   a cricketers score in 10 matches - [90,70,75,0,79,100,50,49,0,45]
				   Mode will be 0, which does not indicate the player's performance in 10 matches.
				   so, we categorize it as,
				   [90,70,75,0,79,100,71,49,0,72] =>  [above, above,above,below,above,above,above,below,below,above]
				   where above is (scores above average) and below is (scores below average)
				   so here mode(player performance) = above, hence cricketer performed well.
				  
	c) Median: sort the data in asc and desc and take exact middle value (called central value). Usually used for comparitive
			   analysis
				example, [10,90,34,6,89,67,12] =>sort [6,10,12,34,67,89,90] => Median is 34 (i,e, 4th value from both ends)
						i.e. 50% of data is less than 34.
					 [10,90,34,6,89,67] => sort [6,10,34,67,89,90] => Median is 50.5 (i.e. (34+67)/2)
					 
				NOTE: We come to median only when mean and mode fetches the same/similar results.
				   
				example, there are 2 cricketers who played 1000 matches.
				C1: median is 60 runs
				C2: median is 80 runs
				implies,
				C1                           			C2
				C1 scored >=60 runs in 500 matches		C2 scored >=80 runs in 500 matches
				-										C2 scored >=60 runs in approx 700 matches (assumptions)
				C1 scored >=80 runs in 300 matches      -
				implies,
				C2 is a better scorer than C1.
				
				if median does not help decide i.e. median is also same/similar, we use QUARTILES (1/4th of data).
				
				there are 3 QUARTILES,
				a) Q1: 25% of entire data i.e. 25% of data is less than Q1 and 75% is greater than Q1
				b) Q2: 50% of entire data i.e. 50% of data is less than Q2 and 50% is greater than Q2 i.e.median.
				c) Q3: 75% of entire data i.e. 75% of data is less than Q3 and 25% is greater than Q3.
				
NOTE: In case of a symmetric distribution, mean,mode and median is identical. In case of skewed data (i.e. lot of outliers in data set), median lies closer to skew than mean.
      for skewed data, mean and median will lie closer to skew than mode.
				
Question) If we have to determine the average salary of an indian citizen, which MCT will we use?
Analysis)
			Mean: will calculate the avg but will ignore the outliers as there can be people with 0 and billion $ salaries.
			Mode: There may be a lot of people with 0 salary, so 0 will be the most repeated value.
			Median: will sort the data and calculate the central value
			
			Thus Median will be the best option at it will neutralize the outliers.
			
Question) If we are estimate next year's budget for a hospital, which MCT will we use.Data is monthly expenditure for last yr
Analysis)
			Mean: will calulate the avg of monthly expenditure. We should consider outliers as we need to consider reason for outliers.
			Mode: will determine the most repeated value. The data may be variable and repeated value may ignore outliers.
			Median: will calculate the central value but may ignore outliers.
			
			Mean will be the best way as when planning for future u need to consider outliers.
				
------------------------------------------
Measures of Variance (MoV)
when all modes of MCT (mean, median, mode, quartile) are equal or similar, how to determine data.

MCT helps determine positivity (how good) of data. When all factors are same, we will need to determine how the negativity of 
data shows.

The negativity of data can be determined by dispersion techniques. Dispersion determines spread of a variable.

spread determines how clustered the values are, such as,
1) if spread is less, the values are clustered closely.
2) if spread is high, the values are distributed far off.

Spread determines:
1) variability/risk : spread is high, variability is high.
2) consistency: spread is high, consistency is less.

example,
in a tournament,
Sachin's score: 60,70,80......i.e. mean is 70
Ganguly's score: 0,10,210.....i.e. mean is 70

from data,
probability of Sachin to score a 70 runs in next match is high and of ganguly is low.
Variability of Sachin's score is less (i.e. less spread), so consistency is high and risk/variability is low. On other hand,
Ganguly's variability is high, consistency is less and risk is high.


Dispersion techniques include (Range, variance, SD are also called measures of Variability):
a) Range: difference between maximum value and minimum value is a data set. Used to give variability or spread of data.
		  High range implies high variability i.e. less consistency, less range implies less variability i.e. high consistency.
		  example,
			in a tournament,
			Sachin's score: 60,70,80......i.e. range is 20 (i.e. 80-60)
			Ganguly's score: 0,10,210.....i.e. range is 210 (i.e. 210-0)
		  Range is good ONLY when there are no outliers.
		  example,
		  Sachin's score: 60,70,80......i.e. range is 20
		  Sachin's score: 0,60,70,80,200 ...i.e. range is 200 i.e. high risk, but sachin's score implies consistency.
		  
		  NOTE: usually Range is not used.
		  
b) IQR (Inter Quartile Range): is the distance/range between the 25th and 75th percentile. 
		i.e. (Q3 - Q1) where,
		Q1 -> 25% is less than Q1
		Q3 -> 25% is greater than Q3
		example,
		Sachin's score: 0,60,70,80,200
		where, 0 will fall below Q1 and 200 will fall above Q3, so the outliers are neglected.
		There are 2 problems with IQR:
		1) We may sometime miss imporant data. example, if Sachin has played 1000 matches,
			250 matches will fall below Q1 and 250 matches will fall above Q3. So we are missing data for 500 matches.
		2) If between Q1 and Q3, data is clustered in certain positions and not consistent, IQT will give incorrect explaination.
		
		So IQR is of greates advantage when data is equally spread between Q1 and Q3.
		
c) Variance: is measure of distribution (i.e. spread of recorded values of a variable) 
		larger the variance, further are the values from mean. Smaller the variance, implies values are closer to mean.
		formula is : sigma(x - xbar)/n where xbar is average, n is number of elements, sigma is sum.
		(x-xbar) is called deviation which can be +ve or -ve. so we square the deviation to calculate Variance.
		so formula becomes
		sigma(x - xbar)^2/n ie devation square divided by number of elements.
		example, variance of [10,20,30]
		average i.e. xbar is 20
		deviation is [10-20,20-20,30-20] i.e. [-10,0,10]
		(x - xbar)^2 is [100,0,100]
		sigma[(x - xbar)^2] is 200 i.e. sum
		variance is 200/3 = 66.66

		variance of population i.e. entire data available is sigma(x - xbar)^2/n
		variance of sample i.e. only sample data available for inference is sigma(x - xbar)^2/(n-1)
		
		why is (n-1) used for sample, as in sample there will always be a bias which can be accounted for when using (n-1).
		
		high variance implies high variability/risk and less consistency
		low variance implies low variability/risk and high consistency.
		
		Variance is only good FOR COMPARISION, we cannot comment by calculating variance for a single dataset
		example, v(sachin) = 110, v(Ganguly) = 180,
		we can say that Sachin is more consistent than Ganguly, but if we have have only Sachin's variance, 
		we cannot comment on anything.
		
d) Standard Deviation: SD is sqrt(variance) i.e. square root of variance. (Definition: spread of data "around MEAN")
	SD of population is sqrt(variance of population)
	SD of sample is sqrt(variance of sample)
	
	SD overcomes the disadvantage of variance, i.e. it can be used to comment on behaviour of a single dataset.
	example,
	Sachin: mean is 70 and SD is 10
	Ganguly: mean is 70 and SD is 50
	
	Sachin is more consistent as SD is less than Ganguly i.e. Ganguly is more variant.
	Now, if we consider only sachin's data i.e. mean is 70 and SD is 10
	High peak is (mean + SD) = 70 + 10 = 80
	Low peak is  (mean - SD) = 70 - 10 = 60
	implies, Sachin's runs are fluctuating between 60 to 80.
	Similarly for Ganguly, runs are fluctuating between 20 and 120.
	
	As spread of Ganguly's runs is more, prediction becomes difficult.
	
	example-2,
	Ola share-ride option, if there is a delay in ETA, we raise a complain to which OLA's response is that they calculated the SD of the ride is within the acceptable limits. 
	
	
NOTE: when determining data behaviour, we should consider BOTH positivity and negativity.
example,
consider the investment returns of IBM and Infosys shares where,
			Mean		SD
Infosys		20			5 
IBM			30			40

mean will imply, if we invest 100,000 INR, for Infy and IBM we get 20,000 and 30,000 INR profit, so IBM is more profitable
SD will imply risk, i.e. IBM has more variance than infy i.e less consistency.

peak for Infy is 15 to 25
	 for IBM is -10 to 70

i.e.
			profit in low peak		profit in high peak		summary
Infosys		15000 INR				25000 INR				less returns but less risk
IBM	 		-10000 INR 				70000 INR				high risk but high returns.
-------------------------------------------------------------
Example of MCT and MOV used together:

In a case of police investigation, a data scientist is asked to determine the trend of car thefts in a city for 2017. Data available is:

Yearly data of car thefts for last 10 years, i.e.:
Year	Total number of Car thefts
2009	158
2010	161
2011	148
2012	155
2013	194
2014	140
2015	169
2016	172

for 2017, data for only 1st five months:
Month		Total number of Car thefts
January		8
Feb			14
Mar			21
Apr			25
May			33

for yearly data:
mean:162
median: 140 148 155 158 161 169 172 194 : 159
Variance = [-4 -1 -14 -7 32 -22 7 10] = [16 + 1 + 196 + 49 + 1024 + 484 + 49 +100]/8 = 240
SD = 15
Low Peak = mean - SD = 147
High Peak = mean + SD = 177
i.e. avg number of car thefts between 2008-2016 is between 146-176. 140 and 194 are outliers as they fall outside this range.

For 2017 data:
Mean for 5 months in 2017: 20
Median for 5 months: 21
Total number of car thefts happened in 5 months of 2017 = 101
predicted thefts of rest of 2017 = (mean of the 5 months thefts available) * (number of months left) = 20 * 7 = 140
therefore, total thefts in 2017 can be predicted= (data of 5 months) + (predicted data on next 7 months) = 101 + 140 = 241
which is more than the mean for 2008-16 and also more than high peak, 

so from this data, DS can predict that the number of car thefts will be more than 2008-16.

Case-2
Assault data: 63 71 74 78 87 98 246
Extortion data: 65 70 72 74 180 201 203

how can DS predict:
1) Assault data: 
	mean = 102, but data has 1 outliner i.e. 246 which impacted the mean
	mode = variable data, cnnot be calculated
	median = 78
	
	Median should be used as it will remove outliners
	
2) Extortion data:
	mean = 123.57 (we cannot ignore outliers as data is spread across certain values, so there are fluctuations in data)
	mode = not possible
	median = 74

	mean should be used in this case
-------------------------------------------------------------
Multivariate analysis: i.e. analysis over combination of variables. example, covariance, correlation, regression (linear and non-linear).

1) Covariance: This helps understand dependency between 2 variables. example, if business question is "if temperature is 30 C, what will be the cold-drink sales",
			   first we will have to establish/prove dependency between temperature and cold-drink sales. 
			   
			   Prediction model can only be based on dependency between variables. No dependency implies no prediction. 
			   i.e. we need to understand how i/p variable is influencing target variable. example,
			   based on height, weight and age, we need to predict cholestrol levels, if we apply,
			   cov(age,cholestrol) = 1.23     i.e. positive dependency
			   cov(height, cholestrol) = 0.0  i.e. zero dependency
			   cov(weight,cholestrol) = -0.34 i.e. negative dependency
			   
			   so we can ignore height for predicting cholestrol levels.
			   
			   "covariance measures the degree to which two variables are linearly associated."
			   
			   Covariance formula,
			   COV(x,y) = sum [(x - mean(x)) * (y - mean(y))] / (n-1)
			   where,
			   (x - mean(x)),(y - mean(y)) -> deviation (distance) of x and y from mean.
			   n -> number of elements.
			   
			   there can be 3 possible values for covariance,
			   =0 : implies no dependency between the variables. i.e. change in value of one variable does not impact value of other variable. i.e. both variables are INDEPENDENT.
			   >0 : implies positive dependency i.e. increase in value of one variable will cause increase in value of other variable. similar for DECREASE.
					example, cov(temperature, cold-drink sales) will be +VE.
			   <0 : implies negative dependency i.e. increase in value of one variable will cause decrease in value of other variable. similar for DECREASE.
					example, cov(temperature, sweater sales) will be -VE.
			   
			   NOTE: is covariance is very close to ZERO, then also we can imply NO DEPENDENCY. example, 0.000073, -0.000058 will also imply NO DEPENDENCY.
			   
			   Problem with covariance is that it can just explain positive OR negative OR no dependencies. How strongly/weakly the variables are associated/bonded/dependent 
			   cannot be explained with covariance.
					 
2) Correlation: Correlation also helps understand dependency between 2 variables. This however helps overcome the disadvantage of covariance.
				i.e. we can use to derive strong dependency or weak dependency.
				
				"Correlation is a scaled version of covariance that takes on values in [−1,1] 
				with a correlation of ±1 indicating perfect linear association and 0 indicating no linear relationship."
				
				correlation is measured by "correlation coefficient" (SYMBOL is "r") where -1 <= r <= 1 (i.e. r lies between -1 and +1)
				
				correlation formula,
				r = n[sum(x*y) - {sum(x)*sum(y)}]/sqrt[{n*sum(x^2)- sum(x)^2}*{n*sum(y^2)- sum(y)^2}]
				where,
				x*y -> product of each x value with corresponding y value 
				n -> number of elements
				sum(x^2)-> sum of square of each element.
				sum(x)^2 -> square of sum of each element.
				
				There are 3 possible values similar to covariance. However based on range of values, we can explain dependencies in 7 ways,
				-1____________-0.5___________0____________0.5_____________+1
				 |     |               |     |      |              |      |
                 3     7               6     1      4              5      2    -> positions explaining dependency. 
                                                                                7 (range between -1 to -0.5)
																				6 (range between -0.5 to 0)
																				4 (range between 0 to 0.5)
																				5 (range between 0.5 to 1)
			   
			    when r = 1         i.e. perfect positive correlation- same percentage reflection happens between values of 2 variables i.e. y = f(x)
																	   ex, if x =(10,20,30,15) then y =(1,2,3,1.5)  i.e. y = 10% of x
																	       if x =(10,100,30,90) then y =(20,200,60,90) i.e. y = 2x 
																		   so both variables move in the same direction and reflection of changes is also same.
				    r = -1         i.e  perfect negative correlation - same percentage reflection happens between values of 2 variables but in opposite direction i.e. y = -f(x)
																	   ex, if x =(10,20,30,15) then y =(-1,-2,-3,-1.5)  i.e. y = minus(10% of x)
																	       if x =(10,100,30,90) then y =(-20,-200,-60,-90) i.e. y = -2x 	
																		   i.e. both move in the opposite direction but reflection of changes is also same.
					0 < r <=0.5   i.e. weak positive correlation     - both variables will move in same direction but reflection is NOT close (i.e. FAR reflection)
																	   ex, x increases by 70% but y increased by only 10%.
																	       x increased by 7% but y increased by 80%
																		   i.e. high change (increment/decrement) in x will cause less change in y OR
																		        high change in y will cause less change in x
																			so less change in one variable will cause high change in other variable.
					 r> 0.5        i.e. strong positive correlation - both variables move in the same direction BUT the percentage reflection is not same but close
					                                                   ex, if x increases by 10% (10,11,12,13) but y increases by 20% (1,2,4,8)
																	       x increases by 10% and y by 9%, x decreases by 11% but y decreases by 13%.
																		   so less change in one variable will cause less change in other variable and
																		   high change in one variable will cause high change in other variable.
					 -0.5 <= r < 0 i.e. weak negative correlation   - both variables move in opposite direction but reflection of changes is Far.
																	   ex, if x gets incremented by 70%, but y gets decreased by 10%
																		   if x gets decremented by 7% but y increased by 80%
																		   i.e. high increment in one variable will cause less decrement in other variable.
																		   OR low decrement in one variable will cause high increment in other variable.
					 r < -0.5      i.e. strong negative correlation	- both variables move in opposite direction but reflection of changes is close.
                                                                      ex, x increased by 10% but y decreased by 9%
																		  x decreased by 9% but y increased by 8% 
			         r = 0         i.e. no dependency
					 
	graphical representations of correlation:
	1) perfect positive correlation: angle of the curve is exactly 45 degree with x-axis.
	2) strong positive correlation: angle of the curve is > or < than 45 degree with x-axis but the angle will be close to 45 degree (ex, 43, 47).
	3) weak positive correlation:  angle of the curve is > or < than 45 degree with x-axis but the angle will be far way from 45 degree (ex, 20, 75).
	
	4) perfect negative correlation: the line will form a triange with x and y axis and where it meets x and y axis, the angle will be exactly 45 degree.
	5) strong negative correlation: the line will form a triange with x and y axis and where it meets x axis, the angle will be close to 45 degree.
	6) weak negative correlation: the line will form a triange with x and y axis and where it meets x axis, the angle will be far away from 45 degree (i.e. close to 90 or 0 deg).
	
----------------------------------------------------
Predictions & forecasting:

Predictive analysis comprises of 2 models -  prediction and forecasting.

Prediction is estimating the value of one variable (i.e. target variable) based on values of other variable (i.e input variable).
example, based on temperature, estimate cold drink sales.
         based on age, experience, designation, industry estimate annual income.
		 
Predictions are used to predict,
1) regression problems: in this case target variable is continuous (i.e. values change). I/p variables should also be continuous.
            regression models primary are of 2 types,
			a) linear regression
			b) non linear regression
			
			there are others like ridge regression, lasso regression.
2) classification problems: target variable is a classifier (i.e. any one of the given options i.e. discrete variables). example, tomorrow will it rain (yes/no).
                            Classifiers (options) can be 2 or more.
			classification models include,
			a) logistic regression
			b) Naive bays classifier
			c) decision tree
			d) random forest
			e) support vector machines (SVM)
		    f) ADABooster	
		 
Forecasting is estimating feature depending on timeline. Input and target variables are same for forecasts.
example, based on a list of temperature for last 20 days, estimate tomorrow's temperature.
         based on last 100 days of Infy's share values, estimate Infy share price for tomorrow.

Forecasting models include,
1) time series analysis: following a series of timelines i.e. tomorrow depends on today's data and today depends of yesterday's data and so on
	example, if we have data for last 1 month, we can use if to predict the next day's data.
	TSA models include,
	a) Auto regression (AR)
	b) Integration
	c) Moving average (MA)
	d) ARMA: combination of AR and MA
	e) ARIMA: combination of AR, Integration and MA
	there are more models.
	
Linear regression: There are 2 LR models
a) Simple LR :  i.e. there will be 1 i/p variable and 1 target variable. 

				Intercept: indicates the location of regression line where it intersects an y-axis. Origin of a line.
				Slope    : indicates the steepness of the regression line. Trignomatically, tan(angle) with x-axis.
				
				The slope and the intercept define the linear relationship between two variables
				
				equation, y = a + bx
				where a-> intercept
				      b -> slope
					 
				example, y = 2 + 5x The y-intercept is 2, so y will be starting from 2 and The slope is positive 5. When x increases by 1, y increases by 5. 
                         y = 7 - 2x The y-intercept is 7.2, so y starts from 7.2 and The slope is negative 0.4. When x increases by 1, y decreases by 0.4. 	

				y = bx will mean that intercept is zero i.e. the regression line will pass thru the origin but this is not a real scenario.
			    as there will always be "bias" associated with i/p variable.
				ex, if temp increases, cold-drink sales increases but if temperature is 0, does not mean that cold-drink sale is 0.
				    if age grows, weight grows BUT if age is 0. does not mean that height or weight will not be zero.
					
				Thus "intercept is used to handle this bias."
				
				how to find intercept and slope in a + bx:
				here,
				slope     -> b = cor(x,y) * [sd(y)/sd(x)] where cor -> correlation between x and y & sd is standard deviation 
				intercept -> a = mean(y) - {b * mean(x)}
				
				example-1,
				x        y
				10		 30
				40		 120
				100      300
				
				1st change in x = 30; change in y = 90
				slope = change in y/change in x = 90/30 = 3
				
				2nd change in x = 60; change in y =180
				slope = change in y/change in x = 180/60 = 3
				
				Matrix representation of SLR: i.e. y = a + bx
				[y] = [1 x][a
							b]
				[1 x] is a 1*2 matrix
				[a    is a 2*1 matrix
				 b]
				
				as per Matrix multiplication rule, num of cols of 1st matrix == num of rows of 2nd matrix
				so this multiplication will produce 1 * 1 matrix	  

b) Multiple LR : i.e. multiple i/p variables and 1 target variable. 
				 
				equation, y = b0 + b1.x1 + b2.x2 + ........ + bn.xn
				where b0 -> intercept
				      b1, b2....bn -> slope of x1,x2,x3.....xn
                 
				 These coefficients are calculated based on given data.
				 
				MLR has multiple slopes as each i/p variable has different impact on target variable. Intercept however remains constant.
				example if income is dependent on 2 variables age and technology, each i/p variable (i.e. age and technology) will have its own impact on the
				target variable (i.e. income), so there will be different slopes. 
				
				example,
				"age","w","h","chol"
				20,56,5.5,45
				25,70,6.0,55
				24,90,5.9,89
				34,100,5.7,99
				45,120,5.8,100
				22,78,5.5,66
				28,80,5.6,77
				30,90,5.9,90
				
				Problem statement: based on age, weight, Height, predict person's cholestrol level.
				here i/p variables:   age, weight, Height
				     target variable: cholestrol level
				i.e multiple i/p variable.
				
				so MLR equation will be cholestrol = b0 + b1*age + b2*w + b3*hand
				ex, 45 = b0 + 20 * b1 + 56 * b2 + 5.5 * b3
				
				in matrix format, this equation may look like,
				y1 = b0 + b1 * x11 + b2 * x12 + b3 * x13 + ....... + bn * x1n
				where,
				y1 -> cholestrol in first row
				x11 -> 1st row * 1st column
				x12 -> 1st row * 2nd column

				similarly y2 = b0 + b1 * x21 + b2 * x22 + b3 * x23 + ....... + bn * x2n
				
				so the matrix will look like
				[y1] = [1 x11 x12 x13 .... x1n][b0
												b1
												b2
												b3
												.
												.
												.
												bn]
												
				1st matrix is : 1 * (n+1)
				2nd matrix is : (n+1) * 1
				multiplication will fetch : 1 * 1 matrix
				
				if we convert all such rows y2, y3....yn into matrix format
				
				[y1   = [1 x11 x12 x13 .... x1n   * [b0
				 y2      1 x21 x22 x23 .... x2n		 b1
				 y3      .                           .
				 .		 . 							 .
				 .
				 .
				 ym]     1 xn1 xn2 xn3 .... xmn]     bn]
				 
				1st matrix is [m * (n+1)] dimension
				2nd matrix is [(n+1) * 1] dimension
				so result is [m * 1] dimension
				
				to find the value of intercept (B) in Y = X * B
				where Y -> Y Matrix in MLR
				      X ->  X Matrix in MLR
					  B -> B Matrix in MLR
					  
				mathematically B = Y/X but this is incorrect as X should be a square matrix to calculate (1/X)
				so we can use
				Transpose(X) * Y = [Transpose(X) * X] * B
				Transpose -> convert rows to columns
				[Transpose(X) * X] will help form a square matrix 
				ex, if X is [5 * 4] matrix, Transpose[X] will be [4 * 5] matrix and
				[Transpose(X) * X] will be [4 * 5] * [5*4] = 4 * 4 matrix

				Transpose(X) * Y is done to maintain equilibrium on both sides
				
				so B = inv[transpose(X) * X] * [Transpose(X) * Y]
				size of B matrix will be 
				inv[transpose(X) * X] = [ (n+1 * m) * (m * n+1)] = [n+1 * n+1] matrix
				[Transpose(X) * Y]    = [ (n+1 * m) * (m * 1) ]  = [n+1 * 1] matrix
				so [n+1 * n+1] * [n+1 * 1] = [n+1 * 1]
				so B is [n+1 * 1] matrix
				
				once we have value of B (Beta), we can put it in MLR and calculate Regression value (Ycap)
				which can then be put into different accuracy models to determine accuracy of a prediction
				
				consider example,
				"age","w","h","chol"
				20,56,5.5,45
				25,70,6.0,55
				24,90,5.9,89
				34,100,5.7,99
				45,120,5.8,100
				22,78,5.5,66
				28,80,5.6,77
				30,90,5.9,90
				
				Problem statement: based on age, weight, Height, predict person's cholestrol level.
		
		
				one age age^2 age^3 w  w^2  w^3 	h 		h^2 	h^3
		X	[	1	20  400   8000  56 3136 175616  5.5     30.25   166.375
		        2nd row
				.
				.
				last row                                                      ]
				
		Y   [45
			 55
			 89
			 .
			 .
			 .
			 90]
			 
		B  	inv[transpose(X) * X] * [Transpose(X) * Y] will be a 10*1 matrix
		
		here,
		B[1,1] = b0 = intercept
		B[2,1] = b1 = slope of age
		B[3,1] = b2 = slope of (age)^2
		B[4,1] = b3 = slope of (age)^3
		
		B[5,1] = b4 = slope of (weight)
		B[6,1] = b5 = slope of (weight)^2
		B[7,1] = b6 = slope of (weight)^3

		B[8,1] = b7 = slope of (height)		
		B[9,1] = b8 = slope of (height)^2
		B[10,1] = b9 = slope of (height)^3
		
		
How to test quality of a regression line:
This is done using  Rquare method i.e r^2 (where r is correlation)
example,
r = 0.9
r^2 = 0.81 i.e 81% of values are close to regression line and remaining is far way from regression line.

75% is the standard for regression line quality to be good.

The problem with this approach is that rsq is hypothetically true but not in reality i.e. there is no benchmark for closeness.
example, if the rsq is 81%, it does not imply how close is the prediction to the regression line.
---------------------------------------------------------
How to choose a model for prediction:

this can be done using "hypothesis testing".
if p <= 0.05, then do linear regression
if p > 0.05 , then do non linear regression
where p -> benchmark

where p is calculated by t-test, z-test, f-test. All these are inferential test metrics.
However these test metrics is good for low sample volume. example,
t is good for 130 sample
z is good for 160 sample
f is good for 450 sample
but in organizations now, the volume of sample data is usually very high.

In ML, the solution for this problem is "accuracy testing" is used to select a model.
step-1: take 20% random sample from data. 
step-2: Apply all possible models. 
step-3: test accuracy of all models.
step-3: chose model with highest accuracy.
Step-4: apply model from step-3 to entire data.
---------------------------------------------------------
non linear regression:
a curve best represents non linear regression [as a st line best represents linear regression]

NLR has 3 models as:
1) Quadratic model:
	y = b0 + b1 * x + b2 * (x^2)
	example,
	x   y
	10	120
	5	50
	
	X i.e. x matrix = [1 10 100
					   1  5  25]
					   
	Y i.e. y matrix = [120
	                    50]
						
	B = inv[transpose(X) * X] * [Transpose(X) * Y] i.e. same as linear reg
	B is 3 * 1 matrix
	
	B[1,1] -> b0 i.e. intercept
	B[2,1] -> b1 i.e. slope of x
	B[3,1] -> b2 i.e. slope of x^2
	
	
2) Cubic models:
	y = b0 + b1 * x + b2 * (x^2) + b3 * (x^3)
	example,
	x   y
	10	120
	5	50

	X i.e. x matrix = [1 10 100 1000
					   1  5  25 125]
					   
	Y i.e. y matrix = [120
	                    50]
						
	B = inv[transpose(X) * X] * [Transpose(X) * Y] i.e. same as linear reg
	B is 4 * 1 matrix
	
	B[1,1] -> b0 i.e. intercept
	B[2,1] -> b1 i.e. slope of x
	B[3,1] -> b2 i.e. slope of x^2
	B[3,1] -> b3 i.e. slope of x^3
	
3) power model: this curve will be like sine wave
	y = b0 * (b1^x) i.e. [a * (b^x)]
	
	for PM, derive b0 and b1.
	
	example,
	x   y
	10	120
	5	50

	X i.e. x matrix = [1 10 
					   1  5]
					   
	Y i.e. y matrix = [120
	                    50]
						
	B = inv[transpose(X) * X] * [Transpose(X) * Y] i.e. same as linear reg
	B is 2 * 1 matrix
	
	B[1,1] -> b0 i.e. intercept
	B[2,1] -> b1 i.e. slope of x

In case we have 4 models,
1) linear regression: Y = X * B
2) quadratic NLR
3) cubic NLR
4) Power NLR
in order to chose which one is appropriate for prediction, we will do "accuracy test". The model which gives highest accuracy value, will be chosen.

----------------------------------------------------------
Logistic regression: this is used to predict classifiers (i.e. options). example, will it rain or not (An. Yes/No)

----------------------------------------------------------
Standard error:

Formula,
se = sqrt[sum(y - mean(y))^2]/(n-2)

example,
y = 3 + 2x        -------> has error quotient
se = 4
y - se = 3 + 2x
i.e. y = 7 + 2x  ---------> has less error
se = 2
i.e. y = 9 + 2x  ---------> has even less error
.
. 
.
at one point the error again starts increasing so we will have to stop at that point.

in this approach, there are 2 problems,
1) Intercept keeps increasing.
2) slope is constant.

but the 2 factors should be ideally neutralized.

In ML, we use Gradiant Descent Algorithm where gradiant will adjust both intercept and slope.so gradient will calculate the slope and intercept also wrt SE.

---------------------------------------------------------
Principal component Analysis (PCA) - This is used in machine learning

PCA is the practice of eliminating unnecessary input variables (i.e. variables not participating well) before training 
a model for machine learning. This process is also called reducing dimensionality.

correlation matrix -> eigen matrix (values and vectors) -> PCA
where -> serves as i/p to
 
----------------------------------------------------------
Inferencial Statistics:

In inferencial statistics, the end goal is to rank an object. example, if score of a student is in the top 1% of the entire class.
example,
In case of car theft example (mentioned above), the goal was to determine if the theft crimes are on the rise
Had it been asked if the car theft between 2008-16 is among the top crimes for those years, it would be handled with inferencial statistics.

1) Probability: is the chance for an event to happen. example, in a football match, the chances of each team winning is 50-50 i.e. probability is 50%.
				if we are throwing a coin, it is an event, the outcome is 2. Out of this outcome, we are interested in only one outcome i.e. 1/2
				so, denominator => number of possible outcomes
				    numerator   => outcome we are interested in 
				so probability is the chance that the interested outcome will come.
				
				advanced example, 
				in 3 flips of coin, we need to determine the number of times "HEADS" appeared.
				So total outcomes will be 8 i.e.
				1	2	3
				H	H	H
				H	H	T
				H	T	H
				H	T	T
				T	H	H
				T	H	T
				T	T	H
				T	T	T
				
				number of outcomes (from the above data) with 
				1) H = 0 will be 1/8
				2) H = 1 will be 3/8
				3) H = 2 will be 3/8
				4) H = 3 will be 1/8

				similarly, if there are 4 coins, the number of outcomes will be 16.
				so, the number of outcomes = 2^n
				i.e. if 1 coin outcomes = 2^1 = 2
					 if 2 coins outcomes = 2^2 = 4
					 if 3 coins outcomes = 2^3 = 8
					 if 4 coins outcomes = 2^4 = 16
					 
					 
				another example,
				if there are 2 dice, how many possible outcomes,
				1,1
				1,2
				1,3
				1,4
				1,5
				1,6
				...
				6,1
				6,2
				6,3
				6,4
				6,5
				6,6
				
				i.e. 36 outcomes.	
				
	
			Probability distribution is the graph (histogram) of any event (ex, in 3 flips of coin, we need to determine the number of times "HEADS" appeared.)
			where x is number of results
				  y is frequency/probability of getting that value.
			PD is a discrete graph (i.e. there are breaks in the value and the values are not continuous)
			
			Probability Density is a continuous curve which defines whether the probability will lie within a range
			ex, P(49.9 < X < 50.1)  = p(x)dx between range 49.0 to 50.1
			In case of a continuous curve/distribution, we calculate the area (of the curve) to determine probability
			
			Perfect normal distribution will be a bell curve.
			
			A renge works for continuous random variiable, but a precise value cannot be determine from continuous random variable curve. 
	
2) Normal distribution: A normal distribution, sometimes called the bell curve, is a distribution that occurs naturally in many situations.
						example, the bell curve is seen in tests like the SAT and GRE. 
						The bulk of students will score the average (C), while smaller numbers of students will score a B or D. 
						An even smaller percentage of students score an F or an A. 
						
						This creates a distribution that resembles a bell (hence the nickname). 
						The bell curve is symmetrical. Half of the data will fall to the left of the mean; half will fall to the right.
						
						other examples include: heights of people, Blood Pressure, IQ Scores, salaries etc.
						
						the rule of what percentage of your data falls within a certain number of standard deviations from the mean:
						• 68% of the data falls within one standard deviation (1 sigma) of the mean.
						• 95% of the data falls within two standard deviations (2 sigma) of the mean. 2 SD provides more accuracy but the range is bigger.
						• 99.7% of the data falls within three standard deviations (3 sigma) of the mean. 3 SD provides more accuracy but the range is bigger.
						
						|
						|
						|
						|                           68%
						|                    -----------------
						|                           95%
						|             ---------------------------------
						|                           99.7%
						|     --------------------------------------------------              
						| ------------------------------------------------------------
						| 0.1%  2.1%     13.6%  34.1%  34.1%   13.6%    2.1%      0.1% 	
						|____________________________________________________________________
						    -3s     -2s     -1s      u      1s       2s        3s
 
						NOTE: distance between -3s and -2s is 1 SD.
						
						SD controls the spread of a distribution i.e.
							smaller SD means  data is tightly clustered around the mean; the normal distribution will be taller.
							larger standard deviation indicates that the data is spread out around the mean; the normal distribution will be flatter and wider.
							
						Properties of a normal distribution
							The mean, mode and median are all equal.
							The curve is symmetric at the center (i.e. around the mean, μ).
							Exactly half of the values are to the left of center and exactly half the values are to the right.
							The total area under the curve is 1.
					
					in a normal distribution, 
					x-axis will be the continuous random variables i.e. data points or possible values.
					y-axis will be the frequency/probability.
					example, in case of a cricket match,
					x-axis: will be the bowling speed a bowler has bowled
					y-axis: will be the frequency with which he can bowl at a particular speed (ex, how many bowls were bowled with speed < 80 kms/hr)
					Peak will be the central tendency.
					
					
					Standard Normal distribution model is when mean == 1 and SD == 1.
					i.e. 90% of data will fall in the inner part of curve	
					     10% will fall in the talls i.e. outer to the curve (5% on each side)


					a SD model tells you the rank for a particular value.
					example,
					if you get a score of 90 in Math and 95 in English, you might think that you are better in English than in Math. 
					However, in Math, your score is 2 standard deviations above the mean. 
					In English, it’s only one standard deviation above the mean. 
					It tells you that in Math, your score is far higher than most of the students (your score falls into the tail).
								
					How to standardize a ND: This can be done with Z-Score ie.
					z = (raw score - mean)/SD
					applies to both population and Sample.
					
					z-score can be +ve (score is less than mean) or -ve (score is more than mean)
					However z is just a ratio which does not have any unit which is contrary to previous SD examples where x-axis had the possible values with unit.
					
					It is hard to calculate area of multiple Normal distribution, so statisticians standardize Normal distribution by marking mean = 0 and SD = 1.
					This can be used to compare different values.
					
					Z score helps to compare between 2 Normal distributions. ex, compare scores of 2 students.
					
					example,
					a person's score is 70 in a class of 50 students
					Mean = 60 and SD = 15
					how to determine if Sarah has achieved one of the best scores (i.e. top 10%) in the class and is eligible for scholarship
					
					z-score = (70 - 60)/15 = 0.67
					
					we now look into z-table (google/wiki for the table) to find the value corresponding to 0.67 and it is 0.7486
					this implies probability of a score lesser than 0.67 is 0.7486 i.e. 74.86%
					i.e. 75% of students scored less than 70, thus, 25% of students (i.e. around 13 students) scored greater than 70.
                    10% of class is 5, so this student is not among them.
					
					90% of distributions are normal but there are cases where the distribution is skewed.
					Skewness and Kurtosis: Skewness is asymmetry of distribution. 
						  i.e. +ve skewed distribution has a tail pulled in +ve direction i.e. outliers are to right
                               -ve skewed distribution has a tail pulled in -ve direction i.e. outliers are to left
							example, most stock market distributions, salary distribution are -vely skewed
							more billionires give +ve skewed data, more poor people gives -vely skewed data.

						  kurtosis represents how peaked the curve is.
						  i.e. +ve kurtosis is steeper curve. frequency of outliers is more i.e. thin tales.
						       -ve kurtosis is flatter curve.Outliers are significant though frequency of outliers is less, this creates fat tales.
								fat tales means, that tail is above the x-axis.
					
					skewed distributions can be converted to normal distributions by having more data points.
					 
----------------------------------------------------------------------
Pictorial Representations:
explained in a word document "Statistics_PICTORIAL_REPRESENTATIONS.docx"
----------------------------------------------------------------------
Central limit theorem:
This is a part of inferential statistics. example, exit poles (i.e. drawing conclusion from sample of population).

In galton board, when we throw balls,
1) large balls form a perfect normal distribution
2) the ND decreases with size of balls getting small till there is no normal distribution for the smallest set of balls.

Conclusions:
1) Independent samples when plotted gives a normal distribution.
2) dependent samples (i.e. samples having considerable bias) when plotted DO NOT give a normal distribution.

example, while determining the per-capita income of India:
1) Sample if collected only from states in Northen India/Southern india will have a large amound of bias thus small balls
2) Samples if distributed from states in different directions from North, South, East, West will have less bia and thus Large balls.
 
"Wisdom of crowd" method is the action of collecting data (enough samples) from random crowd. 
ex: companies call their employees to predict the share price of the company.
ex2: in 1907, Galton found that when 787 people guessed the weight of an ox in county fair, the median
     estimate was only off by 10 pounds

Standard Error: is measure of how much bias will creep into the sample.

SE = SD for population/square root(size of sample)
OR
SE = sample estimate of SD/square root(size of sample)

difference b/w Sample variance and standard error?
Sample Variance							|	Standard Error
variance in a sample available          |   accuracy of the sample wrt population.i.e. determine whether you took right sample or not.

however SE comes from SV as Sample SD = sq root(Sample Variance)

Confidence Interval: 
1) If Population SD is known:
	CI = (sample mean) +/- [z* X SE] #z* multiplied by SE and +/-: plus, minus
	   = (sample mean) +/- [z* X {SD for population/square root(size of sample)}]
	   
2) If Sample SD is known:
	CI = (sample mean) +/- [z* X SE] #z* multiplied by SE and +/-: plus, minus
	   = (sample mean) +/- [z* X {sample estimate of SD/square root(size of sample)}]
	   
	Here, z*values (z-table) of CI are
	CI			z* value
	80%			1.28 
	90%			1.645
	95%			1.96
	98%			2.33
	99%			2.58
	
example,
in order to determine average experience of students in Data Science batch. The institute has sample data only (i.e. possible set of values)

dss_exp = np.array[12,15,13,20,19,20.........,15,16,18,13]

#parameters for sampling
n = 10    #size of the sample from dss_exp (population)
NUM_TRIALS = 1000   #iterations i.e. how many times I will take samples.

#one iteration
samp = np.random.choice(dss_exp,size = n,replace = True)  #replace implies that the sample taken in this iteration will be replaced in the DS and not removed from the DS
														  #so in next iteration, some of the values from this sample will be replaced.
print(samp)                                               # [ 4 18 16 11 13 19 6 3 18 19 ]  NOTE: each run will create a random sample.
samp_mean = samp.mean()									  # 12.7
samp_sd = samp.std()									  # 6.034
print("Samp_mean = {:4.3f}", format(samp_mean,samp_sd))

# draw the samples 1000 times and compute mean each time.
np.random.seed(100)
mn_array = np.zeros(NUM_TRIALS)
sd_array = np.zeros(NUM_TRIALS)

for i in range(NUM_TRIALS):
	samp = np.random.choice(dss_exp,size = n,replace = True)
	mn_array[i] = samp.mean()            #means of 1000 iterations
print(mn_array)
#single number which will show us the result in one variable. i.e. mean of means
mn = mn_array.mean()
sd = mn_array.std()
x5_pct = np.percentile(mn_array,5.0)    //calculates the 5 percentile
x95_pct = np.percentile(mn_array,95.0)  //calculates the 95 percentile. This is used when plotting box-chart.


NOTE: This process is called central limit theorm.


Central limit theorm implies that mean of means will follow a normal distribution.
"In probability theory, the central limit theorem (CLT) establishes that, for the most commonly studied scenarios, 
when independent random variables are added, their sum tends toward a normal distribution (commonly known as a bell curve) 
even if the original variables themselves are not normally distributed"

Generally when the sample size (n) > = 30, irrspective of the original distribution, the sampling distribution
of mean becomes normal distribution.

NOTE: When n is increased the confidence interval becomes smaller 
		which implies that results are obtained with higher certainity.
		
		
samp = np.random.choice(dss_exp, size = n, replace = True)
samp_mean = samp.mean()
samp_sd = samp.std()
sd_ci = samp_sd/np.sqrt(n) # this will form the standard error

samp_lower_5pct = samp_mean - 1.645 * sd_ci # Lower 90% CI
samp_upper_95pct = samp_mean + 1.645 * sd_ci # Upper 90% CI

print("Pop Mean: {:4.3f} | Sample: L_5PCT = {:4.3f} | M = samp_mean = {:4.3f}  | H_95PCT = {:4.3f}"
.format(dss_exp.mean(), samp_lower_5pct, samp_mean, samp_upper_95pct))

Conclusion:
since the CI is 90%, out of every 10 entries 9 will fall within upper/lower limit and 1 will be out

TO check this we write a function:
def samp_mean_within_ci(mn, l_5pct, u_95pct):
    out = True
    if (mn < l_5pct) | (mn > u_95pct):
        out = False        
    return out
# Checking if the population mean lies within 90% Confidence Interval (CI)
mn_within_ci_flag = samp_mean_within_ci(dss_exp.mean(), samp_lower_5pct, samp_upper_95pct)
print("True mean lies with the 90% confidence Intervel = {}".format(mn_within_ci_flag))	
plt.hist(dss_exp, bins = 21)  #plots the histogram.

o/p:
Pop Mean: 10.435 | Sample: L_5PCT = 8.209 | M = samp_mean = 11.100  | H_95PCT = 13.991
True mean lies with the 90% confidence Intervel = True
Pop Mean: 10.435 | Sample: L_5PCT = 8.665 | M = samp_mean = 11.600  | H_95PCT = 14.535
True mean lies with the 90% confidence Intervel = True
.
.
Pop Mean: 10.435 | Sample: L_5PCT = 4.396 | M = samp_mean = 6.400  | H_95PCT = 8.404
True mean lies with the 90% confidence Intervel = False
	

Sampling:
samp_mn_vec, lower_5pct, upper_95pct, flag_true_mean_within_ci = (np.zeros(NUM_TRIALS), np.zeros(NUM_TRIALS), np.zeros(NUM_TRIALS), np.zeros(NUM_TRIALS))
for i in range(NUM_TRIALS):
    samp = np.random.choice(dss_exp, size = n, replace = True)
    samp_mn_vec[i] = samp.mean()
    samp_sd = samp.std()
    sd_ci = samp_sd/np.sqrt(n)
    lower_5pct[i] = samp_mn_vec[i] - 1.645 * sd_ci
    upper_95pct[i] = samp_mn_vec[i] + 1.645 * sd_ci
    flag_true_mean_within_ci[i] = int(samp_mean_within_ci(dss_exp.mean(), lower_5pct[i], upper_95pct[i]))
    print(x5_pct, samp_mn_vec[i], x95_pct, lower_5pct[i], upper_95pct[i], flag_true_mean_within_ci[i])
    
df = pd.DataFrame(np.column_stack((samp_mn_vec,lower_5pct,upper_95pct, flag_true_mean_within_ci)), columns = ['samp_mean', 'lower_5pct', 'upper_95pct', 'flag_true_mean_within_ci' ])    
#df.to_csv('expt_n{}_iters{}.csv'.format(n, NUM_TRIALS)) 



