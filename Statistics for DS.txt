Stat is scientific approach to do data analysis and predictions. However statistics is a science of approximation (not accuracy)
Stats always gives values in a range.

In stat, predictions based on experience (i.e. no proof) is called hypothesis

Correlation value: relation between 2 factors used to calculate +ve or -ve dependencies.
positive dependency: as one increases, other will also increases. ex, if temperature increases, sale of cold drink increases.
negative dependency: as one increase, other will decrease.ex, if temperature decreases, sale of sweaters increase.

TO understand dependency between 2 variables, correlation proof can be used.
Where correlation is estimated by correlation efficient (depicted as "r")
r value will always be between -1 to +1 where 
a) if "r" is 0, there is no dependecy between variables.
b) if "r" is > 0, then positive dependency i.e. both variables flow in same directtion. ex corr(temperature, cold drink sales) = 0.8
c) if "r" is < 0, then negative dependency i.e. both variables flow in opposite direction. ex corr(temperature, sweater sales) = -0.78


----------------------------------------------------------------------------
Random variables:
a variable whose values are a numerical outcomes of Rand() function.

2 types of RV:
1) Discrete : these are integers (no floating point numbers). ex, outcomes of rolling a dice (any value b/w 1-6)
2) Continuous: it can take any value in a range.ex, bowling speed of a bowler in an over (i.e. it will be in a range)
----------------------------------------------------------------------------
Stat analyisis is subclassified as:
1) Descriptive stat: getting better sense of data i.e. describing data. ex, mean, median, mode
					 i.e. collect data, Present data, Summarize data. So we analyze over the entire data for a question
					and then come to conclusion.
2) Inferential stat: drawing conclusion abt something (ex, population) based on sample data.eg: probability.
					 i.e. we analyze over sample data and then come to conclusion.
					example, if we want to know the avg weight of females (age 20-35) in Pune, it will not be a wise choice to 
					collect entire data for entire population.
					Solution will be to collect sample. This process of collecting sample is "Sampling"
					example, collect sample for about 500 females and then apply avg on weight.
					based on this data conclude on entire population.
					When doing Inferencial stats, "confidence Interval" (CI) i.e. the %confidence that data is correct.
					ex,
					"the avg age of females (20-35) is 85 kg with 0.05 CI"
					CI (how it is calculated),this test should be done before coming to conclusion,
					if CI = 0.05
					confidence on data is (1 - CI) i.e. 1 - 0.05 = 0.95 => 95%
					In general, if data is increasing, CI should also increase.
				
					When collecting sample, usually Sampling bias happens, i.e. different people have different interpretations
					for same data.
					
					
Statistical analysis can be divided into:
1) Univariate Analysis 	 : Analysis over single variable. 
							ex, runs scored by Sachin T and Saurav G over 100 matches is given, 
							    among Sacin and Ganguly, whose performance is good? 
								as Statistical instrument is mean. i.e. applied on runs of each player 
								
							ex, MCT (measure of central tendency),Despertions, distributions.
2) MultiVariate Analysis : Analysis over multiple variables i.e. combination of variables.	
							ex,runs scored by Sachin T and Saurav G over 100 matches is given,
								what is dependency between 2 players? as Statistical instrument is correlation.
								correlation applies on combination of runs scored by each player. 
								i.e.corr(s,g) = -0.78 (i.e. -ve dependency)
							
							ex, covariance, correlation, linear and non linear regression, logistic regression.
							

Univariate analysis methods,
1) measure of central tendency (MCT): mean , mode, median. Used to explain a variable's performance with a single value.
	MCT however does not include sum, max, min, count because these do not help explain performance of a variable.
	MCT means tendency of a variable to be centered around a variable (usually the mean in a data set).
	
	following are used in MCT:
	a) Mean: (summing up values/number of data types) i.e. average. It can be used to determine which variables' value is 
			  close to mean. ex, Sachin's avg run is 70 => most of sachin's scores are close to 70.
			  
			  3 types of mean,
			  a) arithmetic mean: (sum of all elements/number of elements) i.e. avg.
					AM can be used wherever decimal is neglectable. 
					ex, if there is a list of weights like [50.25,50.50,49.25,50.75] and we need to determine who among
					these 3 participants is slim, we will concurr all of them are slim as the weights are similar, something
					decimal is not significant here.
					ex 2
			  b) geometric mean: GM is used whenever decimal is important.
					ex, if there is list of weight of gold jewellery like [1.5,1.75,1.25], we cannot ignore the change
					in decimals as each gram will add/deduct considerable money, so GM is used as decimal value is important.
					
					formula : n th root of product of all elements. ex, GM of [10,20,30] = cube root of (10 * 20 * 30)
			  
			  c) harmonic mean: HM is used when there is one or more outliers. HM allows us to neglect the outlier by 
					providing less weightage to outlier.
					Also, when the decimals have to be considered minutely, it is preferrable to use harmonic mean.
					ex, in a tournament, id Sachin's score is following in 4 matches [60,70,80,290], mean will be 125
					i.e. it will imply Sachin performed exceptional in the tournament, but the avg increased just because of score
					of 290 in 1 match (i.e. outlier) thus giving incorrect judgement.
					ex 2, when we are taking weight of chemical composition
					[0.00354678,0.00351111,0.0034999], although, if rounded off to 4 decimals i.e. [0.0035,0.0035,0.0034]
					still there will be considerable difference, so we need to consider decimals minutely.
					
					formula: n/sum of each element inversion => 1/ (1/x + 1/y + ....). ex, HM of [10,20,30]
					will be 1/(1/10 + 1/20 + 1/30) 
		
	b) Mode: most repeated value. ex, if a class has students from following background,[BE, BE,MBA, BE,BE,MCA], then mode is BE.
			 however, if the data is like, in a class of 100 students learning DS,[51 male students and 49 female students], then
			 using mode is not a very good idea to define that males students are preferring DS as career.
			 ex 2, among cancer patients, 
				1) if 100 are non smokers and 900 are smokers, we can say that "smoking cause cancer"
				2) if 501 are non smokers and 499 are smokers, we cannot infer if "smoking causes cancer"
			 
			 Note: Mode is used when the number of choices is less (ex, choice from yes or no).
				   Mode cannot be used when data is variable, in this case we create status (or category)
				   ex,
				   a cricketers score in 10 matches - [90,70,75,0,79,100,50,49,0,45]
				   Mode will be 0, which does not indicate the player's performance in 10 matches.
				   so, we categorize it as,
				   [90,70,75,0,79,100,71,49,0,72] =>  [above, above,above,below,above,above,above,below,below,above]
				   where above is (scores above average) and below is (scores below average)
				   so here mode(player performance) = above, hence cricketer performed well.
				  
	c) Median: sort the data in asc and desc and take exact middle value (called central value). Usually used for comparitive
			   analysis
				example, [10,90,34,6,89,67,12] =>sort [6,10,12,34,67,89,90] => Median is 34 (i,e, 4th value from both ends)
						i.e. 50% of data is less than 34.
					 [10,90,34,6,89,67] => sort [6,10,34,67,89,90] => Median is 50.5 (i.e. (34+67)/2)
					 
				NOTE: We come to median only when mean and mode fetches the same/similar results.
				   
				example, there are 2 cricketers who played 1000 matches.
				C1: median is 60 runs
				C2: median is 80 runs
				implies,
				C1                           			C2
				C1 scored >=60 runs in 500 matches		C2 scored >=80 runs in 500 matches
				-										C2 scored >=60 runs in approx 700 matches (assumptions)
				C1 scored >=80 runs in 300 matches      -
				implies,
				C2 is a better scorer than C1.
				
				if median does not help decide i.e. median is also same/similar, we use QUARTILES (1/4th of data).
				
				there are 3 QUARTILES,
				a) Q1: 25% of entire data i.e. 25% of data is less than Q1 and 75% is greater than Q1
				b) Q2: 50% of entire data i.e. 50% of data is less than Q2 and 50% is greater than Q2 i.e.median.
				c) Q3: 75% of entire data i.e. 75% of data is less than Q3 and 25% is greater than Q3.
				
Question) If we have to determine the average salary of an indian citizen, which MCT will we use?
Analysis)
			Mean: will calculate the avg but will ignore the outliers as there can be people with 0 and billion $ salaries.
			Mode: There may be a lot of people with 0 salary, so 0 will be the most repeated value.
			Median: will sort the data and calculate the central value
			
			Thus Median will be the best option at it will neutralize the outliers.
			
Question) If we are estimate next year's budget for a hospital, which MCT will we use.Data is monthly expenditure for last yr
Analysis)
			Mean: will calulate the avg of monthly expenditure. We should consider outliers as we need to consider reason for outliers.
			Mode: will determine the most repeated value. The data may be variable and repeated value may ignore outliers.
			Median: will calculate the central value but may ignore outliers.
			
			Mean will be the best way as when planning for future u need to consider outliers.
				
------------------------------------------
MCT (mean, median, mode, quartile) when all are equal or similar, how to determine data:

MCT helps determine positivity (how good) of data. When all factors are same, we will need to determine how the negativity of 
data shows.

The negativity of data can be determined by dispersion techniques. Dispersion determines spread of a variable.

spread determines how clustered the values are, such as,
1) if spread is less, the values are clustered closely.
2) if spread is high, the values are distributed far off.

Spread determines:
1) variability/risk : spread is high, variability is high.
2) consistency: spread is high, consistency is less.

example,
in a tournament,
Sachin's score: 60,70,80......i.e. mean is 70
Ganguly's score: 0,10,210.....i.e. mean is 70

from data,
probability of Sachin to score a 70 runs in next match is high and of ganguly is low.
Variability of Sachin's score is less (i.e. less spread), so consistency is high and risk/variability is low. On other hand,
Ganguly's variability is high, consistency is less and risk is high.


Dispersion techniques include (Range, variance, SD are also called measures of Variability):
a) Range: difference between maximum value and minimum value is a data set. Used to give variability or spread of data.
		  High range implies high variability i.e. less consistency, less range implies less variability i.e. high consistency.
		  example,
			in a tournament,
			Sachin's score: 60,70,80......i.e. range is 20 (i.e. 80-60)
			Ganguly's score: 0,10,210.....i.e. range is 210 (i.e. 210-0)
		  Range is good ONLY when there are no outliers.
		  example,
		  Sachin's score: 60,70,80......i.e. range is 20
		  Sachin's score: 0,60,70,80,200 ...i.e. range is 200 i.e. high risk, but sachin's score implies consistency.
		  
		  NOTE: usually Range is not used.
		  
b) IQR (Inter Quartile Range): i.e. (Q3 - Q1) where,
		Q1 -> 25% is less than Q1
		Q3 -> 25% is greater than Q3
		example,
		Sachin's score: 0,60,70,80,200
		where, 0 will fall below Q1 and 200 will fall above Q3, so the outliers are neglected.
		There are 2 problems with IQR:
		1) We may sometime miss imporant data. example, if Sachin has played 1000 matches,
			250 matches will fall below Q1 and 250 matches will fall above Q3. So we are missing data for 500 matches.
		2) If between Q1 and Q3, data is clustered in certain positions and not consistent, IQT will give incorrect explaination.
		
		So IQR is of greates advantage when data is equally spread between Q1 and Q3.
		
c) Variance: 
		formula is : sigma(x - xbar)/n where xbar is average, n is number of elements, sigma is sum.
		(x-xbar) is called deviation which can be +ve or -ve. so we square the deviation to calculate Variance.
		so formula becomes
		sigma(x - xbar)^2/n ie devation square divided by number of elements.
		example, variance of [10,20,30]
		average i.e. xbar is 20
		deviation is [10-20,20-20,30-20] i.e. [-10,0,10]
		(x - xbar)^2 is [100,0,100]
		sigma[(x - xbar)^2] is 200 i.e. sum
		variance is 200/3 = 66.66

		variance of population i.e. entire data available is sigma(x - xbar)^2/n
		variance of sample i.e. only sample data available for inference is sigma(x - xbar)^2/(n-1)
		
		high variance implies high variability/risk and less consistency
		low variance implies low variability/risk and high consistency.
		
		Variance is only good FOR COMPARISION, we cannot comment by calculating variance for a single dataset
		example, v(sachin) = 110, v(Ganguly) = 180,
		we can say that Sachin is more consistent than Ganguly, but if we have have only Sachin's variance, 
		we cannot comment on anything.
		
d) Standard Deviation: SD is sqrt(variance) i.e. square root of variance.
	SD of population is sqrt(variance of population)
	SD of sample is sqrt(variance of sample)
	
	SD overcomes the disadvantage of variance, i.e. it can be used to comment on behaviour of a single dataset.
	example,
	Sachin: mean is 70 and SD is 10
	Ganguly: mean is 70 and SD is 50
	
	Sachin is more consistent as SD is less than Ganguly i.e. Ganguly is more variant.
	Now, if we consider only sachin's data i.e. mean is 70 and SD is 10
	High peak is (mean + SD) = 70 + 10 = 80
	Low peak is  (mean - SD) = 70 - 10 = 60
	implies, Sachin's runs are fluctuating between 60 to 80.
	Similarly for Ganguly, runs are fluctuating between 20 and 120.
	
	As spread of Ganguly's runs is more, prediction becomes difficult.
	
	
NOTE: when determining data behaviour, we should consider BOTH positivity and negativity.
example,
consider the investment returns of IBM and Infosys shares where,
			Mean		SD
Infosys		20			5 
IBM			30			40

mean will imply, if we invest 100,000 INR, for Infy and IBM we get 20,000 and 30,000 INR profit, so IBM is more profitable
SD will imply risk, i.e. IBM has more variance than infy i.e less consistency.

peak for Infy is 15 to 25
	 for IBM is -10 to 70

i.e.
			profit in low peak		profit in high peak		summary
Infosys		15000 INR				25000 INR				less returns but less risk
IBM	 		-10000 INR 				70000 INR				high risk but high returns.
	
	
-------------------------------------------------------------
Multivariate analysis: i.e. analysis over combination of variables. example, covariance, correlation, regression (linear and non-linear).

1) Covariance: This helps understand dependency between 2 variables. example, if business question is "if temperature is 30 C, what will be the cold-drink sales",
			   first we will have to establish/prove dependency between temperature and cold-drink sales. 
			   
			   Prediction model can only be based on dependency between variables. No dependency implies no prediction. 
			   i.e. we need to understand how i/p variable is influencing target variable. example,
			   based on height, weight and age, we need to predict cholestrol levels, if we apply,
			   cov(age,cholestrol) = 1.23     i.e. positive dependency
			   cov(height, cholestrol) = 0.0  i.e. zero dependency
			   cov(weight,cholestrol) = -0.34 i.e. negative dependency
			   
			   so we can ignore height for predicting cholestrol levels.
			   
			   "covariance measures the degree to which two variables are linearly associated."
			   
			   Covariance formula,
			   COV(x,y) = sum [(x - mean(x)) * (y - mean(y))] / (n-1)
			   where,
			   (x - mean(x)),(y - mean(y)) -> deviation (distance) of x and y from mean.
			   n -> number of elements.
			   
			   there can be 3 possible values for covariance,
			   =0 : implies no dependency between the variables. i.e. change in value of one variable does not impact value of other variable. i.e. both variables are INDEPENDENT.
			   >0 : implies positive dependency i.e. increase in value of one variable will cause increase in value of other variable. similar for DECREASE.
					example, cov(temperature, cold-drink sales) will be +VE.
			   <0 : implies negative dependency i.e. increase in value of one variable will cause decrease in value of other variable. similar for DECREASE.
					example, cov(temperature, sweater sales) will be -VE.
			   
			   NOTE: is covariance is very close to ZERO, then also we can imply NO DEPENDENCY. example, 0.000073, -0.000058 will also imply NO DEPENDENCY.
			   
			   Problem with covariance is that it can just explain positive OR negative OR no dependencies. How strongly/weakly the variables are associated/bonded/dependent 
			   cannot be explained with covariance.
					 
2) Correlation: Correlation also helps understand dependency between 2 variables. This however helps overcome the disadvantage of covariance.
				i.e. we can use to derive strong dependency or weak dependency.
				
				"Correlation is a scaled version of covariance that takes on values in [−1,1] 
				with a correlation of ±1 indicating perfect linear association and 0 indicating no linear relationship."
				
				correlation is measured by "correlation coefficient" (SYMBOL is "r") where -1 <= r <= 1 (i.e. r lies between -1 and +1)
				
				correlation formula,
				r = n[sum(x*y) - {sum(x)*sum(y)}]/sqrt[{n*sum(x^2)- sum(x)^2}*{n*sum(y^2)- sum(y)^2}]
				where,
				x*y -> product of each x value with corresponding y value 
				n -> number of elements
				sum(x^2)-> sum of square of each element.
				sum(x)^2 -> square of sum of each element.
				
				There are 3 possible values similar to covariance. However based on range of values, we can explain dependencies in 7 ways,
				-1____________-0.5___________0____________0.5_____________+1
				 |     |               |     |      |              |      |
                 3     7               6     1      4              5      2    -> positions explaining dependency. 
                                                                                7 (range between -1 to -0.5)
																				6 (range between -0.5 to 0)
																				4 (range between 0 to 0.5)
																				5 (range between 0.5 to 1)
			   
			    when r = 1         i.e. perfect positive correlation- same percentage reflection happens between values of 2 variables i.e. y = f(x)
																	   ex, if x =(10,20,30,15) then y =(1,2,3,1.5)  i.e. y = 10% of x
																	       if x =(10,100,30,90) then y =(20,200,60,90) i.e. y = 2x 
																		   so both variables move in the same direction and reflection of changes is also same.
				    r = -1         i.e  perfect negative correlation - same percentage reflection happens between values of 2 variables but in opposite direction i.e. y = -f(x)
																	   ex, if x =(10,20,30,15) then y =(-1,-2,-3,-1.5)  i.e. y = minus(10% of x)
																	       if x =(10,100,30,90) then y =(-20,-200,-60,-90) i.e. y = -2x 	
																		   i.e. both move in the opposite direction but reflection of changes is also same.
					0 < r <=0.5   i.e. weak positive correlation     - both variables will move in same direction but reflection is NOT close (i.e. FAR reflection)
																	   ex, x increases by 70% but y increased by only 10%.
																	       x increased by 7% but y increased by 80%
																		   i.e. high change (increment/decrement) in x will cause less change in y OR
																		        high change in y will cause less change in x
																			so less change in one variable will cause high change in other variable.
					 r> 0.5        i.e. strong positive correlation - both variables move in the same direction BUT the percentage reflection is not same but close
					                                                   ex, if x increases by 10% (10,11,12,13) but y increases by 20% (1,2,4,8)
																	       x increases by 10% and y by 9%, x decreases by 11% but y decreases by 13%.
																		   so less change in one variable will cause less change in other variable and
																		   high change in one variable will cause high change in other variable.
					 -0.5 <= r < 0 i.e. weak negative correlation   - both variables move in opposite direction but reflection of changes is Far.
																	   ex, if x gets incremented by 70%, but y gets decreased by 10%
																		   if x gets decremented by 7% but y increased by 80%
																		   i.e. high increment in one variable will cause less decrement in other variable.
																		   OR low decrement in one variable will cause high increment in other variable.
					 r < -0.5      i.e. strong negative correlation	- both variables move in opposite direction but reflection of changes is close.
                                                                      ex, x increased by 10% but y decreased by 9%
																		  x decreased by 9% but y increased by 8% 
			         r = 0         i.e. no dependency
					 
	graphical representations of correlation:
	1) perfect positive correlation: angle of the curve is exactly 45 degree with x-axis.
	2) strong positive correlation: angle of the curve is > or < than 45 degree with x-axis but the angle will be close to 45 degree (ex, 43, 47).
	3) weak positive correlation:  angle of the curve is > or < than 45 degree with x-axis but the angle will be far way from 45 degree (ex, 20, 75).
	
	4) perfect negative correlation: the line will form a triange with x and y axis and where it meets x and y axis, the angle will be exactly 45 degree.
	5) strong negative correlation: the line will form a triange with x and y axis and where it meets x axis, the angle will be close to 45 degree.
	6) weak negative correlation: the line will form a triange with x and y axis and where it meets x axis, the angle will be far away from 45 degree (i.e. close to 90 or 0 deg).
	
----------------------------------------------------
Predictions & forecasting:

Predictive analysis comprises of 2 models -  prediction and forecasting.

Prediction is estimating the value of one variable (i.e. target variable) based on values of other variable (i.e input variable).
example, based on temperature, estimate cold drink sales.
         based on age, experience, designation, industry estimate annual income.
		 
Predictions are used to predict,
1) regression problems: in this case target variable is continuous (i.e. values change). I/p variables should also be continuous.
            regression models primary are of 2 types,
			a) linear regression
			b) non linear regression
			
			there are others like ridge regression, lasso regression.
2) classification problems: target variable is a classifier (i.e. any one of the given options i.e. discrete variables). example, tomorrow will it rain (yes/no).
                            Classifiers (options) can be 2 or more.
			classification models include,
			a) logistic regression
			b) Naive bays classifier
			c) decision tree
			d) random forest
			e) support vector machines (SVM)
		    f) ADABooster	
		 
Forecasting is estimating feature depending on timeline. Input and target variables are same for forecasts.
example, based on a list of temperature for last 20 days, estimate tomorrow's temperature.
         based on last 100 days of Infy's share values, estimate Infy share price for tomorrow.

Forecasting models include,
1) time series analysis: following a series of timelines i.e. tomorrow depends on today's data and today depends of yesterday's data and so on
	example, if we have data for last 1 month, we can use if to predict the next day's data.
	TSA models include,
	a) Auto regression (AR)
	b) Integration
	c) Moving average (MA)
	d) ARMA: combination of AR and MA
	e) ARIMA: combination of AR, Integration and MA
	there are more models.
	
Linear regression: There are 2 LR models
a) Simple LR :  i.e. there will be 1 i/p variable and 1 target variable. 

				Intercept: indicates the location of regression line where it intersects an y-axis. Origin of a line.
				Slope    : indicates the steepness of the regression line. Trignomatically, tan(angle) with x-axis.
				
				The slope and the intercept define the linear relationship between two variables
				
				equation, y = a + bx
				where a-> intercept
				      b -> slope
					 
				example, y = 2 + 5x The y-intercept is 2, so y will be starting from 2 and The slope is positive 5. When x increases by 1, y increases by 5. 
                         y = 7 - 2x The y-intercept is 7.2, so y starts from 7.2 and The slope is negative 0.4. When x increases by 1, y decreases by 0.4. 	

				y = bx will mean that intercept is zero i.e. the regression line will pass thru the origin but this is not a real scenario.
			    as there will always be "bias" associated with i/p variable.
				ex, if temp increases, cold-drink sales increases but if temperature is 0, does not mean that cold-drink sale is 0.
				    if age grows, weight grows BUT if age is 0. does not mean that height or weight will not be zero.
					
				Thus "intercept is used to handle this bias."
				
				how to find intercept and slope in a + bx:
				here,
				slope     -> b = cor(x,y) * [sd(y)/sd(x)] where cor -> correlation between x and y & sd is standard deviation 
				intercept -> a = mean(y) - {b * mean(x)}
				
				example-1,
				x        y
				10		 30
				40		 120
				100      300
				
				1st change in x = 30; change in y = 90
				slope = change in y/change in x = 90/30 = 3
				
				2nd change in x = 60; change in y =180
				slope = change in y/change in x = 180/60 = 3
				
				Matrix representation of SLR: i.e. y = a + bx
				[y] = [1 x][a
							b]
				[1 x] is a 1*2 matrix
				[a    is a 2*1 matrix
				 b]
				
				as per Matrix multiplication rule, num of cols of 1st matrix == num of rows of 2nd matrix
				so this multiplication will produce 1 * 1 matrix	  

b) Multiple LR : i.e. multiple i/p variables and 1 target variable. 
				 
				equation, y = b0 + b1.x1 + b2.x2 + ........ + bn.xn
				where b0 -> intercept
				      b1, b2....bn -> slope of x1,x2,x3.....xn
                 
				 These coefficients are calculated based on given data.
				 
				MLR has multiple slopes as each i/p variable has different impact on target variable. Intercept however remains constant.
				example if income is dependent on 2 variables age and technology, each i/p variable (i.e. age and technology) will have its own impact on the
				target variable (i.e. income), so there will be different slopes. 
				
				example,
				"age","w","h","chol"
				20,56,5.5,45
				25,70,6.0,55
				24,90,5.9,89
				34,100,5.7,99
				45,120,5.8,100
				22,78,5.5,66
				28,80,5.6,77
				30,90,5.9,90
				
				Problem statement: based on age, weight, Height, predict person's cholestrol level.
				here i/p variables:   age, weight, Height
				     target variable: cholestrol level
				i.e multiple i/p variable.
				
				so MLR equation will be cholestrol = b0 + b1*age + b2*w + b3*hand
				ex, 45 = b0 + 20 * b1 + 56 * b2 + 5.5 * b3
				
				in matrix format, this equation may look like,
				y1 = b0 + b1 * x11 + b2 * x12 + b3 * x13 + ....... + bn * x1n
				where,
				y1 -> cholestrol in first row
				x11 -> 1st row * 1st column
				x12 -> 1st row * 2nd column

				similarly y2 = b0 + b1 * x21 + b2 * x22 + b3 * x23 + ....... + bn * x2n
				
				so the matrix will look like
				[y1] = [1 x11 x12 x13 .... x1n][b0
												b1
												b2
												b3
												.
												.
												.
												bn]
												
				1st matrix is : 1 * (n+1)
				2nd matrix is : (n+1) * 1
				multiplication will fetch : 1 * 1 matrix
				
				if we convert all such rows y2, y3....yn into matrix format
				
				[y1   = [1 x11 x12 x13 .... x1n   * [b0
				 y2      1 x21 x22 x23 .... x2n		 b1
				 y3      .                           .
				 .		 . 							 .
				 .
				 .
				 ym]     1 xn1 xn2 xn3 .... xmn]     bn]
				 
				1st matrix is [m * (n+1)] dimension
				2nd matrix is [(n+1) * 1] dimension
				so result is [m * 1] dimension
				
				to find the value of intercept (B) in Y = X * B
				where Y -> Y Matrix in MLR
				      X ->  X Matrix in MLR
					  B -> B Matrix in MLR
					  
				mathematically B = Y/X but this is incorrect as X should be a square matrix to calculate (1/X)
				so we can use
				Transpose(X) * Y = [Transpose(X) * X] * B
				Transpose -> convert rows to columns
				[Transpose(X) * X] will help form a square matrix 
				ex, if X is [5 * 4] matrix, Transpose[X] will be [4 * 5] matrix and
				[Transpose(X) * X] will be [4 * 5] * [5*4] = 4 * 4 matrix

				Transpose(X) * Y is done to maintain equilibrium on both sides
				
				so B = inv[transpose(X) * X] * [Transpose(X) * Y]
				size of B matrix will be 
				inv[transpose(X) * X] = [ (n+1 * m) * (m * n+1)] = [n+1 * n+1] matrix
				[Transpose(X) * Y]    = [ (n+1 * m) * (m * 1) ]  = [n+1 * 1] matrix
				so [n+1 * n+1] * [n+1 * 1] = [n+1 * 1]
				so B is [n+1 * 1] matrix
				
				once we have value of B (Beta), we can put it in MLR and calculate Regression value (Ycap)
				which can then be put into different accuracy models to determine accuracy of a prediction
				
				consider example,
				"age","w","h","chol"
				20,56,5.5,45
				25,70,6.0,55
				24,90,5.9,89
				34,100,5.7,99
				45,120,5.8,100
				22,78,5.5,66
				28,80,5.6,77
				30,90,5.9,90
				
				Problem statement: based on age, weight, Height, predict person's cholestrol level.
		
		
				one age age^2 age^3 w  w^2  w^3 	h 		h^2 	h^3
		X	[	1	20  400   8000  56 3136 175616  5.5     30.25   166.375
		        2nd row
				.
				.
				last row                                                      ]
				
		Y   [45
			 55
			 89
			 .
			 .
			 .
			 90]
			 
		B  	inv[transpose(X) * X] * [Transpose(X) * Y] will be a 10*1 matrix
		
		here,
		B[1,1] = b0 = intercept
		B[2,1] = b1 = slope of age
		B[3,1] = b2 = slope of (age)^2
		B[4,1] = b3 = slope of (age)^3
		
		B[5,1] = b4 = slope of (weight)
		B[6,1] = b5 = slope of (weight)^2
		B[7,1] = b6 = slope of (weight)^3

		B[8,1] = b7 = slope of (height)		
		B[9,1] = b8 = slope of (height)^2
		B[10,1] = b9 = slope of (height)^3
		
		
How to test quality of a regression line:
This is done using  Rquare method i.e r^2 (where r is correlation)
example,
r = 0.9
r^2 = 0.81 i.e 81% of values are close to regression line and remaining is far way from regression line.

75% is the standard for regression line quality to be good.

The problem with this approach is that rsq is hypothetically true but not in reality i.e. there is no benchmark for closeness.
example, if the rsq is 81%, it does not imply how close is the prediction to the regression line.
---------------------------------------------------------
How to choose a model for prediction:

this can be done using "hypothesis testing".
if p <= 0.05, then do linear regression
if p > 0.05 , then do non linear regression
where p -> benchmark

where p is calculated by t-test, z-test, f-test. All these are inferential test metrics.
However these test metrics is good for low sample volume. example,
t is good for 130 sample
z is good for 160 sample
f is good for 450 sample
but in organizations now, the volume of sample data is usually very high.

In ML, the solution for this problem is "accuracy testing" is used to select a model.
step-1: take 20% random sample from data. 
step-2: Apply all possible models. 
step-3: test accuracy of all models.
step-3: chose model with highest accuracy.
Step-4: apply model from step-3 to entire data.
---------------------------------------------------------
non linear regression:
a curve best represents non linear regression [as a st line best represents linear regression]

NLR has 3 models as:
1) Quadratic model:
	y = b0 + b1 * x + b2 * (x^2)
	example,
	x   y
	10	120
	5	50
	
	X i.e. x matrix = [1 10 100
					   1  5  25]
					   
	Y i.e. y matrix = [120
	                    50]
						
	B = inv[transpose(X) * X] * [Transpose(X) * Y] i.e. same as linear reg
	B is 3 * 1 matrix
	
	B[1,1] -> b0 i.e. intercept
	B[2,1] -> b1 i.e. slope of x
	B[3,1] -> b2 i.e. slope of x^2
	
	
2) Cubic models:
	y = b0 + b1 * x + b2 * (x^2) + b3 * (x^3)
	example,
	x   y
	10	120
	5	50

	X i.e. x matrix = [1 10 100 1000
					   1  5  25 125]
					   
	Y i.e. y matrix = [120
	                    50]
						
	B = inv[transpose(X) * X] * [Transpose(X) * Y] i.e. same as linear reg
	B is 4 * 1 matrix
	
	B[1,1] -> b0 i.e. intercept
	B[2,1] -> b1 i.e. slope of x
	B[3,1] -> b2 i.e. slope of x^2
	B[3,1] -> b3 i.e. slope of x^3
	
3) power model: this curve will be like sine wave
	y = b0 * (b1^x) i.e. [a * (b^x)]
	
	for PM, derive b0 and b1.
	
	example,
	x   y
	10	120
	5	50

	X i.e. x matrix = [1 10 
					   1  5]
					   
	Y i.e. y matrix = [120
	                    50]
						
	B = inv[transpose(X) * X] * [Transpose(X) * Y] i.e. same as linear reg
	B is 2 * 1 matrix
	
	B[1,1] -> b0 i.e. intercept
	B[2,1] -> b1 i.e. slope of x

In case we have 4 models,
1) linear regression: Y = X * B
2) quadratic NLR
3) cubic NLR
4) Power NLR
in order to chose which one is appropriate for prediction, we will do "accuracy test". The model which gives highest accuracy value, will be chosen.

----------------------------------------------------------
Logistic regression: this is used to predict classifiers (i.e. options). example, will it rain or not (An. Yes/No)

----------------------------------------------------------
Standard error:

Formula,
se = sqrt[sum(y - mean(y))^2]/(n-2)

example,
y = 3 + 2x        -------> has error quotient
se = 4
y - se = 3 + 2x
i.e. y = 7 + 2x  ---------> has less error
se = 2
i.e. y = 9 + 2x  ---------> has even less error
.
. 
.
at one point the error again starts increasing so we will have to stop at that point.

in this approach, there are 2 problems,
1) Intercept keeps increasing.
2) slope is constant.

but the 2 factors should be ideally neutralized.

In ML, we use Gradiant Descent Algorithm where gradiant will adjust both intercept and slope.so gradient will calculate the slope and intercept also wrt SE.

---------------------------------------------------------
Principal component Analysis (PCA) - This is used in machine learning

PCA is the practice of eliminating unnecessary input variables (i.e. variables not participating well) before training 
a model for machine learning. This process is also called reducing dimensionality.

correlation matrix -> eigen matrix (values and vectors) -> PCA
where -> serves as i/p to
 
----------------------------------------------------------



	
	

				
	
	
	
								


								
								
								
								
								

					
					
					 






