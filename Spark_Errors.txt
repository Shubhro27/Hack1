Reference: https://stackoverflow.com/questions/24696777/what-is-the-relationship-between-workers-worker-instances-and-executors
https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster/32628057

spark config in /opt/app/workload/deployments/servers/spark-2.2.0/conf/spark-env.sh

SPARK_WORKER_INSTANCES - start more than 1 worker process on a single physical machine. The default value is 1.
						 standalone cluster manager, currently it still only allows one executor per worker process on each physical machine. 
						 Thus in case you have a machine to run multiple exectuors on it, you have to start more than 1 worker process.
						 In YARN/MESOS (i.e. cluster manager), we can run multiple executors on the same machine with one worker, 
						 thus there is really no need to run multiple workers per machine.
						 
SPARK_WORKER_CORES - explicitly to limit the cores per worker, or else each worker will try to use all the cores. This issue has been fixed since SPARK 1.4.


In Spark Standalone mode, there are master node and worker nodes.

Driver Program (SC) -----> Cluster Manager ------> Worker Node [ [EXECUTER  | CACHE] -> Task, Task ] <----------------------------
      |-----------------------------------------------|--------------->  Worker Node [ [EXECUTER  | CACHE] -> Task, Task ] <-----|
	  
Check this link once: https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster/32628057
	  
What is Worker Instance? worker instance is a process to execute spark tasks/jobs. 

Does 2 worker instance mean one worker node with 2 worker processes (worker process = executors)?  
Suggested mapping for node(a physical or virtual machine) and worker is 1 Node = 1 Worker process.
run multiple executors on a node, you have to start more than 1 worker process that is thru following property SPARK_WORKER_INSTANCES.

Does every worker instance hold an executor for specific application (which manages storage, task) or one worker node holds one executor?
Yes, A worker node can be holding multiple executors (processes) if it has sufficient CPU, Memory and Storage.

two reasons why you want to have multiple instances:
 (1) garbage collector pauses can hurt throughput for large JVMs 
 (2) Heap size of >32 GB can’t use CompressedOoops

NOTE: Number of executors in a worker node at a given point of time entirely depends on 
	  a) work load on the cluster AND
      b) capability of the node to run how many executors.

i.e. WORKER NODE will comprise of:

Worker ----> ExecutorRunner ----> ExecuterBackEnd [ Executor [ comprises of {task 1,2,3,4...},{cache} ] ]
  | -------> ExecutorRunner ----> ExecuterBackEnd [ Executor [ comprises of {task 1,2,3,4...},{cache} ] ]
  
############################################################################################
How to set Multiple Spark Worker Instances on a single Node:

Mesos and YARN can, out of the box, support packing multiple, smaller executors onto the same physical host, 
so requesting smaller executors doesn’t mean your application will have fewer overall resources.

However for standalone mode, we have to launch multiple Spark worker instances on a single node as:
	SPARK_WORKER_INSTANCES=3 SPARK_WORKER_CORES=2 ./sbin/start-slaves.sh
	where,
	3 -> launch three worker instances on each node.
	2 -> Each worker instance will use two cores.
	
to manually start workers and connect to Spark’s master node, command is:
	./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT
	
#############################################################################################

set amount of executors to be equal to cluster size but there are always only 2 of them. (YARN and STANDALONE):

In YARN, 
a) if using SparkSubmit class as driver and it has appropriate --num-executors parameter which is exactly what I need.
   Number of executors is used by Driver to place a job in YARN.
   
b) In spark-defaults.conf, we can use spark.executor.instances property inside it.

c) Programatically, by setting the parameters "spark.executor.instances" and "spark.executor.cores" on the SparkConf object.
	i.e. 
	SparkConf conf = new SparkConf()
      // 4 workers
      .set("spark.executor.instances", "4")
      // 5 cores on each workers
      .set("spark.executor.cores", "5");
	  
	This allows an application to run multiple executors on the same worker, provided that there are enough cores on that worker.

In STANDALONE:
In a standalone cluster you will get one executor per worker unless you play with spark.executor.cores and a worker has enough cores to hold more than one executor. (As @JacekLaskowski pointed out, --num-executors is no longer in use in YARN 

##################################################################################################

The workers available in a spark cluster can be viewed from SPARK Master UI as,

Workers 
Worker Id 									Address 			State 	Cores 			Memory 
worker-20190101023931-1XX.6.XXX.XXX-3xx18  	1XX.6.XXX.XXX-3xx18 ALIVE 	12 (12 Used) 	50.0 GB (20.0 GB Used)  
worker-20190101024207-1XX.6.XXX.XXX-3xx01  	1XX.6.XXX.XXX-3xx01 ALIVE 	12 (12 Used) 	50.0 GB (36.0 GB Used)  
worker-20190101024247-1XX.6.XXX.XX-4xx44  	1XX.6.XXX.XX-4xx44 	ALIVE 	12 (11 Used) 	50.0 GB (36.0 GB Used)  
worker-20190101024317-1XX.6.XXX.XX-4xx93  	1XX.6.XXX.XX-4xx93 	ALIVE 	12 (12 Used) 	50.0 GB (20.0 GB Used)

##########################################################################################################
1.	What is the total number of executors? 
[Shubhro] : I hope you are referring to --total-executor-cores 12.

This config runs in coordination with --executor-memory 4G

When this config is passed, Spark Driver will try to find executors which have available executor-memory = 4G and 12 free cores. If found, SPARK Driver will submit the job on these nodes. If not available, spark driver will be unable to submit the job at all. 
This is what was happening when Ankit was trying to submit driver with --total-executor-cores 12, there are no nodes available with 12 free cores, so the job does not submit at all.


2.	Do we know what the batch size at pick hours? Looks like we are getting  more than 1000 records in a batch. If that’s the case then there could be pending jobs.
[Shubhro] : In high-traffic hrs, we have seen batch size with upto 4k records, and there are frequent batches which comprise of > 1000 records.

spark.streaming.kafka.maxRatePerPartition is actually implemented as rate per partition per second of the batch stream. In our case, maxRatePerPartition is 1000 and batch interval is 5 seconds, so the maximum rate per batch is 5,000 records. At least for EDGE, we did not see 5k records in a batch, but there can be batches with 6k records but the driver will only allow 5K and move the remaining 1K to a different batch.

3.	Instead of allocating all the cores (total-executor-cores = 12) , can we use dynamic allocation? The reason being if the same cluster is used by multiple jobs/applications then it would work in FIFO basis. So in that case the first job would try to acquire all the cores and other jobs might not get cores and there could be performance issue there.
[Shubhro] : dynamic allocation on a spark standalone cluster is a good option. We will need to set 2 indicators,
a)	spark.dynamicAllocation.enabled to true
b)	spark.shuffle.service.enabled set to true.
But there are resource utilization calculations, I am not familiar with, so will need to do some web surfing.

To the other question, my understanding is a bit shady, but if driver found a node with 12 cores available, it will be assigned to the job and if the job still consumes time, then certainly the other jobs will wait for the cores to be freed. But then, if a job is running on 12 cores and not proceeding, usually it is observed to be a code issue as 12 cores is a considerable resource to process a job.

When this is required? 
[Shubhro] : Basically we are creating multiple executers on a node (as against 1 by default). More executors will imply more tasks being handled in parallel. This is usually done when there is sufficient CPU and memory on your worker nodes.

More load from source system? As we never did this in earlier submit commands in any of the projects as far as I know.
[Shubhro] : By source system, if you are implying driver node, then I am not sure if there will be addition load. In YARN/MESOS, these cluster manager themselves take up the task to create multiple executors, but in STANDALONE CLUSTER, this num-executors config has to be explicitly mentioned to start multiple executors in one node.

In my previous project, I have worked on SPARK executed over Hadoop 2.0 (cluster manager being YARN). There we didn’t use this parameter, as YARN itself took decision to launch multiple executors. So, it will be advisable to have a second opinion of an expert who has set up and worked on SPARK STANDALONE cluster.




4.	Executor-cores 1. Is this the recommended setting? If I understood it correctly then with this setting an executor would get maximum 1 core to process the task(s). Will this cause a performance issue?
[Shubhro]: Executor-cores is the property that controls the number of concurrent tasks an executor can run i.e. if Executor-cores = 1, then executer can run only one task at a time, if it is “4”, the executer can run 4 tasks concurrently. 

So if Executor-cores = 1 and  --total-executor-cores 8, then I would assume it is under-utilization of available resources as it will reduce parallelism in a executer.
