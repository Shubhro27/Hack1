sreeram hive json blog on google.

-------
SPARK SQL is not DB. SS is a library to process spark data objects using SQL statements (i.e follows mySQL based sql syntax).

SPARK in general has 4 context,
Spark context
sparkstreaming context
Spark SQL provides 2 types of context,
SQL context : i.e. using this context we can process spark objects using select statements.
Hive context: i.e. using this context,we can integrate spark with hive 
					i.e. we can access hive env from spark and use HQL queries.
					
----------------------------------------------------------------------------
creating Dataframe from Sequence:
This can be tried with 2 options,
a) directly setting up schema in toDF functions.
	//example:
	 val list = sc.parallelize(1 to 10)
	 val df1 = list.map(i => (i,i*2)).toDF("single","double")
	 df1.show

b) creating a schema using case class, mapping data to the case class and then applying toDF() with no parameters,
	//example:
	 val list1 = sc.parallelize(1 to 10)
	 case class form1(single:Int,double:Int)
	 val list1_map = list1.map { x =>
	 val sing = x
	 val doub = x * 2
	 form1(sing,doub)
	 }
	 val df2 = list1_map.toDF()
	 df2.show()
	 
//example 2:
	val a = List(("ironman",3),("kabali",2),("bhahubali",5))
	val rating = a.toDF("movie","rating")

	rating.show
	rating.printSchema

//example 3:
	import org.apache.spark.sql.DataFrame

	val df = Seq(("Yoda","Obi-Wan Kenobi"),("Anakin Skywalker", "Sheev Palpatine"),("Luke Skywalker","Han Solo, Leia Skywalker"),("Leia Skywalker","Obi-Wan Kenobi"),
	("Sheev Palpatine",  "Anakin Skywalker"),("Han Solo","Leia Skywalker, Luke kywalker, Obi-Wan Kenobi, Chewbacca"),("Obi-Wan Kenobi","Yoda, Qui-Gon Jinn"),
	("R2-D2","C-3PO"),("C-3PO","R2-D2"),("Darth Maul","Sheev Palpatine"),("Chewbacca","Han Solo"),("Lando Calrissian", "Han Solo"), ("Jabba","Boba Fett")).toDF("name", "friends")

	df.show
	
--------
From Spark 1.6 onwards, SQL context is by default available in spark shell.
However, if creating a program, we will have to,
a) import org.apache.spark.sql.SqlContext
b) val sqc = new SqlContext(sc)


post a,b, steps (in details) to work with SQL context,
1) load data into RDD, example,
	filename -> file1
	sample data ->  100,200,300
					300,400,400
					.
					.
	val data = sc.textFile ("file1")

2) provide schema to RDD i.e. create case class, example,
	case class Rec(a:Int,b:Int,c:Int)

3) create a function to convert file record into case object, i.e. following function will provide schema to the RDD,
	def makeRec(line: String) = {
				val w = line.split(",")
				val a = w(0).toInt
				val b = w(1).toInt
				val c = w(2).toInt
				val r = Rec(a,b,c)
				}
			
4) transform each record into case object,
	val recs = data.map ( x => makeRec(x) )

5) convert rdd into data frame,
	val df = recs.toDF    //there are 2 new data abstractions set with spark 1.3 onwards i.e data frame and data set.

6) create a table instance for the data frame,
	df.registerTempTable("samp")
				
7) apply the sql statement on temp table,
	val r1 = sqc.sql( " select a+b+c as tot from samp ")  //r1 is a data frame i.e. NOT a table.

o/p:
600
900

NOTE: when SQL statement will be applied to temp table, the returned object will be a data frame.
	  TO apply sql statement on result set, we need to register as temp table
	  i.e. r1.registerTempTable("samp1")	  
	  
-----
working on files,example - 2

emp:
101,aaa,70000,m,12
102,bbb,90000,f,12
103,ccc,10000,m,11
104,ddd,20000,f,13
105,eee,90000,f,14
106,fff,40000,m,15
107,iiii,60000,m,13
108,jjj,90000,f,15

val raw = sc.textFile("emp")

case class info(id:Int,name:String,sal:Int,sex:String,dno:Int)

def toInfo (x:String) = {
val w = x.split(",")
val id = w(0).toInt
val name = w(1)
val sal = w(2).toInt
val sex = w(3)
val dno = w(4).toInt
val inf = info(id,name,sal,sex,dno)
inf
}

val infos = raw.map ( x => toInfo(x))

val dfInfo = infos.toDF //converts to dataframe

dfInfo.registerTempTable("dfInfo")

sqc.sql(" select * from dfInfo where sex = 'm' ").show()
sqc.sql(" select sex,sum(sal) as tot from dfInfo group by sex ").show()


emp2:
201,zzzz,14,m,20000
202,yyyy,12,f,10000
203,xxxx,12,m,20000
204,uuuu,11,f,40000

val raw2 = sc.textFile("emp2")

val infos2 = raw2.map { x =>
	val w = x.split(",")
	info(w(0).toInt, w(1),w(4).toInt,w(3),w(2).toInt)
}

val dfinfo2 = infos2.toDF

dfInfo2.registerTempTable("dfInfo2")

val df = sqc.sql("select * from dfInfo union all select * from dfInfo2")  //will have all the info from emp and emp2

df.registerTempTable("df")

sqc.sql("select sex,sum(sal) as tot from df group by sex ").show()

the optimized way to copy data between partitions will be carried by capitalist optimizer.

---------
3 ways to work with .csv file
a) creating dataframe using case class.
	//example,
		val yahoo_stocks=sc.textFile("/user/cloudera/datasets/yahoo_stocks.csv")
		val header =yahoo_stocks.first
		val data =yahoo_stocks.filter(_ != header)
		case class YahooStockPrice(date:String, open:Float, high:Float, low:Float, close:Float, volume:Integer,adjClose:Float)
		val stockprice=data.map(_.split(",")).map(row =>YahooStockPrice(row(0), row(1).trim.toFloat, row(2).trim.toFloat, row(3).trim.toFloat, row(4).trim.toFloat, row(5).trim.toInt, row(6).trim.toFloat)).toDF()

		stockprice.show()
		stockprice.printSchema
		stockprice.registerTempTable("yahoo_stocks_temp") 
		
b) create dataframe using struct i.e. inferring a schema manually
	//example, (general example and not a .csv file)
		import org.apache.spark.sql.{SQLContext,Row}
		import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}

		val schema = StructType(Array(StructField("name",StringType,true),StructField("age",IntegerType,true)))
		val data = sc.parallelize( Seq("john","Adom","Smith")).map(x => (x,20+x.length)) 
			//o/p:Array[(String, Int)] = Array((john,24), (Adom,24), (Smith,25))
		val rowRDD = data.map(x => Row(x._1,x._2)) 
			//O/p: Array[org.apache.spark.sql.Row] = Array([john,24], [Adom,24], [Smith,25])
		//if you are not in spark shell,
		val sqlContext = new SQLContext(new SparkContext("local","Dataframe")
		import sqlContext.implicits._
		//
		val df = sqlContext.createDataFrame(rowRDD,schema)
		df.registerTempTable("people")
		sqlContext.sql("select * from people").show
		
	//example2,
		import org.apache.spark.sql.{SQLContext,Row}
		import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType, FloatType}

		val data = sc.textFile ("sb_sparkSQL_ds/yahoo_stocks.csv")
		val header = data.first
		val yh_file = data.filter ( x => x != header )
		val yh_dump = yh_file.map { x => 
		val arr = x.split(",")
		(arr(0),arr(1).trim.toFloat,arr(2).trim.toFloat,arr(3).trim.toFloat,arr(4).trim.toFloat,arr(5).trim.toInt,arr(6).trim.toFloat)
		}
		val yh_data = yh_dump.map ( x => Row(x._1,x._2,x._3,x._4,x._5,x._6,x._7))
		val schema = StructType(Array(StructField("Date",StringType,true),StructField("Open",FloatType,true),StructField("High",FloatType,true),StructField("Low",FloatType,true),StructField("Close",FloatType,true),StructField("Volume",IntegerType,true),StructField("Adj_Close",FloatType,true)))
		val df = sqlContext.createDataFrame(yh_data,schema)
		df.registerTempTable("yahoo_tab")
		sqlContext.sql("select * from yahoo_tab LIMIT 5").show
		
c) Using databricks package. This is a 3rd party API (i.e. spark-csv API)
	//example,
		//restart your spark shell using this arguments 
		spark-shell --packages com.databricks:spark-csv_2.10:1.5.0
		 
		val characters_df = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").option("delimiter", ",").load("sb_sparkSQL_ds/StarWars.csv")
		characters_df.show
		 
		//dealing with null and empty values
		val filterdata = characters_df.filter(characters_df("haircolor") !== "")
		val changedata = characters_df.filter(characters_df("haircolor") === null || characters_df("haircolor") === "").withColumn("haircolor",lit("some color"))
		filterdata.unionAll(changedata).show

more examples/details in:
https://github.com/databricks/spark-csv
https://community.hortonworks.com/articles/114045/specify-schema-for-csv-files-with-no-header-and-pe.html
-------------------------------------------------------
Why access Hive from Spark?
Hive standalone reside on map-reducse framework i.e. hive queries will be converted to map-reduce job.
In Hive context, we can use the in memory processing power of spark and DAG with persistant features and is free from map-reduce.

Spark SQL limitation: is available only for structured text. i.e. if data is unstructured, we need to process the data with
spark core using RDD API's and spark MLlib

to create a hive context,
a) copy hive-site.xml to /user/spark/conf. If this is not done, spark will not know hive meta-store location.
b) import org.apache.spark.sql.hive.HiveContext
c) val hc = new HiveContext(sc)

post a,b,c steps (in details) to work with hive context, 
	hc.sql (" create database mydb")  //creates DB
	hc.sql (" use mydb")			  //sets the DB as default
	hc.sql (" create table result1 (dno int, tot int)")  //create table within mydb database
	hc.sql ("insert into table result1 select * from <some other table>") //query using HQL
	
-------
combination of SQL and Hive context,

val r1 = sqc.sql ("<some query>")
val r2 = hc.sql ("<some query")

r1.registerTempTable("res1")
r2.registerTempTable("res2")	

we can apply joins, unions or combine the data into any format.

-------
working with JSON using SQLContext,

NOTE: to know the elements and mapping in JSON, use http://jsonviewer.stack.hu/

example,
json -  
	{"name":"ravi","age":20,"sex":"Male"}
	{"name":"vani","city":"Hyd,"sex":"Male"}

val jdf = sqc.read.json("<file path>")    //or sqc.jsonFile("<file path>")

the data frame created will be like, 
name age city sex
ravi 20  null Male
vani null hyd female

consider another json1:
	{"name":"ravi","age":25}
	{"name":"Rani","city":"Hyd"}
	{"name":"Mani","age":24,"city":"Del"}
	
	
example-1 (simple json)
	customers.json:
	{"first_name":"James", "last_name":"Butterburg", "address": {"street": "6649 N Blue Gum St", "city": "New Orleans","state": "LA", "zip": "70116" }}
	{"first_name":"Josephine", "last_name":"Darakjy", "address": {"street": "4 B Blue Ridge Blvd", "city": "Brighton","state": "MI", "zip": "48116" }}
	{"first_name":"Art", "last_name":"Chemel", "address": {"street": "8 W Cerritos Ave #54", "city": "Bridgeport","state": "NJ", "zip": "08014" }}

	val customers = sqlContext.jsonFile("file:///home/shubhro2705854012/sparkLocal/customers.json")

	scala> customers.printSchema
	root
	 |-- address: struct (nullable = true)
	 |    |-- city: string (nullable = true)
	 |    |-- state: string (nullable = true)
	 |    |-- street: string (nullable = true)
	 |    |-- zip: string (nullable = true)
	 |-- first_name: string (nullable = true)
	 |-- last_name: string (nullable = true)
	 
	customers.registerTempTable("customers_tab")
	 
	val cus = sqlContext.sql("SELECT first_name,last_name FROM customers_tab")

	cus.show

	+----------+----------+
	|first_name| last_name|
	+----------+----------+
	|     James|Butterburg|
	| Josephine|   Darakjy|
	|       Art|    Chemel|
	+----------+----------+

NOTE: the customer.json used is not a valid format. Next example will be to pass a valid json and process it.

example-3, valid json i.e. color.json
{
	"colors": [{
			"color": "black",
			"category": "hue",
			"type": "primary",
			"code": {
				"rgba": [255, 255, 255, 1],
				"hex": "#000"
			}
		},
		{
			"color": "white",
			"category": "value",
			"code": {
				"rgba": [0, 0, 0, 1],
				"hex": "#FFF"
			}
		},
		{
			"color": "red",
			"category": "hue",
			"type": "primary",
			"code": {
				"rgba": [255, 0, 0, 1],
				"hex": "#FF0"
			}
		},
		{
			"color": "blue",
			"category": "hue",
			"type": "primary",
			"code": {
				"rgba": [0, 0, 255, 1],
				"hex": "#00F"
			}
		},
		{
			"color": "yellow",
			"category": "hue",
			"type": "primary",
			"code": {
				"rgba": [255, 255, 0, 1],
				"hex": "#FF0"
			}
		},
		{
			"color": "green",
			"category": "hue",
			"type": "secondary",
			"code": {
				"rgba": [0, 255, 0, 1],
				"hex": "#0F0"
			}
		}
	]
}

type-1,
val colorDF = sqlContext.read.json("file:///home/shubhro2705854012/sparkLocal/colors.json")
O/p:
colorDF: org.apache.spark.sql.DataFrame = []

colorDF.show
O/p:
18/06/17 07:10:30 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)
java.lang.RuntimeException: Failed to parse a value for data type StructType() (current token: VALUE_STRING).

type-2,
Note: wholeTextFiles method which produces a PairRDD.  
val colorRDD = sc.wholeTextFiles("file:///home/shubhro2705854012/sparkLocal/colors.json").map(x => x._2)
val colorDF = sqlContext.read.json(colorRDD)  //sqlContext.jsonFile(colorRDD) errors with type mismatch;
colorDF.printSchema
O/p:
	root
	 |-- colors: array (nullable = true)
	 |    |-- element: struct (containsNull = true)
	 |    |    |-- category: string (nullable = true)
	 |    |    |-- code: struct (nullable = true)
	 |    |    |    |-- hex: string (nullable = true)
	 |    |    |    |-- rgba: array (nullable = true)
	 |    |    |    |    |-- element: long (containsNull = true)
	 |    |    |-- color: string (nullable = true)
	 |    |    |-- type: string (nullable = true)
	
colorDF.registerTempTable("color_tab")

sqlContext.sql("select * from color_tab").show

O/p:
	+--------------------+
	|              colors|
	+--------------------+
	|[[hue,[#000,Wrapp...|
	+--------------------+

sqlContext.sql(""" select colors.category FROM color_tab """) //will give the o/p i.e. ignore element in schema.

Reference:https://stackoverflow.com/questions/29948789/how-to-parse-nested-json-objects-in-spark-sql

Other examples,
//example 1 simple json
val persons = sqlContext.read.json("/user/cloudera/datasets/persons.json")
persons.registerTempTable("persons")
sqlContext.sql("select * from persons").show

//example2 nested JSON
val employees_df = sqlContext.read.json("/user/cloudera/datasets/employee.json")
employees_df.registerTempTable("employees")
sqlContext.sql("select id,name,salary,address.city,address.country from employees").show

//example3 JSOn joins,
val store_loc_dump = sqlContext.read.json("sb_sparkSQL_ds/store_locations.json")   //create a DataFrame
val us_states_dump = sqlContext.read.json("sb_sparkSQL_ds/us_states.json")

//store_loc_dump: org.apache.spark.sql.DataFrame = [city: string, state: string, zip_code: string]
//us_states_dump: org.apache.spark.sql.DataFrame = [census_division: string, census_region: string, name: string, state: string]

store_loc_dump.registerTempTable("us_store")
us_states_dump.registerTempTable("us_states")

sqlContext.sql("select sl.city,sl.state,sl.zip_code,us.census_division,us.census_region,us.name from us_store sl JOIN us_states us ON sl.state = us.state")

----------------------------------------
Saving a json format: 
can be done via DF.write.format("json").save("file:/path")

	// Create the case classes for our domain
	case class Department(id: String, name: String)
	case class Employee(firstName: String, lastName: String, email: String, salary: Int)
	case class DepartmentWithEmployees(department: Department, employees: Seq[Employee])

	// Create the Departments
	val department1 = new Department("123456", "Computer Science")
	val department2 = new Department("789012", "Mechanical Engineering")
	val department3 = new Department("345678", "Theater and Drama")
	val department4 = new Department("901234", "Indoor Recreation")

	// Create the Employees
	val employee1 = new Employee("michael", "armbrust", "no-reply@berkeley.edu", 100000)
	val employee2 = new Employee("xiangrui", "meng", "no-reply@stanford.edu", 120000)
	val employee3 = new Employee("matei", null, "no-reply@waterloo.edu", 140000)
	val employee4 = new Employee(null, "wendell", "no-reply@princeton.edu", 160000)

	// Create the DepartmentWithEmployees instances from Departments and Employees
	val departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2))
	O/p:
	DepartmentWithEmployees = DepartmentWithEmployees(Department(123456,Computer Science),List(Employee(mich
	ael,armbrust,no-reply@berkeley.edu,100000), Employee(xiangrui,meng,no-reply@stanford.edu,120000)))

	val departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4))
	O/p:
	DepartmentWithEmployees = DepartmentWithEmployees(Department(789012,Mechanical Engineering),List(Employe
	e(matei,null,no-reply@waterloo.edu,140000), Employee(null,wendell,no-reply@princeton.edu,160000)))

	val departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee1, employee4))
	O/p:
	DepartmentWithEmployees = DepartmentWithEmployees(Department(345678,Theater and Drama),List(Employee(mic
	hael,armbrust,no-reply@berkeley.edu,100000), Employee(null,wendell,no-reply@princeton.edu,160000)))

	val departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3))
	O/p:
	DepartmentWithEmployees = DepartmentWithEmployees(Department(901234,Indoor Recreation),List(Employee(xia
	ngrui,meng,no-reply@stanford.edu,120000), Employee(matei,null,no-reply@waterloo.edu,140000)))

	//create a sequence of DepartmentWithEmployees
	val departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2)
	O/p:
	List(DepartmentWithEmployees(Department(123456,Computer Science),List
	(Employee(michael,armbrust,no-reply@berkeley.edu,100000), Employee(xiangrui,meng,no-reply@stanford.edu,120000))), DepartmentWithEm
	ployees(Department(789012,Mechanical Engineering),List(Employee(matei,null,no-reply@waterloo.edu,140000), Employee(null,wendell,no
	-reply@princeton.edu,160000))))

	val df1 = departmentsWithEmployeesSeq1.toDF()

	val departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4)
	O/p:
	Seq[DepartmentWithEmployees] = List(DepartmentWithEmployees(Department(345678,Theater and Drama),Lis
	t(Employee(michael,armbrust,no-reply@berkeley.edu,100000), Employee(null,wendell,no-reply@princeton.edu,160000))), DepartmentWithE
	mployees(Department(901234,Indoor Recreation),List(Employee(xiangrui,meng,no-reply@stanford.edu,120000), Employee(matei,null,no-re
	ply@waterloo.edu,140000))))

	//convert these sequences to DFs
	val df1 = departmentsWithEmployeesSeq1.toDF()
	val df2 = departmentsWithEmployeesSeq2.toDF()

	df1.show
	+--------------------+--------------------+
	|          department|           employees|
	+--------------------+--------------------+
	|[123456,Computer ...|[[michael,armbrus...|
	|[789012,Mechanica...|[[matei,null,no-r...|
	+--------------------+--------------------+

	df2.show
	+--------------------+--------------------+
	|          department|           employees|
	+--------------------+--------------------+
	|[345678,Theater a...|[[michael,armbrus...|
	|[901234,Indoor Re...|[[xiangrui,meng,n...|
	+--------------------+--------------------+

	//combine these 2 DFs,
	val unionDF = df1.unionAll(df2)

	unionDF.show
	+--------------------+--------------------+
	|          department|           employees|
	+--------------------+--------------------+
	|[123456,Computer ...|[[michael,armbrus...|
	|[789012,Mechanica...|[[matei,null,no-r...|
	|[345678,Theater a...|[[michael,armbrus...|
	|[901234,Indoor Re...|[[xiangrui,meng,n...|
	+--------------------+--------------------+


	//creates JSON file
	unionDF.write.format("json").save("file:///home/shubhro2705854012/sparkLocal/emp_create.json")

	//read JSON file and print schema
	val emp1DF = sqlContext.read.json("file:///home/shubhro2705854012/sparkLocal/emp_create.json")

	emp1DF.printSchema
	O/p:
	root
	 |-- department: struct (nullable = true)
	 |    |-- id: string (nullable = true)
	 |    |-- name: string (nullable = true)
	 |-- employees: array (nullable = true)
	 |    |-- element: struct (containsNull = true)
	 |    |    |-- email: string (nullable = true)
	 |    |    |-- firstName: string (nullable = true)
	 |    |    |-- lastName: string (nullable = true)
	 |    |    |-- salary: long (nullable = true)


----------------------------------------
Processing JSON using Hive:
example-1,

JSON:
{"EmployeeID":10,"FirstName":"Andrew","Title":"Manager","State":"DE","Laptop":"PC"}
{"EmployeeID":11,"FirstName":"Arun","Title":"Manager","State":"NJ","Laptop":"PC"}
{"EmployeeID":12,"FirstName":"Harish","Title":"Sales","State":"NJ","Laptop":"MAC"}

Steps Include:
1) create hiveContext.
2) read json : df_json = hc.read.json("file:///home/username/names.json"), this is not mandatory. Does not solve anything.

import org.apache.spark.sql.hive.HiveContext
val hc = new HiveContext(sc)

val emp_df = hc.read.json("file:///home/shubhro2705854012/sparkLocal/employee.json")
o/p:
emp_df: org.apache.spark.sql.DataFrame = [EmployeeID: bigint, FirstName: string, Laptop: string, State: string, Title: string]

scala> emp_df.printSchema
	root
	 |-- EmployeeID: long (nullable = true)
	 |-- FirstName: string (nullable = true)
	 |-- Laptop: string (nullable = true)
	 |-- State: string (nullable = true)
	 |-- Title: string (nullable = true)
	 

3) Create table and access the JSON:
hc.sql("create database emp_db")
hc.sql("use emp_db")
hc.sql("create table emp_hv_tb2(line STRING)")  ##line will take the entire json section as a line when we load.
hc.sql("load data local inpath 'file:///home/shubhro2705854012/sparkLocal/employee.json' OVERWRITE INTO TABLE emp_hv_tb2")
hc.sql("select * from emp_hv_tb2").show
O/p:
	+--------------------+
	|                line|
	+--------------------+
	|{"EmployeeID":10,...|
	|{"EmployeeID":11,...|
	|{"EmployeeID":12,...|
	+--------------------+

hc.sql("select get_json_object(line,'$.EmployeeID') AS id,get_json_object(line,'$.FirstName') as Name from emp_hv_tb2").show

NOTE: Get_json_object will fetch each section in the line having JSON content.

O/p:
	+---+------+
	| id|  Name|
	+---+------+
	| 10|Andrew|
	| 11|  Arun|
	| 12|Harish|
	+---+------+


other way in Hive json,
hc.sql("select x.* from emp_hv_tb2 lateral view json_tuple(line,'EmployeeID','FirstName','Laptop','State','Title') x as i,f,l,s,t").show

	O/p:
	+---+------+---+---+-------+
	|  i|     f|  l|  s|      t|
	+---+------+---+---+-------+
	| 10|Andrew| PC| DE|Manager|
	| 11|  Arun| PC| NJ|Manager|
	| 12|Harish|MAC| NJ|  Sales|
	+---+------+---+---+-------+

This can also be used to insert data into a new table as,
1) hc.sql("create table emp_hv_tb3(EmployeeID bigint,FirstName string,Laptop string,State string,Title string)")
2) hc.sql("INSERT INTO TABLE emp_hv_tb3 SELECT x.* from emp_hv_tb2 lateral view json_tuple(line,'EmployeeID','FirstName','Laptop','State','Title') x as i,f,l,s,t")
3) hc.sql("select * from emp_hv_tb3").show

	O/p:
	+----------+---------+------+-----+-------+
	|employeeid|firstname|laptop|state|  title|
	+----------+---------+------+-----+-------+
	|        10|   Andrew|    PC|   DE|Manager|
	|        11|     Arun|    PC|   NJ|Manager|
	|        12|   Harish|   MAC|   NJ|  Sales|
	+----------+---------+------+-----+-------+


example with nested json:

Json file,
	[{
		"Year": "2013",
		"First Name": "DAVID",
		"County": "KINGS",
		"Sex": "M",
		"Count": "272"
	}, {
		"Year": "2013",
		"First Name": "JAYDEN",
		"County": "KINGS",
		"Sex": "M",
		"Count": "268"
	}, {
		"Year": "2013",
		"First Name": "JAYDEN",
		"County": "QUEENS",
		"Sex": "M",
		"Count": "219"
	}, {
		"Year": "2013",
		"First Name": "MOSHE",
		"County": "KINGS",
		"Sex": "M",
		"Count": "219"
	}, {
		"Year": "2013",
		"First Name": "ETHAN",
		"County": "QUEENS",
		"Sex": "M",
		"Count": "216"
	}]
	
	import org.apache.spark.sql.hive.HiveContext
	val hc = new HiveContext(sc)

	hc.sql("use emp_db")
	hc.sql("create table random_tab1(line STRING)")
	hc.sql("load data local inpath 'file:///home/shubhro2705854012/sparkLocal/random_ex.json' OVERWRITE INTO TABLE random_tab1")

	hc.sql("select * from random_tab1").show
		O/p:
		+--------------------+
		|                line|
		+--------------------+
		|                  [{|
		|       "Year": "2013",|
		|       "First Name": "D...|
		|       "County": "KINGS",|
		|               "Sex": "M",|
		|       "Count": "272"|
		|                }, {|
		|       "Year": "2013",|
		|       "First Name": "J...|
		|       "County": "KINGS",|
		|               "Sex": "M",|
		|       "Count": "268"|
		|                }, {|
		|       "Year": "2013",|
		|       "First Name": "J...|
		|       "County": "QUEENS",|
		|               "Sex": "M",|
		|       "Count": "219"|
		|                }, {|
		|       "Year": "2013",|
		+--------------------+
		only showing top 20 rows

	hc.sql("select get_json_object(line,'$.Year'),get_json_object(line,'$.Count') from random_tab1").show

	O/p: this will treat each line separate.

--------
working with XML using SQLContext,
1) third party library example databricks jar file, will be kept in spark lib directory i.e.
//dealing xml files

//to read xml files you need to restart your spark-shell with the below given arguments
spark-shell --packages com.databricks:spark-xml_2.10:0.4.1

val employees_df = sqlContext.read.format("com.databricks.spark.xml").option("inferSchema", "true").option("rootTag","employees").option("rowTag","employee").load("/user/cloudera/datasets/employees.xml")

employees_df.registerTempTable("employee_tab")

sqlContext.sql("select emp_no, emp_name, address.city, address.country, address.pincode, salary, dept_no from employee_tab").show

OR

val emp_dataNormal = employees_df.select("emp_no","emp_name","address.city","address.country","address.pincode","salary","dept_no").show

o/p:
+------+--------+-----+-------+-------+------+-------+
|emp_no|emp_name| city|country|pincode|salary|dept_no|
+------+--------+-----+-------+-------+------+-------+
|    10|     jon|Paris| London| 200010| 15000|      2|
|    11|    Adom|Texas|America| 200040| 25000|      5|
+------+--------+-----+-------+-------+------+-------+

2) integrate spark with hive using hiveContext and apply xml parsers like xpath(), xpath_string(), xpark_int()..etc.

example,xml1
<rec><name>Ravi</name><age>25</age></rec>
<rec><name>Rani</name><sex>f</sex></rec>
<rec><name>Giri</name><age>35</age><sex>f</sex></rec>

hc.sql("create table raw(line string)")
hc.sql("load data local inpath 'xml1' into table raw")
hc.sql ("create table info (name string, age int, sex string, city string)
		row format delimited fields terminated by ','")
hc.sql("load data inpath xml1 into table raw")
hc.sql ("insert overwrite table info
		select xpath_string(line,rec/name),
		xpath_int(line,rec/age),
		xpath_string(line,rec/sex)
		from raw").show()  //all null entries for age will be retained as ZERO

hc.sql("create table results(name string,age int,sex string) row format delimited fields terminated by ','")
hc.sql ("insert overwrite table results
		select xpath_string(line,rec/name),
		xpath_int(line,rec/age),
		xpath_string(line,rec/sex)
		from raw")

hc.sql ("select * from results").show()
--------------------
to convert RDDs into dataframes implicitely, we import (import sqlContext.implicits._)
In spark shell, it is available.

import sqlContext.implicits._

then, val df = <RDD>.toDF //will convert the RDD to data frame.NOTE, the RDD should have schema.
df.show() //similar to collect.
df.printSchema //show the schema of the data frame.

--------------------


-------------------------------
accessing Hive tables using Spark SQL:

in cloudera VM - 

ls usr/lib/spark/conf and check for hive-site.xml (if not copy from hive directory /etc/hive/conf.dist/hive-site.xml)

import org.apache.spark.sql.hive.HiveContext

val hc = new HiveContext(sc)

hc.sql("create database mySpark")  //Database will be created in Hive. this will return a dataframe.
								   //you can check in hive shell via show databases
								   
hc.sql ("use mySpark")

hc.sql(" create table samp(id int, name string, sal int, sex string,dno int) rowformat delimited....")

hc.sql ("load data local inpath 'emp' into table samp")

hc.sql(" select * from samp").show()   //the data created is in the form of dataframe

-----------------------------
nested JSONs,

in Hive,
create table jraw (line string);
load data local inpath 'json2' into table jraw;
create table raw2(name string, age int, wife string, city string);
insert into table raw2 select x.* from jraw lateral view json_tuple(line,'name','age','wife','city') x as n,a,w,c
select * from raw2

o/p:
Ravi 25 {"name":"Rani","age":24,"city":"Hyd"}	del
Kiran	30	{"name":"Veni","qual":"btech","city":"Hyd"} Hyd	

select name, get_json_object(wife,'$.name),get_json_object(wife,'$.age),get_json_object(wife,'$.qual),get_json_object(wife,'$.city)
city from raw2;

//will make data completely structured

in Spark SQL,
 val spouse = sqc.read.json("HDFS location")
 
this automatically does the structuring in the form dataframe

spouse.show()

o/p:
25 del	Ravi	[24,hyd,Rani,null]
30 hyd	Kiran	[null,hyd,Veni,btech]

Note: if we do a spouse.collect, it gives,
Array([25,del,Ravi,[24,hyd,Rani,null]],[30,hyd,kiran,[null,hyd,Veni,btech]])

couples.collect ( x => x(3)) //will give the nested JSON


-----------------------
Spark Dataframes and datasets

there are 3 types of data objects:
a) RDD
b) dataframe 
c) dataset

RDD						Dataframes			dataSets
RDD API available		not available		available
DF API not aval			available			not available
DS API not avail		not available		available
no default optmz		catalyst optimizer	catalyst optimizer + tungsten optimizer
fast					faster than RDD		faster than RDD and dataframes
use in memory computing use in memory c		uses CPU caches along with in memory computing
											(L1,L2,L3 caching i.e. different layers of caches)
											caching is even faster than in memory computing
											
import sqlContext.implicits._
case class sample(a:Int,b:Int)
val rdd = sc.parallelize(List(Sample(10,20),Sample(1,2),Sample(5,6),Sample(100,200),Sample(1000,2000))
val df = rdd.toDF
df.select("a").show()  //creates a dataframe and prints it

o/p
10
1
5
100
1000

df.select ("a","b").show() //entire data into tabular format.
df.select ("a","b"+100).show() //ERRORS. i.e. operations not allowed on columns using this command.
df.select (df("a"),df("b")+100).show() //VALID. will add 100 to all b column elements.
df.select (df("a"),df("b")+1000).show() //will add 1000 on the result of previous statement.
df.filter (df("a") >= 100).show() //will filter
df.groupBy("age").count().show()  //VALID.

emp:
101,aaa,70000,m,12
102,bbb,90000,f,12
103,ccc,10000,m,11
104,ddd,20000,f,13
105,eee,90000,f,14
106,fff,40000,m,15
107,iiii,60000,m,13
108,jjj,90000,f,15

val data = sc.textFile("emp")
case class info(id:Int,name:String,sal:Int,sex:String,dno:Int)

val emp = data.map { x => 
val w = x.split(",")
val id = w(0).toInt
val name = w(1)
val sal = w(2).toInt
val sex = w(3)
val dno = w(4).toInt
val inf = info(id,name,sal,sex,dno)
inf
}


val empdf = emp.toDF //converts to dataframe

empdf.groupBy(empdf("sex")).count.show()  //i.e. select sex,count(*) from emp group by sex;
empdf.groupBy(empdf("sex")).agg(sum("sal")).show() //i.e. sum of sal i.e. for each gender what is sum salary.
empdf.groupBy(empdf("sex")).agg(sum("sal"),max("sal")).show()

o/p:
sex		sum(sal)	max(sal)

empdf.groupBy(empdf("dno"),empdf("sex")).agg(sum("sal"),max("sal")).show()

o/p:
dno	sex		sum(sal)	max(sal)


working with data sets:
example1,
val ds = Seq(1,2,3).toDS()  //creates dataset i.e. org.apache.spark.sql.Dataset[Int]
ds.map( x => x +10 )        //all RDD functions can be applied to DS with the advantage of increased speed.

example2,
case class Person(name:String, age:Long)
val ds = Seq(Person("Andy",32),Person("Murray",42)).toDS()
ds.collect  //Array(Person(Andy,32),Person(Murray,42))

example3,
sampjson (in HDFS)
{"name":"Hari","age":30}
{"name":"Lata","age":25}
{"name":"Mani","age":23}


val info1 = sqc.read.json("sampjson")   //sqc is sql context created above. o/p will be a dataframe
case class Person(name:String, age:Long)
val info2 = sqc.read.json("sampjson").as[Person]  //info2 will be created as a datasets. RDD API can be applied to this DS

word count example RDD vs DS,

val lines = sqlContext.read.text("<file name>")
val words = lines.flatMap(_.split(" ")).filter(_! = "")

val counts = words.groupBy(._toLowerCase).map( w => w._1,w._2.size)   //i.e. RDD functions

val counts = words.groupBy(._toLowerCase).count()   //i.e. dataset style of grouping aggregation.

-----------------------
//dealing parquet files
val baby_names_df = sqlContext.read.parquet("/user/cloudera/datasets/baby_names.parquet")
baby_names_df.show
baby_names_df.write.json("/user/cloudera/parquet-to-json")

----------------------

Format and Write modes for dataframes:
a) orc format: this works with HiveContext only. The DF is saved as orc format.
b) Parquet format: DF is saved in Parquet format.
c) Json format: DF is saved in json format. 
d) Text format: This works only when DF has single column data. DF is saved in text format.
e) avro and csv format: we will need to provide external jars but DF can be stored in avro and csv format.

*** write can be done via write.format method. example, <DF>.write.format("json").save("file:/path")

Save modes for data frames:
for this we will have to import org.apache.spark.sql.SaveMode

a) SaveMode.Append: if data/table already exists, contents of DF will be appended.
b) SaveMode.Overwrite: if data/table already exists, it will be overwritten by contents of DF.
c) SaveMode.Ignore: This is similar to "CREATE TABLE IF NOT EXISTS" in SQL. i.e.
					if data/table already exists, it will not change the contents of existing data.
					
Save options:
a) Jdbc(url String,table String, prop Properties): to save DF to external databases.
		We will have to provide driver jar, connection properties (username, password, connection URL, table name)
b) Save(path String): Save DF in provided location.
c) SaveAsTable(table String): default save is as Hive table and DF will be saved to /hadoop/hive/<> i.e. hive location.





-------------------
DF docs to be further referred:
https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html
https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/
https://www.tutorialspoint.com/spark_sql/spark_sql_dataframes.htm

if possible: 
https://mapr.com/blog/using-apache-spark-dataframes-processing-tabular-data/



















