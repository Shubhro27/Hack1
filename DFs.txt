https://www.tutorialspoint.com/spark_sql/spark_sql_dataframes.htm
https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html
https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/

dataframe is a distributed collection of rows under named columns i.e. conceptually equivalent to relational tables.
DFs are,
1) Immutable
2) Supports Lazy evaluation i.e. task is executed only when an  action triggers
3) Distributed

consider employee.json as,
{"EmployeeID":10,"FirstName":"Andrew","Title":"Manager","State":"DE","Laptop":"PC"}
{"EmployeeID":11,"FirstName":"Arun","Title":"Manager","State":"NJ","Laptop":"PC"}
{"EmployeeID":12,"FirstName":"Harish","Title":"Sales","State":"NJ","Laptop":"MAC"}


DF operations include:
1) val dfs = sqlContext.read.json("file:///home/shubhro2705854012/sparkLocal/employee.json") to read JSON document

2) dfs.show to show the data i.e. sysnonymous to RDD collect
	O/p:
	+----------+---------+------+-----+-------+
	|EmployeeID|FirstName|Laptop|State|  Title|
	+----------+---------+------+-----+-------+
	|        10|   Andrew|    PC|   DE|Manager|
	|        11|     Arun|    PC|   NJ|Manager|
	|        12|   Harish|   MAC|   NJ|  Sales|
	+----------+---------+------+-----+-------+
	
3) select method:
	dfs.select("FirstName","EmployeeID").show
	O/p:
	+---------+----------+
	|FirstName|EmployeeID|
	+---------+----------+
	|   Andrew|        10|
	|     Arun|        11|
	|   Harish|        12|
	+---------+----------+	
	
4) filter method:
	a) dfs.filter(dfs("EmployeeID") > 9).show
		O/p:
		+----------+---------+------+-----+-------+
		|EmployeeID|FirstName|Laptop|State|  Title|
		+----------+---------+------+-----+-------+
		|        10|   Andrew|    PC|   DE|Manager|
		|        11|     Arun|    PC|   NJ|Manager|
		|        12|   Harish|   MAC|   NJ|  Sales|
		+----------+---------+------+-----+-------+
		
	b) dfs.filter(dfs("Laptop") === "PC").show
		O/p:
		+----------+---------+------+-----+-------+
		|EmployeeID|FirstName|Laptop|State|  Title|
		+----------+---------+------+-----+-------+
		|        10|   Andrew|    PC|   DE|Manager|
		|        11|     Arun|    PC|   NJ|Manager|
		+----------+---------+------+-----+-------+
		
		OR
		dfs.filter($"laptop" === "PC").show()
		
	c) 	dfs.filter(dfs("Laptop") === "PC" && dfs("State") === "NJ").show   NOTE: single & does not work. OR is represented by ||.
		O/p:
		+----------+---------+------+-----+-------+
		|EmployeeID|FirstName|Laptop|State|  Title|
		+----------+---------+------+-----+-------+
		|        11|     Arun|    PC|   NJ|Manager|
		+----------+---------+------+-----+-------+
		
	d) Using filter and select together:
		dfs.filter(dfs("Laptop") === "PC" && dfs("State") === "NJ").select("EmployeeID","FirstName").show
		O/p:
		+----------+---------+
		|EmployeeID|FirstName|
		+----------+---------+
		|        11|     Arun|
		+----------+---------+
		
	e) negation of a condition using filter:
		dfs.filter(not(dfs("Laptop") === "PC")).show OR dfs.filter(not($"Laptop" === "PC")).show 
		O/p:
		+----------+---------+------+-----+-----+
		|EmployeeID|FirstName|Laptop|State|Title|
		+----------+---------+------+-----+-----+
		|        12|   Harish|   MAC|   NJ|Sales|
		+----------+---------+------+-----+-----+
		
	f) pattern matching with filter:
		dfs.filter($"state" like "N%").show.  NOTE: dfs.filter($"state" like "N*").show DOES NOT WORK
		O/p:
		+----------+---------+------+-----+-------+
		|EmployeeID|FirstName|Laptop|State|  Title|
		+----------+---------+------+-----+-------+
		|        11|     Arun|    PC|   NJ|Manager|
		|        12|   Harish|   MAC|   NJ|  Sales|
		+----------+---------+------+-----+-------+

5) groupBy method:
	dfs.groupBy("Laptop").count().show()
	O/p:
	+------+-----+
	|Laptop|count|
	+------+-----+
	|   MAC|    1|
	|    PC|    2|
	+------+-----+
	
6) sort method:
	dfs.sort(dfs("EmployeeID").asc).show
	O/p:
	+----------+---------+------+-----+-------+
	|EmployeeID|FirstName|Laptop|State|  Title|
	+----------+---------+------+-----+-------+
	|        10|   Andrew|    PC|   DE|Manager|
	|        11|     Arun|    PC|   NJ|Manager|
	|        12|   Harish|   MAC|   NJ|  Sales|
	+----------+---------+------+-----+-------+
	
	dfs.sort(dfs("EmployeeID").desc).show
	O/p:
	+----------+---------+------+-----+-------+
	|EmployeeID|FirstName|Laptop|State|  Title|
	+----------+---------+------+-----+-------+
	|        12|   Harish|   MAC|   NJ|  Sales|
	|        11|     Arun|    PC|   NJ|Manager|
	|        10|   Andrew|    PC|   DE|Manager|
	+----------+---------+------+-----+-------+
	
7) where clause (is same as filter)
	dfs.where(dfs("Title") === "Manager").sort($"EmployeeID".desc).show
	O/p:
	+----------+---------+------+-----+-------+
	|EmployeeID|FirstName|Laptop|State|  Title|
	+----------+---------+------+-----+-------+
	|        11|     Arun|    PC|   NJ|Manager|
	|        10|   Andrew|    PC|   DE|Manager|
	+----------+---------+------+-----+-------+
	
8) handling Null values in dataframe.
	JSON file:
	{
	   {"id" : "1201", "name" : "satish", "age" : "25"}
	   {"id" : "1202", "name" : "krishna", "age" : "28"}
	   {"id" : "1203", "name" : "amith", "age" : null}
	   {"id" : "1204", "name" : "javed", "age" : "23"}
	   {"id" : "1205", "name" : "prudvi", "age" : "23"}
	}
	 
	val empN_DF = sqlContext.read.json("file:///home/shubhro2705854012/sparkLocal/emp_new.json")

	empN_DF.select("id","name","age").show

		O/p:
		+----+-------+----+
		|  id|   name| age|
		+----+-------+----+
		|null|   null|null|
		|1201| satish|  25|
		|1202|krishna|  28|
		|1203|  amith|null|
		|1204|  javed|  23|
		|1205| prudvi|  23|
		|null|   null|null|
		+----+-------+----+
	 
	get NULL values and fill null values across all columns in a dataframe:
	val emp_na = empN_DF.na   // emp_na will be a org.apache.spark.sql.DataFrameNaFunction
	val notNull_emp = emp_na.fill("--") // will be a dataframe
	notNull_emp.show

		O/p:
		+---------------+---+----+-------+
		|_corrupt_record|age|  id|   name|
		+---------------+---+----+-------+
		|              {| --|  --|     --|
		|             --| 25|1201| satish|
		|             --| 28|1202|krishna|
		|             --| --|1203|  amith|
		|             --| 23|1204|  javed|
		|             --| 23|1205| prudvi|
		|              }| --|  --|     --|
		+---------------+---+----+-------+
	 
	replace NULL values with a specific value in dataframe:
	val emp_na2 = empN_DF.na.fill("23",Seq("age"))

		O/p:
		+---------------+---+----+-------+
		|_corrupt_record|age|  id|   name|
		+---------------+---+----+-------+
		|              {| 23|null|   null|
		|           null| 25|1201| satish|
		|           null| 28|1202|krishna|
		|           null| 23|1203|  amith|
		|           null| 23|1204|  javed|
		|           null| 23|1205| prudvi|
		|              }| 23|null|   null|
		+---------------+---+----+-------+

	NOTE: multiple columns can be done as empN_DF.na.fill("23",Seq("age")).na.fill("Shubhro",Seq("Name")	
	
9) Drop columns in a dataframe.
	we will have to select all columns and then drop the columns to be dropped as
	emp_na2.select("_corrupt_record","age","id","name").drop("_corrupt_record").show
	
	O/p:
	+---+----+-------+
	|age|  id|   name|
	+---+----+-------+
	| 23|null|   null|
	| 25|1201| satish|
	| 28|1202|krishna|
	| 23|1203|  amith|
	| 23|1204|  javed|
	| 23|1205| prudvi|
	| 23|null|   null|
	+---+----+-------+
	
	Drop rows???????