Spark-Kafka Streaming with Scala code compiled with SBT:

References: 
create an all inclusive uber-jar, using sbt assembly: https://stackoverflow.com/questions/27710887/kafkautils-class-not-found-in-spark-streaming
sbt-assembly: https://github.com/sbt/sbt-assembly
logging error: https://community.hortonworks.com/questions/58286/noclassdeffounderror-orgapachesparklogging-using-s.html

1) create a topic:
   kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sb-wrdcnt-topic
   
2) start a kafka producer:
	kafka-console-producer.sh --broker-list ip-172-31-13-154.ec2.internal:6667 --topic sb-wrdcnt-topic
	
3) Set up SBT,
   a) create a directory: sb_sbt_stream_test.
   b) create sub directories /src/main/scala (the WordCount.scala in this location)
   c) create a simple.sbt file
   d) Add the dependencies to this simple.sbt file as,
		name := "Spark_Stream_App1"
		version := "1.5.2"
		scalaVersion := "2.10.4"
		libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "1.5.2",
		"org.apache.spark" %% "spark-streaming" % "1.5.2",
		"org.apache.spark" %% "spark-streaming-kafka" % "1.5.2")

		mergeStrategy in assembly := {
		  case m if m.toLowerCase.endsWith("manifest.mf")          => MergeStrategy.discard
		  case m if m.toLowerCase.matches("meta-inf.*\\.sf$")      => MergeStrategy.discard
		  case "log4j.properties"                                  => MergeStrategy.discard
		  case m if m.toLowerCase.startsWith("meta-inf/services/") => MergeStrategy.filterDistinctLines
		  case "reference.conf"                                    => MergeStrategy.concat
		  case _                                                   => MergeStrategy.first
		}

		NOTE:
		1. The spark version, scala version should be same as what has been installed on the VM or cloud.
		2. We will have to add dependency for spark-core, spark-streaming and spark-streaming-kafka. This is done in "libraryDependencies".
		   There is a difference between,
		   "org.apache.spark" %% "spark-streaming-kafka" % "1.5.2" % "Provided"
		   and 
		   "org.apache.spark" %% "spark-streaming-kafka" % "1.5.2"
			i.e. Dependencies with "provided" scope are only available during compilation and testing, and are not available at runtime or for packaging.
		3. SBT Assembly is a sbt plugin which creates a fat JAR of your project with all of its dependencies.
	e) In case you are adding an assembly, create a plugins.sbt in /project.
			resolvers ++= Seq(
				"Maven Central" at "https://repo1.maven.org/maven2/"
			)

			addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.1")	
			
	f) Write the WordCount.scala program,
		import org.apache.spark._
		import org.apache.spark.streaming.StreamingContext
		import org.apache.spark.streaming.Seconds
		import org.apache.spark.streaming.kafka.KafkaUtils
		object WordCount {
		  def main( args:Array[String] ){
			val conf = new SparkConf().setMaster("local[*]").setAppName("KafkaReceiver")
			val ssc = new StreamingContext(conf, Seconds(10))
			val kafkaStream = KafkaUtils.createStream(ssc, "localhost:2181","spark-streaming-consumer-group", Map("sb-wrdcnt-topic" -> 5))
			//need to change the topic name and the port number accordingly
			val words = kafkaStream.flatMap(x => x._2.split(" "))
			val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
			kafkaStream.print()  //prints the stream of data received
			wordCounts.print()   //prints the wordcount result of the stream
			ssc.start()
			ssc.awaitTermination()
		  }
		}
	g) Instead of using "sbt package" as assembly is added, you hit "sbt assembly" to create package.
	   This will create a package in /sb_sbt_stream_test/target/scala-2.10
	   package name: Spark_Stream_App1-assembly-1.5.2.jar
	   
	h) Run the "WordCount" API via,
		spark-submit \
		--class "WordCount" \
		--master local[4] \
		target/scala-2.10/spark_stream_app1_2.10-1.5.2.jar
	
4)  Start typing on the producer terminal and the SBT will start counting words as,
		-------------------------------------------
		Time: 1530779540000 ms
		-------------------------------------------
		.
		.
		-------------------------------------------
		Time: 1530779550000 ms
		-------------------------------------------
		(cool,3)
		(need,1)
		(very,1)
		(be,1)
		(to,1)
	

Errors and information while setting up SBT:
a) If you are using Spark 1.x, it is mandatory to specify right Spark and Scala version in your file along with the dependency. 
	example,
		For Spark version1:
		The simple.sbt file looks like:
		name = Spark_Stream_App1
		version = 1.5.2
		scalaVersion = 2.10.4
		libraryDependencies += "org.apache.spark" %% "spark-streaming-kafka-assembly" % "1.5.2"


		For Spark version2:
		The simple.sbt file looks like:
		name = Spark_Stream_App1
		version = 2.0.1
		scalaVersion = 2.11.8
		libraryDependencies += "org.apache.spark" %% "spark-streaming-kafka-0-8" % "2.0.1" % "provided"
		
	There is no dependency named spark-streaming-kafka-0-9. Only spark-streaming-kafka-0-8 and spark-streaming-kafka-0-10 jars are available for integration of Spark streaming and Kafka in Spark 2 version.

	These dependencies can be connected from: https://mvnrepository.com/artifact/org.apache.spark/

b)  If for spark version-1, 	
		simple.sbt:
		name := "Spark_Stream_App1"
		version := "1.5.2"
		scalaVersion := "2.10.4"
		libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "1.5.2",
		"org.apache.spark" %% "spark-streaming" % "1.5.2" % "provided",
		"org.apache.spark" %% "spark-streaming-kafka" % "1.5.2")
		
	This dependency will allow the package to compile successfully but fail during runtime with the error:
		Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/streaming/kafka/KafkaUtils$
		Caused by: java.lang.ClassNotFoundException: org.apache.spark.streaming.kafka.KafkaUtils$
		
	In order to handle this error, you will have to create an all inclusive uber-jar, using sbt assembly.[Step 3-d to 3-e]
	
c) In case we have the following in simple.sbt file,
	libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "1.5.2",
	"org.apache.spark" %% "spark-streaming" % "1.5.2" % "provided",
	"org.apache.spark" %% "spark-streaming-kafka-0-8" % "2.2.2" % "provided")
	
	this will give a "Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/Logging"
	
	Reason being,
	You need to compile and run your spark job with an SBT that points to the same version of Spark and same version of Scala. 
	You must be using Scala 2.10 and make the code appropriately. 2.0.0 doesn't make sense in that SBT.
	
	So, you will have to change the library dependencies as,
		libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "1.5.2",
		"org.apache.spark" %% "spark-streaming" % "1.5.2",
		"org.apache.spark" %% "spark-streaming-kafka" % "1.5.2")
	
	and it runs fine.
	
------------------------------------------
email:

Hi,
Thank you for your suggestions.

I was able to successfully create the SBT package (spark_stream_app1_2.10-1.5.2.jar) by adding the following library dependencies:
		libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "1.5.2",
		"org.apache.spark" %% "spark-streaming" % "1.5.2",
		"org.apache.spark" %% "spark-streaming-kafka" % "1.5.2")
		
but at runtime, it failed with error:
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/streaming/kafka/KafkaUtils$
Caused by: java.lang.ClassNotFoundException: org.apache.spark.streaming.kafka.KafkaUtils$

To handle this error, I created an all inclusive uber-jar, using sbt assembly (reference: https://github.com/sbt/sbt-assembly).

While my application is working fine, the size of the jar is very large and I am not using most of the classes, for instance,
-rw-rw-r-- 1 shubhro2705854012 shubhro2705854012     5095 Jul  5 06:45 spark_stream_app1_2.10-1.5.2.jar       #i.e. around 5 MB
-rw-rw-r-- 1 shubhro2705854012 shubhro2705854012 93216095 Jul  5 08:30 Spark_Stream_App1-assembly-1.5.2.jar   #i.e. around 93 MB

Is there an alternate way to optimize the package and accomplish the wordCount logic.

-----------------------------------------
email-2:
Hi,

Thanks for the response,I have already accomplished word count and some more case studies using spark streaming from shell,
But, I am trying to accomplish "kafka - spark streaming - kafka"  E2E via SBT to emulate production like scenario.

With that said, I did try the approach you have mentioned i.e. in my build.sbt file,
name := "Spark_Stream_App1"
version := "1.5.2"
scalaVersion := "2.10.4"
libraryDependencies ++= Seq("org.apache.spark" %% "spark-streaming-kafka-assembly" % "1.5.2",
"org.apache.spark" %% "spark-core" % "1.5.2",
"org.apache.spark" %% "spark-streaming" % "1.5.2")

The build was able to package successfully, but when I try to run as,
spark-submit \
--class "WordCount" \
--master local[4] \
target/scala-2.10/spark_stream_app1_2.10-1.5.2.jar

It errors with:
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/streaming/kafka/KafkaUtils$

The only way I could successfully run is using sbt-assembly. Could you please help me by sharing any example where just adding spark-streaming-kafka-assembly
worked.

PFA for the approach I have used as per your suggestion.


