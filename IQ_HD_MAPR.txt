MapR and Hadoop: Imporant link: https://data-flair.training/blogs/hadoop-inputformat/

Part-1: https://www.dezyre.com/article/mapreduce-interview-questions-and-answers-for-2017/248

Part-2: https://www.guru99.com/hadoop-mapreduce-interview-question.html

Part-3: https://intellipaat.com/interview-question/map-reduce-interview-questions/

Part-4: https://mindmajix.com/hadoop-interview-questions

Part-5: https://www.dezyre.com/article/top-100-hadoop-interview-questions-and-answers-2018/159


----------------------------------------------------------------------------------------------------
Q. What are the daemons of HDFS?
1. NameNode
2. DataNode
3. Secondary NameNode.

-----
Inputformats is the first phase in a map-reduce process where inputformats decipher the type of file contents before creating data splits and sending it to mapper.
Input file resides in HDFS, the content can be line based files or binary files. Using inputformat class define how the files will be split. 

These are the most common input formats in Hadoop.
File Input Format: this is base class for all file based i/p formats. this reads the file from HDFS and divides the files into input splits (one or more). 
Text Input Format: this is default. Treat each line as a value and key is the file offset of a split.
Key Value Input Format: this derives key value format from line based on the "\t". so content before \t form key and after form value.
Sequence File Input Format: binay format files  that stores sequences of binary key-value pairs. It extends file Input format.
							.It passes data between output-input (between output of one MapReduce job to input of another MapReduce job)phases of MapReduce job
Among them Text Input Format is the Hadoop default one.

Imporant link: https://data-flair.training/blogs/hadoop-inputformat/

----
How to check  check whether Namenode is working?

Linux command:
JPS - Java Virtual Machine Process Status Tool
jps [option] [hostid] 

jps localhost:50070
**** Java Process Status which is used to list all the processes that are running on java virtual machine.

Information about Namenode, Datanode and Secondary Namenode processes can be seen using the command:
ps -fu hdfs
Information about ResourceManager, NodeManager processes can be seen using the command:
ps -fu yarn

----
Map Only Job in Hadoop MapReduce?

	// Sets reducer tasks to 0
	job.setNumReduceTasks(0);

	Can be quite helpful when you need to launch job with mappers only from terminal. You can turn off reducers by specifing 0 reducers in hadoop jar command implicitly:
	-D mapred.reduce.tasks=0 
	OR
	-jobconf mapred.reduce.tasks=0

	So the result command will be following:
	hadoop jar myJob.jar -D mapred.reduce.tasks=0 -input myInputDirs -output myOutputDir
	To be backward compatible, Hadoop also supports the "-reduce NONE" option, which is equivalent to "-D mapred.reduce.tasks=0".

----
How to do I copy data from one HDFS to another HDFS?
Distcp (distributed copy) is a tool used for copying data between clusters.
NOTE: copies data to and from the hadoop filesystems in parallel.
 It is implemented as MapReduce job where copying is done through maps that run in parallel across the cluster.
Usage: $ hadoop distcp <src> <dst>
example: $ hadoop distcp hdfs://nn1:8020/file1 hdfs://nn2:8020/file2
file1 from nn1 is copied to nn2 with filename file2 

% hadoop distcp dir1 dir2
If dir2 doesn't exist then it will create that folder and copy the contents

----
what is NAS and what is the difference between HDFS and NAS? 
NAS is Network Attached Storage. NAS is like a privatized cloud.
a storage device connected to a network that allows storage and retrieval of data from a centralised location for authorised network users. NAS has a scaling out model.

HDFS data blocks are distributed across local drives of all machines in a cluster while NAS data is stored on dedicated hardware. 	

----
What are Different modes of Hadoop?

Standalone Mode 
Pseudo Distributed Mode(Single-Node Cluster) 
Fully distributed mode (or multiple node cluster)

Standalone Mode (this is default mode):
-HDFS is not utilized in this mode
-Local file system is used for input and output

Pseudo Distributed Mode (all daemons are running in a single cluster):
Replication factory is one for HDFS.
Here one node will be used as Master Node / Data Node / Job Tracker / Task Tracker

Fully-Distributed Mode:
all daemons execute in separate nodes forming a multi node cluster.

----
Mention how many InputSplits is made by a Hadoop Framework? 
Hadoop will make 5 splits 
1 split for 64K files
2 split for 65mb files
2 splits for 127mb files

-----
distributed cache in Hadoop:
 is a facility provided by MapReduce framework.  At the time of execution of the job, it is used to cache file.  
 The Framework copies the necessary files to the slave node before the execution of any task at that node. 
 
-----
Explain JobConf in MapReduce.

It is a primary interface to define a map-reduce job in the Hadoop for job execution. JobConf specifies mapper, Combiner, partitioner, Reducer,InputFormat , OutputFormat implementations.

----
What is RecordReader in a Map Reduce?
RecordReader is used to read key/value pairs form the InputSplit by converting the byte-oriented view  and presenting record-oriented view to Mapper.

----
What is OutputCommitter?
Create temporary output directory for the job during initialization.
Then, it cleans the job as in removes temporary output directory post job completion.
Sets up the task temporary output.
Identifies whether a task needs commit. The commit is applied if required.
JobSetup, JobCleanup and TaskCleanup are important tasks during output commit.

----
What is HDFS High Availability?
1. In HDFS High Availability (HA) cluster; two separate machines are configured as NameNodes.
2. But one of the NameNodes is in an Active state; other is in a Standby state.
3. The Active NameNode is responsible for all client operations in the cluster, 
   while the Standby is simply acting as a slave, maintaining enough state to provide a fast failover if necessary.
4. They shared the same storage and all DataNodes connects to both the NameNodes.

What is HDFS Federation?
1. HDFS federation allows scaling the name service horizontally; it uses multiple independent NameNodes for different namespaces.
2. All the NameNodes use the DataNodes as common storage for blocks.
3. Each DataNode registers with all the NameNodes in the cluster.
4. DataNodes send periodic heartbeats and block reports and handles commands from the NameNodes

-----
What is rack-aware replica placement policy?
1. Rack-awareness is used to take a node’s physical location into account while scheduling tasks and allocating storage.
2. Default replication factor is 3 for a data blocks on HDFS.
3. The first two copies are stored on DataNodes located on the same rack while the third copy is stored on a different rack.

-----
ADD THIS TO Hadoop_HDFS_MapR.docx (section: HDFS command set)
 
fsck command:  hadoop fsck -files -blocks –racks [Command for finding the blocks for a file:]
fsck a utility to check health of the file system, to find missing files, over-replicated, under-replicated and corrupted blocks 

What is the purpose of dfsadmin tool?
1. It is used to find information about the state of HDFS
2. It performs administrative tasks on HDFS
3. Invoked by hadoop dfsadmin command as superuser

command for printing the topology (tree of racks and DataNodes attached to the tracks): sudo -u hdfs dfsadmin -printTopology


------
What is RAID?
RAID is a way of combining multiple disk drives into a single entity to improve performance and/or reliability. There are a variety of different levels in RAID
For example, In RAID level 1 copy of the same data on two disks increases the read performance by reading alternately from each disk in the mirror.
Q. Does Hadoop requires RAID?
1. In DataNodes storage is not using RAID as redundancy can be achieved by replication between the Nodes.
2. In NameNode’s disk RAID is recommended.

------
What are different daemons in YARN?
1. ResourceManager: Global resource manager.
2. NodeManager: One per data node, It manages and monitors usage of the container (resources in terms of Memory, CPU).
3. ApplicationMaster: One per application, Tasks are started by NodeManager

What are the two main components of ResourceManager?
Scheduler: It allocates the resources (containers) to various running applications: Container elements such as memory, CPU, disk etc.
ApplicationManager: It accepts job-submissions, negotiating for container for executing the application specific ApplicationMaster and 
					provides the service for restarting the ApplicationMaster container on failure.

-----
Assuming default configurations, how is a file of the size 1 GB (uncompressed) stored in HDFS?
Default block size is 64MB. So, file of 1GB will be stored as 16 blocks. MapReduce job will create 16 input splits; each will be processed with separate map task i.e. 16 mappers.

-----
What are Hadoop Writables?
Hadoop Writables allows Hadoop to read and write the data in a serialized form for transmission as compact binary files. 
This helps in straightforward random access and higher performance. 
Hadoop provides in built classes, which implement Writable: Text, IntWritable, LongWritable, FloatWritable, and BooleanWritable.
-----
How to set the number of reducers?
The number of reduces for the user sets the job:
1. Job.setNumReduceTasks(int)
2. -D mapreduce.job.reduces