/user/shubhro2705854012/wordCount/Corporate_Stories.txt

val rdd_WordCount = sc.textFile("wordCount/Corporate_Stories.txt")
rdd_WordCount.take(10)
val rdd_WC_splitting = rdd_WordCount.map ( x => x.split(" "))
rdd_WC_splitting.take(10)
val rdd_WC_mapping = rdd_WC_splitting.flatMap( x => x )
rdd_WC_mapping.take(10)
val rdd_WC_map_tuples = rdd_WC_mapping.map ( x => (x,1))
val rdd_WC_count = rdd_WC_map_tuples.reduceByKey (_+_)
val rdd_WC_count_sort = rdd_WC_count.sortBy ( x => x._2,false)
rdd_WC_count_sort.collect

-------------------
val sentence = Array("This is a new attempt. Spark, Hadoop are not just file systems but note: This is just scribble","but why do we say Apache_pig is not as good as Apache_hive - well that is a mystery","also I have asked 10 times no 11 times whay scala")

--val s_1 = sentence.map(x => x.split(" "))
--val s_2 = sentence.map(x => x.split("\\W"))
val s_3 = sentence.flatMap(x => x.split("[\\W|_|^0-9]"))  //this will split based on space, number and "_"
val s_3_filter = s_3.filter ( x => x != "" )
val s_3_mp = s_3_filter.map ( x => (x,1) )
val rdd_s_3_mp = sc.parallelize(s_3_mp)
val rdd_s_3_count = rdd_s_3_mp.reduceByKey(_+_)

o/p:
res3: Array[(String, Int)] = Array((are,1), (hive,1), (scribble,1), (have,1), (why,1), (new,1), (just,2), (asked,1), (file,1), (systems,1), (Spark,1), (a,2), (attempt,1), (no,1), (I,1), (good,1), (but,2), (whay,1), (Hadoop,1), (scala,1), (is,4), (note,1), (times,2), (Apache,2), (we,1), (This,2), (as,2), (pig,1), (well,1), (not,2), (mystery,1), (that,1), (say,1), (do,1), (also,1))

-------------------------------------------------------------
Fields in data set are: (comma separated)
id
age
gender
profession
zipcode

--
EXERCISE 1: Load the data from file â€œuserdata.txtâ€ into HDFS.

hdfs dfs -mkdir sb_sparkCore_assgmnt_ds
 load the file to this directory via Hue
 
val rdd_user_data1 = sc.textFile("sb_sparkCore_assgmnt_ds/attachment_userdata.txt")

------------
EXERCISE 2: Return the RDD as a set of tuples.

val rdd_user_data2 = rdd_user_data1.map { x =>
						val arr = x.split(",")
						(arr(0),arr(1).toInt,arr(2),arr(3),arr(4).toInt)
					}
					
o/p: Array[(String, Int, String, String, Int)] = Array((1,24,M,technician,85711), (2,53,F,other,94043),...)
------------
EXERCISE 3: Find the number of unique professions in the data file.

val rdd_user_data3 = rdd_user_data1.map { x =>
					 val arr = x.split(",")
					 val prof = arr(3)
					 prof}
					 
val rdd_distinct_prof = rdd_user_data3.distinct

retired
healthcare
administrator
student
doctor
writer
engineer
homemaker
educator
artist
marketing
librarian
technician
scientist
none
executive
other
programmer
lawyer
entertainment
salesman

count is 21.
------------
EXERCISE 4: How many different users belong to unique profession.

val rdd_user_data4 = rdd_user_data1.map { x => 
					 val arr = x.split(",")
					 val dept = arr(3)
					 (dept,1)
					}
					
val rdd_prof_count = rdd_user_data4.reduceByKey(_+_)

rdd_prof_count.foreach(println)

(marketing,26)
(librarian,51)
(technician,27)
(scientist,31)
(none,9)
(executive,32)
(other,105)
(programmer,66)
(lawyer,12)
(entertainment,18)
(salesman,12)
(retired,14)
(healthcare,16)
(administrator,79)
(student,196)
(doctor,7)
(writer,45)
(engineer,67)
(homemaker,7)
(educator,95)
(artist,28)

------------
EXERCISE 5: How many different users belong to unique zip code.

val rdd_user_data5 = rdd_user_data1.map { x => 
					 val arr = x.split(",")
					 val zipcode = arr(4)
					 (zipcode,1)
					}

val rdd_uniq_zip = rdd_user_data5.reduceByKey(_+_)


------------
EXERCISE 6: Find how many users are male and female.

val rdd_user_data6 = rdd_user_data1.filter { x => 
					 val arr = x.split(",")
					 arr(2) == "M"
					 }
					 
rdd_user_data6.count //670

val rdd_user_data7 = rdd_user_data1.filter { x => 
					 val arr = x.split(",")
					 arr(2) == "F"
					 }
					 
rdd_user_data7.count //273

val rdd_user_data8 = rdd_user_data1.map{ x => 
					 val arr = x.split(",")
					 val gender = arr(2)
					 (gender,1)
				}
				
val rdd_gender_count = rdd_user_data8.reduceByKey(_+_)
